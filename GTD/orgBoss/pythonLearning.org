#+OPTIONS: num:nil toc:nil
#+REVEAL_TRANS: linear
#+REVEAL_THEME: jr0cket
#+CATEGORY: Python
#+Title: Python-Front
#+Author:  Ye Zhaoliang
#+Email: yezhaoliang@ncepu.edu.cn
* 学习笔记
# 导入包的默认格式
#+BEGIN_SRC  python
    import numpy as np
    import os
    import pandas as pd
    from pandas import Series,DataFrame
    import matplotlib.pyplot as plt
    from pylab import mpl

#+END_SRC

# 处理画图汉字显示的问题
#+BEGIN_SRC python
    mpl.rcParams['font.sans-serif'] = ['FangSong']  # 指定默认字体
    mpl.rcParams['axes.unicode_minus'] = False  # 解决保存图像是负号'-'显示为方块的问题
#+END_SRC
# 处理程序运行过程中的编码问题
#+BEGIN_SRC python
    import sys
    reload(sys)
    sys.setdefaultencoding("utf-8")
#+END_SRC
# 数据横向有关联的合并（类似sql的join）
使用merge 实现字段关联的横向连接 对于数量很大的数据 可以将 sort = False
两个datafram 的关联字段名字不同，除了重命名外，可以使用right_on ，left_on
多字段关联使用 on =['key1','key2',...]
关联方法 how = 'outer'，left,right,inner 默认为 inner
dataframe内置的join方法是一种快速合并的方法。它默认以index作为对齐的列。
#对于层次化索引
方法1: 使用 right_index = True
方法2：使用 reset_index() 方法后再正常merge()
#数据拼接（对index和colunms而言的拼接）

#+BEGIN_SRC python
df = concat([df1, df2, df3])，
#+END_SRC

#重塑层次化索引（行列转换）
stack:将数据的列“旋转”为行
unstack：将数据的行“旋转”为列
也可以制定索引列进行操作
# 将“长格式”转换为“宽格式”
pivot（）
# 去重 

#+BEGIN_SRC python
drop_duplicates().
#+END_SRC

# 利用函数或者映射进行数据转换

#+BEGIN_SRC python
map()
data[u'食物'].map(lambda x:meat_to_animal[x.lower()])
#+END_SRC

# 替换

#+BEGIN_SRC python
data.replace([-999,-1000],np.nan)
#+END_SRC

# 离散化和面元划分 按步长统计（cut按步长统计，qcut按步长内数量统计具体百度）
https://blog.csdn.net/cc_jjj/article/details/78878878

#+BEGIN_SRC python
data = np.random.randn(1000)
cats = pd.qcut(data,4) #四份位数进行切割
print cats
print pd.value_counts(cats)
#+END_SRC

#当然可以设置自定义的分位数（0到1的值）

#+BEGIN_SRC python
print pd.qcut(data,[0,0.1,0.5,0.9,1.])
#+END_SRC

# 函数、方法应用
apply()是一种让函数作用于列或者行操作，applymap()是一种让函数作用于DataFrame每一个元素的操作，而map是一种让函数作用于Series每一个元素的操作
# 分组统计 groupby 
https://blog.csdn.net/youngbit007/article/details/54288603
用法太多，看例子吧，多跟聚合结合，也可以自定义apply()方法，看第九章
# 获取datafram的列名

#+BEGIN_SRC python
df.columns.values.tolist()
#+END_SRC


** TODO [#B] python pandas 数据分析学习 <2018-10-21 周日 18:09 +4d>    :python:
DEADLINE: <2018-11-11 周日>
                    :PROPERTIES:
                    :Effort: 1:00
                    :END:
                    :LOGBOOK:
                    CLOCK: [2019-01-13 周日 19:13]
                    CLOCK: [2018-12-15 周六 17:27]--[2018-12-15 周六 17:29] =>  0:02
                    - Clocked out on T:[2018-12-15 周六 17:29]
                    CLOCK: [2018-12-03 周一 20:58]--[2018-12-03 周一 23:29] =>  2:31
                    - Clocked out on T:[2018-12-03 周一 23:29] \\
                      小礼物
                    CLOCK: [2018-11-24 周六 15:30]--[2018-11-24 周六 19:51] =>  4:21
                    - Clocked out on T:[2018-11-24 周六 19:51] \\
                      python生态了解
                    CLOCK: [2018-10-23 周二 22:55]--[2018-10-23 周二 23:33] =>  0:38
                    - Clocked out on T:[2018-10-23 周二 23:33] \\
                      学习pandas 分析网络数据，并使用jupyter记录数据
                    CLOCK: [2018-10-23 周二 22:54]--[2018-10-23 周二 22:55] =>  0:01
                    - Clocked out on T:[2018-10-23 周二 22:55] \\
                      fine
                    CLOCK: [2018-10-23 周二 22:47]--[2018-10-23 周二 22:47] =>  0:00
                    - Clocked out on T:[2018-10-23 周二 22:47] \\
                      fine
                    CLOCK: [2018-10-23 周二 22:46]--[2018-10-23 周二 22:47] =>  0:01
                    - Clocked out on T:[2018-10-23 周二 22:47] \\
                      fine
                    CLOCK: [2018-10-22 周一 02:37]--[2018-10-22 周一 12:54] => 10:17
                    - Clocked out on T:[2018-10-22 周一 12:54] \\
                      安装了jupyter,和所有常用的python3.7的库，比如scipy,numpy
                    CLOCK: [2018-10-21 周日 23:33]--[2018-10-22 周一 00:02] =>  0:29
                    - Clocked out on T:[2018-10-22 周一 00:02] \\
                      testOk
                    CLOCK: [2018-10-21 周日 18:09]--[2018-10-21 周日 18:10] =>  0:01
                    :END:

 《利用python进行数据分析》  pandas作者之一Wes Mckinney 所著
 
https://note.youdao.com/share/?id=d73ecd8bfe94d2328316bd194b5de781&type=notebook#/

[[file:pythonLearning.org][pythonLearningByLiuXJ]]

目的： 

   学会简单统计
   整合常用脚本
   主要功能就是进行数据清理(by qiu）

                    
http://pandas.pydata.org/

python科学计算之Pandas
 http://www.datadependence.com/2016/05/scientific-python-pandas/                         
 https://www.kaggle.com/jpiyush3008/cnn-starter-nasnet-mobile
kaggle kernal

https://www.youtube.com/watch?v=ENPBTl0uNOE

https://www.kaggle.com/   kaggle kernal is just the same as  the jupyter notebook

The ingredients to grow your work as a data scientists

Kaggle kernals is a great place to showcase your work, learn new things, and collaborate

With a large community of 1.1MM data scientists, we can learn valuable industry trends.

Apache kaggle组织
https://github.com/apachecn/kaggle
***  What pandas solve?

Python has long been great for data munging(整理) and preparation, but less so for data
analysis and modeling(数据分析和建模). pandas helps fill this gap, enabling you to carry out your
entire data analysis workflow in Python without having to switch to a more domain
specific language like R.


*** Library Highlights¶

**** A fast and efficient DataFrame object for data manipulation with integrated
 indexing; 
**** Tools for reading and writing data between in-memory data structures and
 different formats: CSV and text files, Microsoft Excel, SQL databases, and the
 fast HDF5 format; 
**** Intelligent data alignment and integrated handling of missing data: gain
 automatic label-based alignment in computations and easily manipulate messy
 data into an orderly form; 
**** Flexible reshaping and pivoting of data sets; 
**** Intelligent label-based slicing, fancy indexing, and subsetting of large data
 sets; 
**** Columns can be inserted and deleted from data structures for size mutability; 
**** Aggregating or transforming data with a powerful group by engine allowing
 split-apply-combine operations on data sets; 
**** High performance merging and joining of data sets; 
**** Hierarchical axis indexing provides an intuitive way of working with
 high-dimensional data in a lower-dimensional data structure; 
**** Time series-functionality: date range generation and frequency conversion,
 moving window statistics, moving window linear regressions, date shifting and
 lagging. Even create domain-specific time offsets and join time series without
 losing data; 
**** Highly optimized for performance, with critical code paths written in Cython or
 C. 
**** Python with pandas is in use in a wide variety of academic and commercial
 domains, including Finance, Neuroscience, Economics, Statistics, Advertising,
 Web Analytics, and more. 


***  如何使用?

*** 运用场景?
    每本书都有其运用场景，这本书不是以一种普通学者的方式教你
    而是让你能够` 加载点儿数据，做点计算，再画点儿图` 

    关注处理大数据集高性能数组计算工具

    常常需要把乱七八糟的数据进行结构化（处理成漂亮点的结构数据）
*** 常用函数？

  注意点
    
*** 软件安装
pip install ..
1. numpy
2. scipy
3. matplotlib
4. imageio
5. jupyter
   1. pyqt5
6. requests
IPython 
Jupyter内核的Ipython  http://ipython.org/
    https://jupyter.org/

    Jupeter notebook
https://jupyter.readthedocs.io/en/latest/install.html
    #+BEGIN_SRC python
      pip install jupyter
    #+END_SRC
于共存我弄好了，我想了想还是说说吧。 
将两个文件夹都放到环境变量中，同时不要用 PYTHONHOME 这个变量值。最后，我把 python3.4 的文件夹中的 python.exe 重命名为 python3.exe 了，这样在命令行可以直接以 python 与 python3 分别调用。 

#+BEGIN_SRC python
  四个环境变量配置好
  1.c:\Python27
  2.c:\Python27\Scripts
  3.c:\Python37
  4.c:\Python37\Scripts

  进入python3.7安装目录。找到python.exe程序，把它重命名为python3.exe

  pip的问题 
  两个python版本分别安装了pip以后怎么区分它们。进入python安装路径找到Scripts文件夹，进入里面找到pip*-script.py，打开修改第一句为你要指定的那个python解释器

  如果使用版本3就执行pip3 (这是python2没有的)
#+END_SRC

*** jupyter notebook
*****  启动notebook   

#+BEGIN_SRC python
  jupyter notebook  # default port 8888

  jupyter notebook *.ipynb

  jupyter notebook --port 7194


#+END_SRC

***** 启动qtconcole



#+BEGIN_SRC python
  jupyter console
  jupyter qtconsole  # need pyqt5
#+END_SRC


***** 安装jupyter拓展


http://www.elecfans.com/d/650252.html

#+BEGIN_SRC python
  pip install jupyter_nbextensions_configurator jupyter_contrib_nbextensions

# 目的 download插件文件
  jupyter contrib nbextension install --user
# 使插件生效
  jupyter nbextensions_configurator enable --user


#+END_SRC


1. Notify
2. Collapsible headings
3. code folding
4. tqdm botebok
5. table of contents
6. debug

https://github.com/ipython-contrib/jupyter_contrib_nbextensions 

python最佳实战(可阅读版本)
https://pythonguidecn.readthedocs.io/zh/latest/

https://docs.python-guide.org/




直接使用 http://localhost:8888/jupyter/nbextensions 打开拓展页面
或者直接通过命令行安装
比如

到c://Python37/lib/site_packages//jupyter_nbextensions_configurator//nbextensions// 目录下找到拓展
  jupyter nbextensions_configurator enable zenmode/main (main代表main.js)

  jupyter nbextensions_configurator enable notify/notify (notify代表notify.js)
  

C:\Python37\Lib\site-packages\jupyter_contrib_nbextensions\nbextensions


当拷贝你的python37目录到其他电脑，记得重新下载和enable一下
这样才能使用  http://localhost:8888/nbextensions
链接。
***** 装载文件和运行文件



#+BEGIN_SRC python

  %load *.py
  %run *.py
#+END_SRC


***** jupyter 代码snippets

在cmd运行

#+BEGIN_SRC bash

  jupyter --data-dir
#+END_SRC


    C:\Users\yzl\AppData\Roaming\jupyter\nbextensions
    
找到snippets文件夹修改对应的snippets.json文件即可

所以这个文件夹也是挺重要的哈！得备份！
  在该文件进行编辑


#+BEGIN_SRC js

    {
        "snippets" : [
            {
                "name" : "example",
                "code" : [
                    "# This is an example snippet!",
                    "# To create your own, add a new snippet block to the",
                    "# snippets.json file in your jupyter nbextensions directory:",
                    "# /nbextensions/snippets/snippets.json",
                    "import this"
                ]
            }
          {
          
                "name" : "Newer imports",
                "code" : [
                    "import numpy as np",
                    "import matplotlib as mpl",
                    "print('spam')"
                ]
           }
        ]
    }
#+END_SRC

可能需要重启一下 jupyter notebook,  不但添加{}块(注意在不同的blocks之间使用逗号进行分割)，定义name和code即可 每行语句通过双引号包裹，
并放入中括号匿名数组中。



https://github.com/ipython-contrib/jupyter_contrib_nbextensions/tree/master/src/jupyter_contrib_nbextensions/nbextensions/snippets


*** Python Programming 网格
:LOGBOOK:
CLOCK: [2018-10-23 周二 22:48]--[2018-10-23 周二 22:49] =>  0:01
:END:
https://pythonprogramming.net/using-pandas-structure-process-data/

https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000

https://www.liaoxuefeng.com

http://www.newthinktank.com/2014/11/python-programming/


https://www.youtube.com/watch?v=N4mEzFDjqtA

Learning
https://www.youtube.com/watch?v=rfscVS0vtbw

input get the characters from input keyboards

*** pandas_datareader

pandas主要处理的是data formats or file formats,虽然他能处理
许多data types. 但是很多人在说道pandas的“Data Type”一般是指
data format。


Location of python install might not be the same,
make sure you use pandas_datareader instead of pandas.oi.data 
and ensure that you have it installed - pip install pandas-datareader.
 other than that the code should work.
*** TODO [#B] 流畅的python <2018-10-31 周三 20:16 +7d>                :python:
DEADLINE: <2018-12-10 周一> SCHEDULED: <2018-11-01 周四>
                    :PROPERTIES:
                    :Effort: 2:00
                    :END:
                    :LOGBOOK:
                    CLOCK: [2018-11-30 周五 14:48]--[2018-11-30 周五 18:57] =>  4:09
                    - Clocked out on T:[2018-11-30 周五 18:57] \\
                      数组、元组、字典
                    CLOCK: [2018-11-01 周四 17:19]--[2018-11-02 周五 00:13] =>  6:54
                    - Clocked out on T:[2018-11-02 周五 00:13] \\
                      pandas,datetime, string模块等学习
                    CLOCK: [2018-10-31 周三 20:16]--[2018-10-31 周三 20:17] =>  0:01
                    :END:
                    
                    
                    
花一段时间学习，据说这本书不错

http://python.jobbole.com/88735/

FluentPython源码阅读
https://github.com/fluentpython/example-code


有经验的程序员experienced programmers(bend python)

***** Python data model
: understand how special methods are the key to the consistent behavior of objects
***** Data structures 
: take full advantage of built-in types, and understand the text vs bytes duality in the Unicode age
***** Functions as objects
: view Python functions as first-class objects, and understand how this affects popular design patterns
***** Object-oriented idioms
: build classes by learning about references, mutability, interfaces, operator overloading,
: and multiple inheritance
***** Control flow
: leverage context managers, generators, coroutines, and concurrency with the concurrent.
: futures and asyncio packages
***** Metaprogramming
: understand how properties, attribute descriptors, class decorators, and metaclasses work

python-pandas
https://github.com/paulQuei/pandas_tutorial



新的学习平台
https://www.safaribooksonline.com/videos/python-for-beginners/9781789617122/9781789617122-video11_2


****** chapter1. Course Overview
0. Section Overview
****** chapter2. Python setup(different platform)
0. Section Overview
****** chapter3. String and variabls
0. Section Overview
****** chapter4. numbers and math
0. Section Overview
****** chapter5. booleans and conditions
0. Section Overview

x>6 and y<10

****** chapter6. functions
  DRY= Don't repeat yourself(write one time,use many place，times)
0. Section Overview
  **** Functions,Part I
  **** Functions,Part II
****** chapter7. lists
0. Section Overview
1. Exception handling
2. Sorting and Ranges
3. section summary

****** Chapter8. Dictionaries
0. Section Overview
  **** Dictionaries,Part I
  **** Dictionaries,Part II
****** Chapter9. Tuples
****** Chapter10. Files
0. Section Overview
  **** Files,Part I
  **** Files,Part II
  1. Section summary

****** Chapter11. Modules


dir(module_name) 有用哈


还是需要进一步沉稳！

python的所有东西都是对象。同时也需要结合函数式编程方式，来学习面向对象编程。

python使用手册
https://docs.python.org/3.7/contents.html

*** pyCharm快捷键使用
Alt+左右箭头，代表切换tab
Alt+上下箭头，代表切换上下函数定义(切换快的作用)


在学习的时候特别适合这种方式:
问题：在pycharm中点击run运行程序，发现没有打开run窗口，而是打开的Python console窗口。
解决方法：打开菜单栏run->edit configurations，把下图中的复选框取消(复选框的意义是说run with Python Console)就可以了。

或者  runfile('learnPython.py')


`Ctrl+反引号`代表更换主题
`Alt+Shift+C` 查看最近的改变文件
*** 安装python-mode for  vim
https://github.com/python-mode/python-mode#troubleshootingdebugging
cd ~/.vim/pack/foo/start
git clone https://github.com/python-mode/python-mode.git
cd python-mode
git submodule update --init --recursive
https://docs.python.org/3/reference/datamodel.html

python不错的菜鸟教程
http://www.runoob.com/python/python-exceptions.html

1. pandas read_csv忽略指定行


#+BEGIN_SRC python
pd.read_csv('test.csv', sep='|', skiprows=range(1, 10))
#+END_SRC


https://cloud.tencent.com/developer/ask/188794

2. pandas read_csv指定分隔符

https://blog.csdn.net/sinat_29957455/article/details/79054126

#+BEGIN_SRC python
data = pd.read_csv(path+"point-1.0-1.0r.out",sep=" ",skiprows=range(1,2))
#+END_SRC

3. pandas 信息整理

https://blog.csdn.net/abcdrachel/article/details/80362623


#+BEGIN_SRC python
import pandas as pd
# 读取文件
# data = pd.read_csv("E:\\working\\2018_5_9\\data\\Cl_data\\grids.csv",header = None)
data = pd.read_excel("E:\\working\\2018_5_9\\data\\Cl_data\\grids.xlsx",header = None)
# 写入文件
data1 = data[data[0]<60000]
data1.to_excel("data1.xlsx",index = False)
data2 = data[(data[0]>= 60000) & (data[0] <120000) ]
data2.to_excel("data2.xlsx",index = False)
data3= data[data[0]>=120000]
data3.to_excel('data3.xlsx',index = False)
#+END_SRC


4. pandas数据压缩

http://www.cnblogs.com/dev-liu/p/pandas_2.html

5. os获取文件名

#+BEGIN_SRC python
使用到的函数有: 
os.path.splitext():分离文件名与扩展名

os.path.splitext(file)[0] 获得文件名

os.path.splitext(file)[1] 获得文件扩展名
#+END_SRC

https://www.jianshu.com/p/1a787ff721ba

6. python  the differences between split and splittext

http://www.cnblogs.com/jielongAI/p/9323257.html

7. 写入数据到不同的excel sheets中
https://blog.csdn.net/F229338596/article/details/80475329

新版本得通过openpyxl.reader.excel导入库

pip3 install openpyxl
#+BEGIN_SRC python
    from openpyxl.reader.excel import load_workbook
  #enter code here
  # dataframe: 需要写入excel的数据
  # outfile：输出的文件地址
  # your_sheet_name: 单独的sheet的文件名称
  def excelAddSheet(dataframe, outfile, your_sheet_name):
      writer = pd.ExcelWriter(outfile, enginge='openpyxl')
      if os.path.exists(outfile) != True:
          dataframe.to_excel(writer, your_sheet_name, index=None)
      else:
          book = load_workbook(writer.path)
          writer.book = book
          dataframe.to_excel(excel_writer=writer, sheet_name = your_sheet_name, index=None)
      writer.save()
      writer.close()
#+END_SRC

8. python字符串截取功能

在python中没有类似sub()或者subString()的方法，但是字符串的截取操作却是更加简单。

只需要把字符串看作是一个字符数组，截取子串非常方便。

多余的话就不啰嗦了，看下面的例子就明白了。


#+BEGIN_SRC python
str = ’0123456789′
print str[0:3] #截取第一位到第三位的字符
print str[:] #截取字符串的全部字符
print str[6:] #截取第七个字符到结尾
print str[:-3] #截取从头开始到倒数第三个字符之前
print str[2] #截取第三个字符
print str[-1] #截取倒数第一个字符
print str[::-1] #创造一个与原字符串顺序相反的字符串
print str[-3:-1] #截取倒数第三位与倒数第一位之前的字符
print str[-3:] #截取倒数第三位到结尾
print str[:-5:-3] #逆序截取，具体啥意思没搞明白？
#+END_SRC

9. 最后的礼物
   

#+BEGIN_SRC python

  import pandas as pd
  import os
  import matplotlib.pyplot as plt

  from openpyxl.reader.excel import load_workbook
  def file_list(dirname, ext='.out'):
      """获取目录下所有特定后缀的文件
      @param dirname: str 目录的完整路径
      @param ext: str 后缀名, 以点号开头
      @return: list(str) 所有子文件名(不包含路径)组成的列表
      """
      return list(filter(
          lambda filename: os.path.splitext(filename)[1] == ext,
          os.listdir(dirname)))

  # pip3 install openpyxl
  #enter code here
  # dataframe: 需要写入excel的数据
  # outfile：输出的文件地址
  # your_sheet_name: 单独的sheet的文件名称
  def excelAddSheet(dataframe, outfile, your_sheet_name):
      writer = pd.ExcelWriter(outfile, enginge='openpyxl')
      if os.path.exists(outfile) != True:
          dataframe.to_excel(writer, your_sheet_name, index=True)
      else:
          book = load_workbook(writer.path)
          writer.book = book
          dataframe.to_excel(excel_writer=writer, sheet_name = your_sheet_name, index=True)
      writer.save()
      writer.close()
  ,#+END_SRC

  processing=['M:\\fluentYaw0','M:\\fluentYaw5','M:\\fluentYaw15','M:\\fluentYaw20',
              'M:\\fluentYaw30','P:\\NacelleYaw\\NacelleYaw','P:\\NacelleYaw\\NacelleYaw\\20181102Newer',
              'P:\\NacelleYaw\\NacelleYaw\\20181102Newer\\19.1']
  for path in processing:
      #path = 'M:\\fluentYaw10\\'    # 设置文件路径
      #df = pd.read_csv(path + '\\data\\ex1.csv')  # 自动识别csv文件的分隔符
      #https://blog.csdn.net/sinat_29957455/article/details/79054126
      folds=path.split("\\")
      print(folds[folds.__len__()-1])
      fileList=file_list(path,'.out')
      for fileTemp in fileList:
          words=fileTemp.split("-")
          data = pd.read_csv(path+"\\"+fileTemp,sep=" ",skiprows=range(1,2))
          print(words[1]," ", words[2][:-4])
          excelAddSheet(data,path+"\\"+folds[1]+"-velocitySeries.xlsx",words[1]+"-"+words[2][:-4])

      #    data.to_excel(path+"pp.xls",sheet_name=words[1]+"-"+words[2],index=False)

  #print(data)
#+END_SRC

pandas厉害之处 6百万excel数据读入没问题, 听说亿级的也没问题。
https://python.freelycode.com/contribution/detail/38

*  第一、二章 准备与例子
:LOGBOOK:
CLOCK: [2018-11-23 周五 13:11]--[2018-11-23 周五 14:14] =>  1:03
- Clocked out on T:[2018-11-23 周五 14:14] \\
  help(json.loads)
CLOCK: [2018-11-23 周五 12:21]--[2018-11-23 周五 12:57] =>  0:36
- Clocked out on T:[2018-11-23 周五 12:57] \\
  二维数组用法
:END:
第一章 准备工作
今天开始码这本书--《利用python进行数据分析》。R和python都得会用才行，
这是码这本书的原因。首先按照书上说的进行安装，google下载了epd_free-7.3-1-win-x86.msi，译者建议按照作者的版本安装,EPDFree包括了 Numpy,Scipy,matplotlib,Chaco,IPython.这里的pandas需要自己安装，对应版本为pandas-0.9.0.win32-py2.7.exe.数据下载地址：github.com/pydata/pydata-book.下面是一个文档：
Welcome to Python For Data Analysis’s documentation!
http://pda.readthedocs.org/en/latest/
第二章 引言
本章是一些例子。
1、来自bit.ly的1.usa.gov数据
首先，遇到的问题是pycharm的中文编码问题，注意IDEencoding改为utf-8，
同时文件最开始加#-*- encoding:utf-8 -*-，同时含有中文的字符串记得加u。
下面是代码： 


#+BEGIN_SRC python
# -*- encoding: utf-8 -*-
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import defaultdict
from collections import Counter
#注意这里的中文路径
path = u'D:\\你好\\usagov_bitly_data2012-03-16-1331923249.txt'
print open(path).readline()

#注意这里的json模块中的loads函数将字符串转换为字典，非常有用！
#注意这里的缩略循环形式
records = [json.loads(line) for line in open(path)]

print records[0]
print type(records)
print type(records[0])
print records[0]['tz']

#注意这里的判断条件
time_zones = [rec['tz'] for rec in records if 'tz' in rec]
print time_zones[:10]

#下面定义函数对时区进行计数统计,注意这里计数的方式,注意这里的字典初始化方式
def get_counts(squence):
    counts = defaultdict(int)
    for x in squence:
        counts[x] += 1
    return counts

counts = get_counts(time_zones)
print counts['America/New_York']
def top_counts(count_dict,n = 10):
    value_key_pairs = [(count,tz) for tz,count in count_dict.items()]
    value_key_pairs.sort()
    #请注意这里的索引方式，很好
    return value_key_pairs[-n:]
#这里是打印最后面的十个数，值得注意的是从倒数第十个开始一直到最后一个
print top_counts(counts)
#这里的Counter是一个神器，作者真实强大
counts = Counter(time_zones)

print counts.most_common(10)

#+END_SRC

上面是利用python标准库中的函数进行数据分析。需要注意的几个方面：
1、关于列表索引的说明：

a = range(0,10,1)
则

#+BEGIN_SRC python
a[0] >>>0
a[-1] >>> 9
a[:5] >>> [0,1,2,3,4]
a[0:2] >>> [0,1]
a[-3:-1] >>> [7,8]
a[-3:] >>> [7,8,9]
a[-1:-3:-1] >>> [9,8]
a[::2] >>> [0,2,4,6,8]
#+END_SRC

说明：
1、索引包含第一个，不包含第一个冒号后面的部分
2、符号表示从后面开始计数
3、第二个冒号后面是间隔，如果有负号，表示从后面开始计数,例如a[-1:-3]这种表示方式得到空列表。

2、关于模块 collections 的应用，见下面的地址：
http://www.zlovezl.cn/articles/collections-in-python/
collections 主要包括下面几个“数据类型”：namedtuple() 生成可以使用名字来访问元素内容的tuple子类；deque()双端队列,它最大的好处就是实现了从队列 头部快速增加和取出对象;Counter用来统计个数，字典、列表、字符串都能用，很方便；OrderedDict 生成有序字典；defaultdict 也有用 比如 defaultdict(int) 表示字典中每个值都是int型，defaultdict(list)表示字典每个值都是列表。更多更详细的内容见：
https://docs.python.org/2/library/collections.html#module-collections。
下面是用pandas对时区进行计数
DataFrame是Pandas最重要的数据结构，应该就是R语言中的数据框。下面看一下实现方式：

#+BEGIN_SRC python

# -*- encoding: utf-8 -*-
import json
import numpy as np
import pandas as pd
from pandas import DataFrame,Series
import matplotlib.pyplot as plt

#注意这里的中文路径
path = u'D:\\你好\\usagov_bitly_data2012-03-16-1331923249.txt'
#注意这里的json模块中的loads函数将字符串转换为字典，非常有用！
#注意这里的缩略循环形式
records = [json.loads(line) for line in open(path)]
#注意这里的DataFrame可以将每个元素都是字典的列表自动整理为数据框的形式，每一列是字典的key
frame = DataFrame(records)
#数据太多只是会显示缩略图
#print frame
#下面是列名为tz的前十个元素
#print frame['tz'][:10]
#下面是用value_counts方法对不同的tz计数,太方便了！
#print type(frame['tz'])
tz_counts = frame['tz'].value_counts()
#print tz_counts[:10]
#下面想画一个茎叶图，首先将缺失值NA进行填充
clean_tz = frame['tz'].fillna('Missing')
#下面是对空白符通过布尔型数组索引加以替换
#值得注意的是，空白符和NA缺失值是不一样的，跟R中道理一样
clean_tz[clean_tz  == ''] = 'Unknown'
tz_counts = clean_tz.value_counts()
print tz_counts[:10]
#书上说下面这条语句在pylab中打开才管用,其实加一句plt.show()就可以了
tz_counts[:10].plot(kind = 'barh',rot = 0)
plt.show()
#+END_SRC


下面是对数据中的字符串和表达式之类的进行的工作（前些日子经人指点Beautiful Soup是个爬虫包）：


#+BEGIN_SRC python
# -*- encoding: utf-8 -*-
import json
import numpy as np
import pandas as pd
from pandas import DataFrame,Series
import matplotlib.pyplot as plt
from collections import defaultdict
from collections import Counter

#注意这里的中文路径
path = u'D:\\你好\\usagov_bitly_data2012-03-16-1331923249.txt'
#print open(path).readline()
#注意这里的json模块中的loads函数将字符串转换为字典，非常有用！
#注意这里的缩略循环形式
records = [json.loads(line) for line in open(path)]
frame = DataFrame(records)
#对于一个 Series，dropna 返回一个仅含非空数据和索引值的 Series
results = Series([x.split()[0] for x in frame.a.dropna()])
#print results.value_counts()
cframe = frame[frame.a.notnull()]
#np.where函数是一个矢量化ifelse函数
operating_system = np.where(cframe['a'].str.contains('Windows'),'Windows','Not Windows')
#print operating_system[:5]
#下面是将tz按照operating_system进行分组并计数并用unstack进行展开并填充na为0
by_tz_os = cframe.groupby(['tz',operating_system])
agg_counts = by_tz_os.size().unstack().fillna(0)
#print agg_counts
#下面注意 sum函数 默认axis = 0，是普通加和，axis = 1是按行加和,argsort是从小到大排序并返回下表
indexer = agg_counts.sum(1).argsort()
#下面是取出时区最多的值，注意take函数，接下标
count_subset = agg_counts.take(indexer)[-10:]
print count_subset
#下面的图很好，是累积条形图
count_subset.plot(kind = 'barh',stacked = True)
plt.show()
#下面进行比例展示
normed_subset = count_subset.div(count_subset.sum(1),axis = 0)
normed_subset.plot(kind = 'barh',stacked = True)
plt.show()

#+END_SRC

上面一个例子已经完成，看下一个例子。
GroupLens Research 采集了一组从20世纪90年代末到21世纪初由MovieLens用户提供的电影评分数据。这里的目的在于对数据进行切片分析。


#+BEGIN_SRC python
#-*-coding:utf-8-*-
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

path1 = 'E:\\Pyprojects\\usepython_2.2\\movielens\\users.dat'
path2 = 'E:\\Pyprojects\\usepython_2.2\\movielens\\ratings.dat'
path3 = 'E:\\Pyprojects\\usepython_2.2\\movielens\\movies.dat'

unames = ['user_id','gender','age','occupation','zip']
users = pd.read_table(path1,sep = '::',header = None,names = unames)

rnames = ['user_id','movie_id','rating','timestamp']
ratings = pd.read_table(path2,sep = '::',header = None,names = rnames)

mnames = ['movie_id','title','genres']
movies = pd.read_table(path3,sep = '::',header = None,names = mnames)

#print users.head()

#下面是对三个数据集合进行merge操作，最终的行数由ratings决定，原因显然
data  = pd.merge(pd.merge(ratings,users),movies)
#print data.ix[0]
#下面按照性别计算每部电影的平均得分,说实话，这个透视表函数还真是通俗易懂
mean_ratings = data.pivot_table('rating',rows = 'title',cols = 'gender',aggfunc = 'mean')
#print mean_ratings.head()
#下面是按照title对data分组并计数
ratings_by_title = data.groupby('title').size()
#下面的index返回的下标
active_titles = ratings_by_title.index[ratings_by_title >= 251]
#下面之所以可以这样做是因为groupby函数和透视表都是按照相同是顺序排序的
mean_ratings = mean_ratings.ix[active_titles]
#print mean_ratings
top_female_ratings = mean_ratings.sort_index(by = 'F',ascending = False)
#print top_female_ratings.head()
#下面一部分计算男性和女性分歧最大的电影
#注意，下面的语句直接加入了一列diff,这样得到的就是女性最喜欢的电影，注意方法sort_index的应用
mean_ratings['diff'] = mean_ratings['M'] - mean_ratings['F']
sorted_by_diff = mean_ratings.sort_index(by = 'diff')
#下面是对数据框的行反序并取出前15行，但是 如何对行反序呢？(哦，就是按照原来的行的反向就行)
#print sorted_by_diff[::-1][:15]
#下面考虑分歧最大的电影，不考虑性别因素
rating_std_by_title = data.groupby('title')['rating'].std()
rating_std_by_title = rating_std_by_title.ix[active_titles]
#对Series对象进行排序，需要用order
print rating_std_by_title.order(ascending=False)[:10]
#+END_SRC


上面的例子中，有不少需要注意的地方，信息量比较大(对于新手)。下面的例子内容更多一些：


#+BEGIN_SRC python
# -*- encoding: utf-8 -*-
import json
import numpy as np
import pandas as pd
from pandas import DataFrame,Series
import matplotlib.pyplot as plt
from collections import defaultdict
from collections import Counter

path_base = u'E:\\BaiduYun\\计算机\\python\\利用python进行数据分析\\pydata-book-master\ch02\\names\\'

#下面读入多个文件到同一个DataFrame中
years = range(1880,2011)
pices = []
columns = ['name','sex','births']
for year in years:
    path = path_base + 'yob%d.txt' % year
    frame = pd.read_csv(path,names=columns)

    frame['year'] = year
    pices.append(frame)
    break
#注意pd.concat是默认按行进行的合并，是一种outer外连接，按照索引作为连接键 
names = pd.concat(pices,ignore_index=True)
#下面进行一下聚合,注意这里的pivot_table真是太有用了！
total_births = names.pivot_table('births',rows = 'year',cols = 'sex',aggfunc=sum)
#print total_births.tail()
#total_births.plot(title = 'Total births by sex and year')
#3plt.show()
#下面要插入一列，出生量占总出生量的比例
def add_prop(group):
    #下面将数据换为float类型
    births =group.births.astype(float)

    group['prop'] = births / births.sum()
    return group
names = names.groupby(['year','sex']).apply(add_prop)
#下面对prop列进行加和看是不是等于1，由于是浮点型数据，用的是allclose函数,判断是否和1足够接近
#print np.allclose(names.groupby(['year','sex']).prop.sum(),1)
#现在要取一个子集，是每一个‘year’‘sex’对的出生量前1000名

def get_top1000(group):
    return group.sort_index(by = 'births',ascending=False)[:1000]
grouped = names.groupby(['year','sex'])
top1000 = grouped.apply(get_top1000)
#print top1000.head()
#+END_SRC


下面是把后半部分补充完整：


#+BEGIN_SRC python
# -*- encoding: utf-8 -*-
import os
import json
import numpy as np
import pandas as pd
from pandas import DataFrame,Series
import matplotlib.pyplot as plt

path_base = u'D:\\pydata-book-master\\ch02\\names\\'

#下面读入多个文件到同一个DataFrame中

years = range(1880,2011)
pices = []
columns = ['name','sex','births']
for year in years:
    path = path_base + 'yob%d.txt' % year
    frame = pd.read_csv(path,names=columns)
    frame['year'] = year
    pices.append(frame)

#注意pd.concat是默认按行进行的合并，是一种outer外连接，按照索引作为连接键
names = pd.concat(pices,ignore_index=True)
#下面进行一下聚合,注意这里的pivot_table真是太有用了！

total_births = names.pivot_table('births',rows = 'year',cols = 'sex',aggfunc=sum)
#print total_births.tail()
#total_births.plot(title = 'Total births by sex and year')
#3plt.show()
#下面要插入一列，出生量占总出生量的比例
def add_prop(group):
    #下面将数据换为float类型
    births =group.births.astype(float)
    group['prop'] = births / births.sum()
    return group
names = names.groupby(['year','sex']).apply(add_prop)
#下面对prop列进行加和看是不是等于1，由于是浮点型数据，用的是allclose函数,判断是否和1足够接近
#print np.allclose(names.groupby(['year','sex']).prop.sum(),1)
#现在要取一个子集，是每一个‘year’‘sex’对的出生量前1000名

def get_top1000(group):
    return group.sort_index(by = 'births',ascending=False)[:1000]
grouped = names.groupby(['year','sex'])
top1000 = grouped.apply(get_top1000)

#print top1000.head()
#下面是分析命名趋势
boys = top1000[top1000.sex == 'M']
girls = top1000[top1000.sex == 'F']
#下面做一个透视表
total_births = top1000.pivot_table('births',rows = 'year',cols = 'name',aggfunc = sum)

subset = total_births[['John','Harry','Mary','Marilyn']]
#下面的subplots是用来标明是否将几个图画在一起,figsize用来标明大小，grid 是标明是否有网格线
#subset.plot(subplots = True,figsize = (12,10),grid = True,title = 'Number of births per year')
#plt.show()

#下面评估明明多样性的增长，计算最流行的1000个名字所占的比例
#table = top1000.pivot_table('prop',rows = 'year',cols = 'sex',aggfunc = sum)
#table.plot(title = 'Sum of table1000.prop by year and sex',yticks = np.linspace(0,1.2,13),xticks = range(1880,2020,10))
#plt.show()
#另一个方式是计算总出生人数前50%的不同名字的数量
#df = boys[boys.year == 2010]
#下面就要找到prop的和是0.5的位置，书上说写循环也行，但是numpy中也有cunsum函数，R语言中也有，这当然是极好的。

#prop_cumsum = df.sort_index(by = 'prop',ascending = False).prop.cumsum()
#print prop_cumsum[:10]
#下面这个函数简直太方便，searchsorted
#print prop_cumsum.searchsorted(0.5)
#注意下面的函数，将所有的年份都进行一次计算
def get_quantile_count(group,q = 0.5):
    group = group.sort_index(by = 'prop',ascending= False)
    return group.prop.cumsum().searchsorted(q) + 1

diversity = top1000.groupby(['year','sex']).apply(get_quantile_count)
diversity = diversity.unstack('sex')
#print diversity.head()
diversity.plot(title = 'Number of popular names in top 50%')
plt.show()



#最后一个字母的变革
#从name列取出最后一个字母,注意lamda这个语句使用来创建匿名函数
get_last_letter = lambda x:x[-1]
#注意这里的map函数是一种 “并行”式的函数，对name的每个元素进行后面的函数
last_letters = names.name.map(get_last_letter)
last_letters.name = 'last_letter'
#下面的语句让我感到了奇怪，为何last_latters不在names中却还能顺利生成数据透视表？毁三观呐
table = names.pivot_table('births',rows = last_letters,cols = ['sex','year'],aggfunc = sum)

subtable = table.reindex(columns = [1910,1960,2010],level = 'year')
#print subtable.head()
letter_prop = subtable / subtable.sum().astype(float)
fig,axes = plt.subplots(2,1,figsize=(10,8))
letter_prop['M'].plot(kind = 'bar',rot = 0,ax = axes[0],title = 'Male')
letter_prop['F'].plot(kind = 'bar',rot = 0,ax = axes[1],title = 'Female',legend = False)
plt.show()

letter_prop = table / table.sum().astype(float)
dny_ts = letter_prop.ix[['d','n','y'],'M'].T
dny_ts.plot()
plt.show()

#下面是最后一项，变成女孩名字的男孩名字（以及相反的情况）
all_names = top1000.name.unique()
#这里的in函数应该是一个部分匹配函数，另外上面的语句中的unique很熟悉，R语言中也有
mask = np.array(['lesl' in x.lower() for x in all_names])
lesley_like = all_names[mask]
#然后用这个结果过滤掉其他的名字，并按名字分组计算出生数以查看相对频率
#下面这个isin函数非常方便
flitered = top1000[top1000.name.isin(lesley_like)]
flitered.groupby('name').births.sum()
table = flitered.pivot_table('births',rows = 'year',cols = 'sex',aggfunc = 'sum')
#print table.head()
#注意这里的div函数是做一个归一化
table = table.div(table.sum(1),axis = 0)
print table.head()
#print table.tail()
table.plot(style = {'M':'k-','F':'k--'})
plt.show()

#+END_SRC
*  第四章 numpy基础：数组和矢量计算

得补充几张图片

第一部分：numpy的ndarray:一种多维数组对象  
实话说，用numpy的主要目的在于应用矢量化运算。Numpy并没有多么高级的数据分析功能，理解Numpy和面向数组的计算能有助于理解后面的pandas.按照课本的说法，作者关心的功能主要集中于：
用于数据整理和清理、子集构造和过滤、转换等快速的矢量化运算
常用的数组解法，如排序、唯一化、集合运算等
高效的描述统计和数据聚合/摘要运算
用于异构数据集的合并/连接运算的数据对齐和关系型数据运算
将条件逻辑表述为数组表达式（而不是带有if-elif-else分支的循环）
数据的分组运算（聚合、转换、函数应用等）。
作者说了，可能还是pandas更好一些，我感觉显然pandas更高级，其中的函数真是太方便了，数据框才是最好的数据结构。只是，Numpy中的函数之类的是基础，需要熟悉。
NumPy的ndarray：一种多维数组对象
ndarray对象是NumPy最重要的对象，特点是矢量化。ndarray每个元素的数据类型必须相同，每个数组有两个属性：shape和dtype.


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

data = [[1,2,5.6],[21,4,2]]
data = np.array(data)
print data.shape
print data.dtype
print data.ndim
>>>
(2, 3)
float64
2
#+END_SRC


array函数接受一切序列型的对象（包括其他数组），然后产生新的含有传入数据的NumPy数组，array会自动推断出一个合适的数据类型。还有一个方法是ndim：这个翻译过来叫维度，标明数据的维度。上面的例子是两维的。zeros和ones可以创建指定长度或形状全为0或1的数组。empty可以创建一个没有任何具体值的数组，arange函数是python内置函数range的数组版本。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

data = [[1,2,5.6],[21,4,2],[2,5,3]]
data1 = [[2,3,4],[5,6,7,3]]
data = np.array(data)
data1 = np.array(data1)

arr1 = np.zeros(10)
arr2 = np.ones((2,3))
arr3 = np.empty((2,3,4))

print arr1
print arr2
print arr3
print arr3.ndim
>>>
[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
[[ 1.  1.  1.]
 [ 1.  1.  1.]]
[[[  3.83889007e-321   0.00000000e+000   0.00000000e+000   0.00000000e+000]
  [  0.00000000e+000   0.00000000e+000   0.00000000e+000   0.00000000e+000]
  [  0.00000000e+000   0.00000000e+000   0.00000000e+000   0.00000000e+000]]
[[  0.00000000e+000   0.00000000e+000   0.00000000e+000   0.00000000e+000]
  [  0.00000000e+000   0.00000000e+000   0.00000000e+000   0.00000000e+000]
  [  0.00000000e+000   0.00000000e+000   0.00000000e+000   0.00000000e+000]]]
3
#+END_SRC




上面是常用的生成数组的函数。
ndarray的数据类型
dtype（数据类型）是一个特殊的对象。它含有ndarray将一块内存解释为指定数据类型所需的信息。他是NumPy如此灵活和强大的原因之一。多数情况下，它们直接映射到相应的机器表示，这使得“读写磁盘上的二进制数据流”以及“集成低级语言代码（C\Fortran）”等工作变得更加简单。dtype命名方式为，类型名+表示元素位长的数字。标准双精度浮点型数据需要占用8字节（64位）。记作float64.常见的数据类型为：


我终于找到了f4,f8的含义了……布尔型数据的代码倒是很有个性。函数astype可以强制转换数据类型。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

arr = np.array([1,2,3,4,5])
print arr.dtype
float_arr = arr.astype(np.float64)
print float_arr.dtype

arr1 = np.array([2.3,4.2,32.3,4.5])
#浮点型会被整型截断
print arr1.astype(np.int32)
#一个全是数字的字符串也可以转换为数值类型
arr2 = np.array(['2323.2','23'])
print arr2.astype(float)

#数组的dtype还有一个用法
int_array = np.arange(10)
calibers = np.array([.22,.270,.357,.44,.50],dtype = np.float64)
print int_array.astype(calibers.dtype)
print np.empty(10,'u4')
#+END_SRC


    调用astype总会创建一个新的数组（原始数组的一个拷贝），即使和原来的数据类型相同。警告：浮点数只能表示近似数，比较小数的时候要注意。
    数组与标量之间的运算
    矢量化（vectorization）是数组最重要的特点了。可以避免（显示）循环。注意加减乘除的向量化运算。不同大小的数组之间的运算叫广播（broadcasting）。
    索引和切片，不再赘述，注意的是 广播的存在使得数组即使只赋一个值也会被广播到所有数组元素上，其实和R语言中自动补齐功能相同。下面的性质有点蛋疼：跟列表最重要的区别在于，数组切片是原始数组的视图，对视图的任何修改都会反映到源数据上。即使是下面的情况：


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

arr = np.array([1,2,3,4,5,6,7,8,9])
arr1 = arr[1:2]
arr1[0] = 10
print arr
#如果想得到拷贝，需要显示地复制一份
arr2 = arr[3:4].copy()
arr2[0] = 10
print arr

arr2d = np.array([[1,2,3],[4,5,6],[7,8,9]])
#下面两种索引方式等价
print arr2d[0][2]
print arr2d[0,2]
print arr2d[:,1] #注意这里的方式和下面的方式
print arr2d[:,:1]

arr3d = np.array([[[1,2,3],[4,5,6]],[[7,8,9],[[10,11,12]]]])
print arr3d[(1,0)]
>>>
[ 1 10  3  4  5  6  7  8  9]
[ 1 10  3  4  5  6  7  8  9]
3
3
[2 5 8] #注意这里的方式和下面的方式
[[1]
 [4]
 [7]]
[7, 8, 9]

#+END_SRC

布尔型索引
这里的布尔型索引就是TRUE or FALSE索引。==、！=、-（表示否定）、&（并且）、|（或者）。注意布尔型索引选取数组中的数据，将创建数据的副本。python关键字and、or无效。
花式索引（Fancy indexing）
花式索引指的是利用整数数组进行索引。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

arr = np.arange(32).reshape(8,4)

print  arr
#注意这里的向量式方式
print arr[[1,5,7,2],[0,3,1,2]]
print arr[[1,5,7,2]][:,[0,3,1,2]]
#也可以使用np.ix_函数，将两个一维整数数组组成选取方形区域的索引器
print arr[np.ix_([1,5,7,2],[0,3,1,2])]
>>>
[[ 0  1  2  3]
 [ 4  5  6  7]
 [ 8  9 10 11]
 [12 13 14 15]
 [16 17 18 19]
 [20 21 22 23]
 [24 25 26 27]
 [28 29 30 31]]
[ 4 23 29 10]
[[ 4  7  5  6]
 [20 23 21 22]
 [28 31 29 30]
 [ 8 11  9 10]]
[[ 4  7  5  6]
 [20 23 21 22]
 [28 31 29 30]
 [ 8 11  9 10]]

#+END_SRC

花式索引总是将数据复制到新数组中，跟切片不同,一定要注意下面的区别：


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

arr = np.arange(32).reshape(8,4)
arr1 = np.arange(32).reshape(8,4)
#注意下面得到的结果是一样的
arr3 = arr[[1,2,3]][:,[0,1,2,3]]
arr3_1 = arr1[1:4][:]

#注意下面是区别了
arr3[0,1] = 100  #花式索引得到的是复制品，重新赋值以后arr不变化
arr3_1[0,1] = 100 #切片方式得到的是一个视图，重新赋值后arr1会变化

print arr3
print arr3_1
print arr
print arr1
>>>
[[  4 100   6   7]
 [  8   9  10  11]
 [ 12  13  14  15]]
[[  4 100   6   7]
 [  8   9  10  11]
 [ 12  13  14  15]]
[[ 0  1  2  3]
 [ 4  5  6  7]
 [ 8  9 10 11]
 [12 13 14 15]
 [16 17 18 19]
 [20 21 22 23]
 [24 25 26 27]
 [28 29 30 31]]
[[  0   1   2   3]
 [  4 100   6   7]
 [  8   9  10  11]
 [ 12  13  14  15]
 [ 16  17  18  19]
 [ 20  21  22  23]
 [ 24  25  26  27]
 [ 28  29  30  31]]

#+END_SRC

数组转置和轴转换
转置transpose，是一种对源数据的视图，不会进行复制。调用T就可以。np中的矩阵乘积函数为np.dot。
比较复杂的是高维数组：


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

arr = np.arange(24).reshape((2,3,4))
#下面解释一下transpose：
#（1,0,2） 是将reshape中的参数 (2,3,4) 进行变化 ，变为（3,2,4）
#但是由于是转置，所以是将所有元素的下标都进行了上述变化，比如 12这个元素，原来索引为 (1,0,0) ,现在为 (0,1,0)
arr1 = arr.transpose((1,0,2))
arr2 = arr.T #直接用T是变为了(4,3,2)的形式

#arr3 = np.arange(120).reshape((2,3,4,5))
#arr4 = arr3.T #直接用T就是将形式变为 (5,4,3,2)
#ndarray还有swapaxes方法，接受一对轴编号
arr5 = arr.swapaxes(1,2)

#print arr
#print arr1
#print arr2
#print arr3
#print arr4
print arr5

>>>
[[[ 0  4  8]
  [ 1  5  9]
  [ 2  6 10]
  [ 3  7 11]]

 [[12 16 20]
  [13 17 21]
  [14 18 22]
  [15 19 23]]]

#+END_SRC

第二部分是关于一些元素级函数：即作用于数组每个元素上的函数，用过R语言之后就觉得其实没什么了。
下面是一些常见的矢量化函数（姑且这么叫吧）。


下面是几个例子：


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-

import numpy as np
import numpy.random as npr
import pandas as pd

#接收两个数组的函数,对应值取最大值
x = npr.randn(8)
y = npr.randn(8)
#注意不是max函数
z = np.maximum(x,y)
print x,y,z

#虽然并不常见，但是一些ufunc函数的确可以返回多个数组。modf函数就是一例，用来分隔小数的整数部分和小数部分，是python中divmod的矢量化版本
arr = npr.randn(8)
print np.modf(arr)
#ceil函数取天花板，不小于这个数的最小整数
print np.ceil(arr)
#concatenate函数是将两个numpy数组连接，注意要组成元组方式再连接
#arr = np.concatenate((arr,np.array([0,0])))
#logical_not函数, 非 函数
#print np.logical_not(arr)
print np.greater(x,y)
print np.multiply(x,y)

#+END_SRC

第三部分：利用数组进行数据处理
作者说矢量化数组运算比纯pyhton方式快1-2个数量级（or more）,又一次强调了broadcasting作用很强大。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


#假设想在一个二维网格上计算一个 sqrt(x^2 + y^2)
#生成-5到5的网格，间隔0.01
points = np.arange(-5,5,0.01)
#meshgrid返回两个二维矩阵，描述出所有（-5,5）* （-5,5）的点对
xs,ys = np.meshgrid(points,points)

z = np.sqrt(xs ** 2 + ys ** 2)
#print xs
#print ys
#不做个图都对不起观众
#imshow函数，展示z是一个矩阵，cmap就是colormap，用的时候值得研究
plt.imshow(z,cmap=plt.cm.gray)
plt.colorbar()
plt.title("Image plot of $\sqrt{x^2 + y^2}$ for a grid of values")
plt.show()

#+END_SRC


上面的画图语句在用的时候还需要好好研究一下。
下面的一个例子是np.where函数，简洁版本的if-else。


#+BEGIN_SRC python
#np.where函数通常用于利用已有的数组生产新的数组
arr = npr.randn(4,4)
#正值赋成2，负值为-2
print np.where(arr > 0,2,-2)
#注意这里的用法
print np.where(arr > 0,2,arr)
#可以用where表示更为复杂的逻辑表达
#两个布尔型数组cond1和cond2，4种不同的组合赋值不同
#注意：按照课本上的说法，下面的语句是从左向右运算的，不是从做内层括号计算起的；这貌似与python的语法不符
np.where(cond1 & cond2,0,np.where(cond1,1,np.where(cond2,2,3)))
#不过感觉没有更好的写法了。
#书上“投机取巧”的式子，前提是True = 1，False = 0
result = 1 * (cond1 - cond2) + 2 * (cond2 & -cond1) + 3 * -（cond1 | cond2）


#+END_SRC


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import numpy.random as npr
#值得注意的是，mean、sum这样的函数，会有一个参数axis表示对哪个维度求值
arr = np.array([[0,1,2],[3,4,5],[6,7,8]])
#cumsum不是聚合函数，维度不会减少
print arr.cumsum(0)
#+END_SRC


下面是常用的数学函数：


用于布尔型数组的方法
sum经常用于True的加和；any和all分别判断是否存在和是否全部为True。
排序及唯一化


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import numpy.random as npr

#sort函数是就地排序
arr = npr.randn(10)
print arr
arr.sort()
print arr
#多维数组可以按照维度排序，把轴编号传递给sort即可
arr = npr.randn(5,3)
print arr
#sort传入1，就是把第1轴排好序,即按列
arr.sort(1)
print arr
#np.sort返回的是排序副本，不是就地排序
#输出5%分位数
arr_npr = npr.randn(1000)
arr_npr.sort()
print arr_npr[int(0.05 * len(arr_npr))]
#pandas中有更多排序、分位数之类的函数，直接可以取分位数的，第二章的例子中就有
#numpy中有unique函数，唯一化函数，R语言中也有
names = np.array(['Bob','Joe','Will','Bob','Will'])
print sorted(set(names))
print np.unique(names)
values = np.array([6,0,0,3,2,5,6])
#in1d函数用来查看一个数组中的元素是否在另一个数组中,名字挺好玩，注意返回的长度与第一个数组相同
print np.in1d(values,[6,2,3])

#+END_SRC

下面是常用集合运算

用于数组的文件输入输出
NumPy能够读写磁盘上的文本数据或二进制数据。后面的章节将会给出一些pandas中用于将表格型数据读取到内存的工具。
np.save 和 np.load是读写磁盘数据的两个主要函数。默认情况下，数组是以未压缩的原始二进制文件格式保存在扩展名为.npy的文件中。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import numpy.random as npr
arr = np.arange(10)
np.save('some_array',arr)
np.savez('array_archive.npz',a = arr,b = arr)
arr1 = np.load('some_array.npy')
arch = np.load('array_archive.npz')
print arr1
print arch['a']
#+END_SRC

#下面是存取文本文件，pandas中的read_csv和read_table是最好的了
#有时需要用np.loadtxt或者np.genfromtxt将数据加载到普通的NumPy数组中
#这些函数有许多选项使用：指定各种分隔符，针对特定列的转换器函数，需要跳过的行数等
#np.savetxt执行的是相反的操作：将数组写到以某种分隔符隔开的文本文件中
#genfromtxt跟loadtxt差不多，只不过它面向的是结构化数组和缺失数据处理

线性代数
关于线性代数的一些函数，NumPy的linalg中有很多关于矩阵的函数，与MATLAB、R使用的是相同的行业标准级Fortran库。

随机数生成
NumPy.random模块对Python内置的random进行了补充，增加了一些用于高效生成多种概率分布的样本值的函数。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import numpy.random as npr
from random import normalvariate
#生成标准正态4*4样本数组
samples = npr.normal(size = (4,4))
print samples
#从下面的例子中看出，如果产生大量样本值，numpy.random快了不止一个数量级
N = 1000000
#xrange()虽然也是内置函数，但是它被定义成了Python里一种类型(type),这种类型就叫做xrange.
#下面的循环中，for _ in xrange(N) 非常good啊，查了一下和range的关系，两者都用于循环，但是在大型循环时，xrange好得多
%timeit samples = [normalvariate(0,1) for _ in xrange(N)]
%timeit npr.normal(size = N)

#+END_SRC



范例：随机漫步


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import random #这里的random是python内置的模块
import matplotlib.pyplot as plt

position = 0
walk = [position]
steps = 1000
for i in xrange(steps):
    step = 1 if random.randint(0,1) else -1
    position += step
    walk.append(position)
plt.plot(walk)
plt.show()

#+END_SRC


#+BEGIN_SRC python
#下面看看简单的写法
nsteps = 1000
draws = np.random.randint(0,2,size = nsteps)
steps = np.where(draws > 0,1,-1)
walk = steps.cumsum()
plt.plot(walk)
plt.show()
#argmax函数返回数组第一个最大值的索引，但是在这argmax不高效，因为它会扫描整个数组
print (np.abs(walk) >= 10).argmax()

nwalks = 5000
nsteps = 1000
draws = np.random.randint(0,2,size = (nwalks,nsteps))
steps = np.where(draws > 0,1,-1)
walks = steps.cumsum(1)
print walks.max()
print walks.min()
#这里的any后面的参数1表示每行(轴为1)是否存在true
hist30 = (np.abs(walks) >= 30).any(1)
print hist30
print hist30.sum()  #这就是有多少行超过了30
#这里argmax的参数1就是
crossing_time = (np.abs(walks[hist30]) >= 30).argmax(1)
print crossing_time.mean()
X = range(1000)
plt.plot(X,walks.T)
plt.show()

#+END_SRC


NumPy写完了，接下来写pandas.NumPy写的还好，比较顺利。
* 第五章pandas入门

pandas是本书后续内容的首选库。pandas可以满足以下需求： 
具备按轴自动或显式数据对齐功能的数据结构。这可以防止许多由于数据未对齐以及来自不同数据源（索引方式不同）的数据而导致的常见错误。.
集成时间序列功能
既能处理时间序列数据也能处理非时间序列数据的数据结构
数学运算和简约（比如对某个轴求和）可以根据不同的元数据（轴编号）执行
灵活处理缺失数据
合并及其他出现在常见数据库（例如基于SQL的）中的关系型运算
1、pandas数据结构介绍
两个数据结构：Series和DataFrame。Series是一种类似于以为NumPy数组的对象，它由一组数据（各种NumPy数据类型）和与之相关的一组数据标签（即索引）组成的。可以用index和values分别规定索引和值。如果不规定索引，会自动创建 0 到 N-1 索引。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
from pandas import Series,DataFrame

#Series可以设置index，有点像字典，用index索引
obj = Series([1,2,3],index=['a','b','c'])
#print obj['a']
#也就是说，可以用字典直接创建Series

dic = dict(key = ['a','b','c'],value = [1,2,3])
dic = Series(dic)
#下面注意可以利用一个字符串更新键值
key1 = ['a','b','c','d']
#注意下面的语句可以将 Series 对象中的值提取出来，不过要知道的字典是不能这么做提取的
dic1 = Series(obj,index = key1)
#print dic
#print dic1
#isnull 和  notnull 是用来检测缺失数据
#print pd.isnull(dic1)
#Series很重要的功能就是按照键值自动对齐功能
dic2 = Series([10,20,30,40],index = ['a','b','c','e'])
#print dic1 + dic2
#name属性,可以起名字
dic1.name = 's1'
dic1.index.name = 'key1'
#Series 的索引可以就地修改
dic1.index = ['x','y','z','w']

#+END_SRC

DataFrame是一种表格型结构，含有一组有序的列，每一列可以是不同的数据类型。既有行索引，又有列索引，可以被看做由Series组成的字典（使用共同的索引）。跟其他类似的数据结构（比如R中的data.frame），DataFrame面向行和列的操作基本是平衡的。其实，DataFrame中的数据是以一个或者多个二维块存放的（不是列表、字典或者其他）。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
from pandas import Series,DataFrame

#构建DataFrame可以直接传入等长的列表或Series组成的字典
#不等长会产生错误
data = {'a':[1,2,3],
        'c':[4,5,6],
        'b':[7,8,9]
}
#注意是按照列的名字进行列排序
frame = DataFrame(data)
#print frame
#指定列之后就会按照指定的进行排序
frame = DataFrame(data,columns=['a','c','b'])
print frame
#可以有空列,index是说行名
frame1 = DataFrame(data,columns = ['a','b','c','d'],index = ['one','two','three'])
print frame1
#用字典方式取列数据
print frame['a']
print frame.b
#列数据的修改直接选出来重新赋值即可
#行，可以用行名或者行数来进行选取
print frame1.ix['two']
#为列赋值，如果是Series，规定了index后可以精确赋值
frame1['d'] = Series([100,200,300],index = ['two','one','three'])
print frame1
#删除列用del 函数
del frame1['d']
#警告：通过列名选出来的是Series的视图，并不是副本，可用Series copy方法得到副本

#+END_SRC

另一种常见的结构是嵌套字典，即字典的字典，这样的结构会默认为外键为列，内列为行。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
from pandas import Series,DataFrame
#内层字典的键值会被合并、排序以形成最终的索引
pop = {'Nevada':{2001:2.4,2002:2.9},
       'Ohio':{2000:1.5,2001:1.7,2002:3.6}}
frame3 = DataFrame(pop)
#rint frame3
#Dataframe也有行和列有name属性，DataFrame有value属性
frame3.index.name = 'year'
frame3.columns.name = 'state'
print frame3
print frame3.values

#+END_SRC

下面列出了DataFrame构造函数能够接受的各种数据。

索引对象

#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
from pandas import Series,DataFrame
#pandas索引对象负责管理轴标签和其他元数据，构建Series和DataFrame时，所用到的任何数组或其他序列的标签都被转换为Index
obj = Series(range(3),index = ['a','b','c'])
index = obj.index
#print index
#索引对象是无法修改的,这非常重要，因为这样才会使得Index对象在多个数据结构之间安全共享
index1 = pd.Index(np.arange(3))
obj2 = Series([1.5,-2.5,0],index = index1)
print obj2.index is index1

#除了长得像数组，Index的功能也类似一个固定大小的集合
print 'Ohio' in frame3.columns
print 2003 in frame3.index

pandas中的Index是一个类，pandas中主要的Index对象（什么时候用到）。

下面是Index的方法与属性，值得注意的是：index并不是数组。

2、基本功能
下面介绍基本的Series 和 DataFrame 数据处理手段。首先是索引：


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

#Series有一个reindex函数，可以将索引重排，以致元素顺序发生变化

obj = Series([1,2,3,4],index=['a','b','c','d'])
#注意这里的reindex并不改变obj的值，得到的是一个“副本”
#fill_value 显然是填充空的index的值
#print obj.reindex(['a','c','d','b','e'],fill_value = 0)
#print obj
obj2 = Series(['red','blue'],index=[0,4])
#method = ffill，意味着前向值填充
obj3 = obj2.reindex(range(6),method='ffill')
#print obj3

#DataFrame 的reindex可以修改行、列或者两个都改
frame = DataFrame(np.arange(9).reshape((3,3)),index = ['a','c','d'],columns = ['Ohio','Texas','California'])
#只是传入一列数，是对行进行reindex,因为...frame的行参数叫index...(我这么猜的)
frame2 = frame.reindex(['a','b','c','d'])
#print frame2
#当传入原来没有的index是，当然返回的是空NaN
#frame3 = frame.reindex(['e'])
#print frame3
states = ['Texas','Utah','California']
#这是对行、列重排
#注意：这里的method是对index 也就是行进行的填充，列是不能填充的（不管method的位置如何）
frame4 = frame.reindex(index = ['a','b','c','d'],columns=states).ffill()
#print frame4

#使用ix的标签索引功能，重新索引变得比较简洁
print frame.ix[['a','d','c','b'],states]

#+END_SRC

关于ix，是DataFrame的一个方法，http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.ix.html。


丢弃指定轴上的项


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame
#drop函数可以丢弃轴上的列、行值
obj = Series(np.arange(3.),index = ['a','b','c'])
#原Series/datafram并不丢弃
obj.drop('b')
#print obj
#注意下面，行可以随意丢弃，列需要加axis = 1
print frame.drop(['a'])
print frame.drop(['Ohio'],axis = 1)

#+END_SRC

    下面说索引、选取和过滤


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

obj = Series([1,2,3,4],index=['a','b','c','d'])
frame = DataFrame(np.arange(9).reshape((3,3)),index = ['a','c','d'],columns = ['Ohio','Texas','California'])

#Series切片和索引
#print obj[obj < 2]
#注意：利用标签的切片与python的切片不同，两端都是包含的（有道理）
print obj['b':'c']
#对于DataFrame，列可以直接用名称
print frame['Ohio']
#特殊情况：通过切片和bool型索引，得到的是行(有道理)
print frame[:2]
print frame[frame['Ohio'] != 0]
#下面的方式是对frame所有元素都适用，不是行或者列,下面的得到的是numpy.ndarray类型的数据
print frame[frame < 5],type(frame[frame < 5])
frame[frame < 5] = 0
print frame

#对于DataFrame上的标签索引，用ix进行
print frame.ix[['a','d'],['Ohio','Texas']]
print frame.ix[2] #注意这里默认取行
#注意下面默认取行
print frame.ix[frame.Ohio > 0]
#注意下面的逗号后面是列标
print frame.ix[frame.Ohio > 0,:2]

#+END_SRC

    下面是常用的索引选项：


算术运算和数据对齐


#+BEGIN_SRC python
#pandas 有一个重要的功能就是能够根据索引自动对齐,其中索引不重合的部分值为NaN
s1 = Series([1,2,3],['a','b','c'])
s2 = Series([4,5,6],['b','c','d'])
#print s1 + s2
df1 = DataFrame(np.arange(12.).reshape(3,4),columns=list('abcd'))
df2 = DataFrame(np.arange(20.).reshape(4,5),columns=list('abcde'))
#print df1 + df2
#使用add方法，并传入填充值,注意下面的fill_value函数是先对应填充再进行加和，而不是加和得到NaN之后再填充
#print df1.add(df2,fill_value = 1000)
#df1.reindex(columns = df2.columns,fill_value=0)

#+END_SRC

除了add之外，还有其他的方法：

DataFrame和Series之间的运算


#+BEGIN_SRC python
#下面看一下DataFrame和Series之间的计算过程
arr = DataFrame(np.arange(12.).reshape((3,4)),columns = list('abcd'))
#下面的结果标明，就是按行分别相减即可，叫做 broadcasting
#注意：默认情况下，DataFrame和Series的计算会将Series的索引匹配到DataFrame的列，然后进行计算，再沿着行一直向下广播
#注意：下面的式子中，如果写arr - arr[0]是错的，因为只有标签索引函数ix后面加数字才表示行
print arr - arr.ix[0]
Series2 = Series(range(3),index = list('cdf'))
#按照规则，在不匹配的列会形成NaN值
print arr + Series2
#如果想匹配行且在列上广播，需要用到算术运算方法
Series3 = arr['d']
#axis就是希望匹配的轴
print arr.sub(Series3,axis = 0)

#+END_SRC

下面是函数应用和映射


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

#NumPy的元素级数组方法也适用于pandas对象
frame = DataFrame(np.random.randn(4,3),columns = list('abc'),index = ['Ut','Oh','Te','Or'])
print frame
#下面是求绝对值：
#print np.abs(frame)
#另一种常见的做法是：将一个函数应用到行或者列上,用apply方法，与R语言类似
fun = lambda x:x.max() - x.min()
#默认是应用在每一列上
print frame.apply(fun)
#下面是应用在列上
print frame.apply(fun,axis = 1)
#很多统计函数根本不用apply，直接调用方法就可以了
print frame.sum()
#除了标量值之外，apply函数后面还可以接返回多个值组成的的Series的函数,有没有很漂亮？
def f(x):
    return Series([x.min(),x.max()],index = ['min','max'])
#print frame.apply(f)
#元素级的python函数也是可以用的，但是要使用applymap函数
format = lambda x: '%.2f' % x
print frame.applymap(format)
#之所以要用applymap是因为Series有一个应用于元素级函数的map方法？？
#这里的map很有用
print frame['b'].map(format)

#+END_SRC

排序与排名


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame
#用sort_index函数对行、列的索引进行排序
obj = Series(range(4),index = ['d','a','b','c'])
print obj.sort_index()

frame = DataFrame(np.arange(8).reshape((2,4)),index = ['three','one'],columns = ['d','a','b','c'])
#默认是对行 “索引” 进行排序，如果对列 “索引” 进行排序，axis = 1 即可
print frame.sort_index()
print frame.sort_index(axis = 1)
print frame.sort_index(axis = 1,ascending = False)

#如果对值进行排序，用的是order函数,注意所有的缺失值会放到最后（如果有的话）
print obj.order()
#numpy中的sort也可以用来排序
print np.sort(obj)
#如果相对DataFrame的值进行排序，函数还是sort_index，只不过后面需要加一个参数by
frame = DataFrame({'b':[4,7,-3,2],'a':[0,1,0,1]})
print frame.sort_index(by = ['a','b'])

#rank函数返回从小到大排序的下标，对于平级的数，rank是通过“为各组分配一个平均排名”的方式破坏评级关系
#下标从1开始
obj = Series([7,-5,7,4,2,0,4])
print obj.rank()
#而numpy中的argsort函数比较奇怪，返回的是把数据进行排序之后，按照值得顺序对应的下标，下标从0开始
print np.argsort(obj)
 #打印结果为：1,5,4,3,6,0,2 按照这个下标顺序恰好可以得到从小打到的值，见下面
print obj[np.argsort(obj)]
#rank函数中有一个method选项，用来规定下标的方式

print obj.rank(method = 'first',ascending=False)
print obj.rank(method = 'max',ascending=False)
print obj.rank(method = 'min',ascending=False)

#对于DataFrame，rank函数默认把每一列排好并返回坐标
print frame.rank()
print frame.rank(axis = 1)
#+END_SRC


带有重复值的轴索引


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

#虽然pandas的很多函数（如reindex）要求标签唯一，但是并不具有强制性
obj = Series(range(5),index = list('aabbc'))
print obj
#索引是否唯一用is_unique看是否唯一
print obj.index.is_unique
#对于重复值的索引，选取的话返回一个Series，唯一的索引返回一个标量
print obj['a']
#对于DataFrame也是如此
df = DataFrame(np.random.randn(4,3),index = list('aabb'))
print df
print df.ix['b']
#####自己导入数据的时候数据处理之前可以做一下index唯一性等，自己创建DataFrame注意不能这样

#+END_SRC

3、汇总和计算描述统计


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import os
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt
import time

#pandas 对象拥有一组常用的数学和统计方法，大部分属于简约统计，用于从Series中提取一个值，或者   从DataFrame中提取一列或者一行Series
#注意：与NumPy数组相比，这些函数都是基于没有缺失数据的建设构建的，也就是说：这些函数会自动忽略缺失值。
df = DataFrame([[1.4,np.nan],[7.1,-4.5],[np.nan,np.nan],[0.75,-1.3]],index = list('abcd'),columns=['one','two'])
print df.sum()
print df.sum(axis = 1)
#下面是一些函数，idxmin 和 idmax 返回的是达到最小或者最大的索引
print df.idxmin()
print df.idxmin(axis=1)
#关于累积型的函数
print df.cumsum()
#describe函数，与R语言中的describe函数基本相同
print df.describe()
#对于非数值型的数据，看看下面的结果

obj = Series(['c','a','a','b','d'] * 4)
print obj.describe()
#+END_SRC


结果为：
count     20
unique     4
top        a
freq       8
其中，freq是指字母出现的最高频率







#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import os
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt
import time

#下面看一下cummin函数
#注意：这里的cummin函数是截止到目前为止的最小值，而不是加和以后的最小值
frame = DataFrame([[1,2,3,4],[5,6,7,8],[-10,11,12,-13]],index = list('abc'),columns = ['one','two','three','four'])
print frame.cummin()
print frame
>>>
   one  two  three  four
a    1    2      3     4
b    1    2      3     4
c  -10    2      3   -13
   one  two  three  four
a    1    2      3     4
b    5    6      7     8
c  -10   11     12   -13
#+END_SRC


相关系数与协方差
有些汇总统计（如相关系数和协方差）是通过参数对计算出来的。这一节数据得不到？


#+BEGIN_SRC python
#-*- encoding:utf-8 -*- import numpy as np import os import pandas as pd from pandas import Series,DataFrame import matplotlib.pyplot as plt import time #pandas 对象拥有一组常用的数学和统计方法，大部分属于简约统计，用于从Series中提取一个值，或者 从DataFrame中提取一列或者一行Series #注意：与NumPy数组相比，这些函数都是基于没有缺失数据的建设构建的，也就是说：这些函数会自动忽略缺失值。 frame = DataFrame([[1,2,3,4],[5,6,7,8],[-10,11,12,-13]],index = list('abc'),columns = ['one','two','three','four']) print frame # print '++++++++++++++++++++++++++++++' # print df.sum() # print '++++++++++++++++++++++++++++++' print frame.corr()

唯一值、值计数以及成员资格

#-*- encoding:utf-8 -*-
import numpy as np
import os
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt

obj = Series(['a','a','b','f','e'])
uniques = obj.unique()
uniques.sort() #记住这是就地排序
#print uniques
#下面进行计数统计,注意得到的是按照出现的频率降序排列
#print obj.value_counts()
#value_counts还是一个顶级的pandas方法。可用于任何是数组或者序列
#print obj.values
#print pd.value_counts(obj.values,sort = False)
#最后是isin 判断矢量化集合的成员资格，可用于选取Series中或DataF列中的子集
mask = obj.isin(['b','c'])
print mask
print obj[mask]

data = DataFrame({'Qu1':[1,3,4,3,4],
                  'Qu2':[2,3,1,2,3],
                  'Qu3':[1,5,2,4,4]})
print data
print data.apply(pd.value_counts).fillna(0)
#+END_SRC



上面这几个函数是真的非常实用！
4、处理缺失数据


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import os
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt
import time
from numpy import nan as NA

#pandas本来就被设计成自动忽略了缺失值、
#nan None 都看做缺失值
str_data = Series(['a',np.nan,'b','c'])
str_data[0] = None
print str_data.isnull()
print str_data.notnull()
>>>
0     True
1     True
2    False
3    False
0    False
1    False
2     True
3     True
#NumPy的数据类型中缺少真正的NA数据类型或位模式？？
#+END_SRC


 


滤除缺失数据


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import os
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt
import time
from numpy import nan as NA

data = Series([1,NA,3.5,7,NA])
#注意返回的是不为NA的值的原来的索引，不是移除之后的索引
#有一个函数 reset_index 这个函数（方法？）可以重新设置index，其中drop = True选项会丢弃原来的索引而设置新的从0开始的索引，这个方法只对DataFrame有用貌似。
print data.dropna()
#下面的结果一样
print data[data.notnull()]
data1 = DataFrame([[1,2,3],[NA,2.3,4],[NA,NA,NA]])
#注意：由于DataFrame的设定，只要有NA的行就会舍弃
print data1.dropna()
#传入how = 'all' 则丢掉全为NA的行，这里的 how 的起名真的有点随心所欲了，哈哈
print data1.dropna(how = 'all')
#丢弃列
print data1.dropna(how = 'all',axis = 1)
#还有一个参数，thresh
data2 = DataFrame(np.random.randn(7,3))
data2.ix[:4,1] = NA
data2.ix[:2,2] = NA
#print data2
#这里的thresh函数是选取最少non-NA值个数的行选出来
print data2.dropna(thresh = 2)
print data2.dropna(thresh = 4,axis = 1)
#+END_SRC


填充缺失数据


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import os
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt
import time
from numpy import nan as NA

#主要用fillna方法填充NA处的值
data2 = DataFrame(np.random.randn(7,3))
data2.ix[:4,1] = NA
data2.ix[:2,2] = NA
#fillna返回一个新对象，inplace = True 可以就地填充
print data2.fillna(0)
#print data2.fillna(0,inplace = True)
#print data2
#为不同的列填充要用到字典
print data2.fillna({1:0.5,3:-1})
#对reindex有效的的那些差值方法也可适用于fillna,请向上看，或者搜索 reindex 即可
df = DataFrame(np.random.randn(6,3))
df.ix[2:,1] = NA
df.ix[4:,2] = NA
print df.fillna(method = 'ffill',limit = 2)
#只要稍微动动脑子，我们就可以知道向NA处可以填充均值等其他数
data = Series([1.2,NA,4,NA])
print data.fillna(data.mean())

#+END_SRC

    fillna的参数如下：


5、层次化索引
层次化索引（hierarchical index）是pandas的重要功能，这能使在一个轴上拥有两个以上的索引级别。抽象点说，它能使你以低维度形式处理高维度。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import os
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt
import time

data = Series(np.random.randn(10),index=[['a','a','a','b','b','b','c','c','d','d'],[1,2,3,1,2,3,1,2,2,3]])
#print data
#下面是索引的选取方式

print data.index
print data['b']
print data['b':'c']
print data.ix[['b','d']]
#下面是“内层”的选取方式
print data[:,2]
#层次化索引在数据重塑和基于分组操作（如透视表生成）中扮演者重要的角色，比如用unstack方式重排DataFrame:
print data.unstack()
#stack是unstack的逆运算
print data.unstack().stack()

#对于DataFrame，每个轴都可以有分层索引
frame = DataFrame(np.arange(12).reshape((4,3)),index = [['a','a','b','b'],[1,2,1,2]],columns = [['Ohio','Ohio','Colorado'],['Green','Red','Green']])
#print frame
#注意下面的方式：是为每一个轴规定名字，跟
frame.index.names = ['key1','key2']
frame.columns.names = ['state','color']
#print frame
#print frame['Ohio']

#可以单独创建MultiIndex然后复用
#下面的multiindex可以这样创建,注意下面的生成方式
columns = pd.MultiIndex.from_arrays([['Ohio','Ohio','Colorado'],['Green','Red','Green']],names = ['state','color'])
frame1 = DataFrame(np.arange(12).reshape((4,3)),columns = columns)
print frame1
#重排顺序，调整索引级别
print frame.swaplevel('key1','key2')
#sortlevel则根据但各级别中的值对数据进行排序，通常用swaplevel是也会用到sortlevel（很合理）
#注意得到的是副本，不是就地修改
print frame.sortlevel(1)
print frame.swaplevel(0,1).sortlevel(0)
print frame

#许多对DataFrame和Series进行描述汇总的统计都有一个level选项，用于指定汇总方式
print frame.sum(level = 'key2')
#不指定level的话，会按照列汇总出所有列名的和
print frame.sum()
print frame.sum(level = 'color',axis = 1)

#+END_SRC



#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import os
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt
import time
#人们经常想将DataFrame的一个或者多个列当作行索引来用，或者可能需要将行索引变成DataFrame的列
frame = DataFrame({'a':range(7),'b':range(7,0,-1),'c':['one','one','one','two','two','two','two'],'d':[0,1,2,0,1,2,3]})
print frame
#DataFrame中的set_index函数会将其一个或者多个列转换为行索引
frame2 = frame.set_index(['c','d'])
print frame2  #其实就是利用第3、4列进行一次分类汇总
frame3 = frame.set_index(['c','d'],drop = False)
#与set_index相反的是reset_index函数
print frame2.reset_index()
#下面进行一次测试
frame4 = DataFrame([[0,7],[1,6],[2,5],[3,4],[4,3],[5,2],[6,1]],index = [['one','one','one','two','two','two','two'],[0,1,2,0,1,2,3]],columns=['a','b'])
frame4.index.names = ['c','d']
print frame4
print frame4.reset_index().sort_index(axis = 1)

#+END_SRC


其他有关pandas的话题


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import os
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt
import pandas.io.data  as web
#这里说的是一些蛋疼的问题：整数索引和整数标签
ser = Series(np.arange(3.))
#print ser[-1]  #报错，因为整数索引的歧义性
ser2 = Series(np.arange(3.),index = ['a','b','c'])
print ser2[-1] #正确
#ix函数总是面向标签的
print ser.ix[:1]
#如果需要可靠的、不考虑索引类型的、基于位置的索引，可以使用Series的iget_value方法，Dataframe的irow 和 icol方法
ser3 = Series(range(3),index= [-5,1,3])
print ser3.iget_value(2)
frame = DataFrame(np.arange(6).reshape(3,2),index = [2,0,1])
print frame.irow(0)

#pandas 有一个Panel数据结构（不是主要内容），可以看作是三维的DataFrame。pandas中的多维数据可以利用多层索引进行处理
#可以利用DataFrame对象组成的字典或者一个三维ndarray来创建Panel对象
pdata = pd.Panel(dict((stk,web.get_data_yahoo(stk,'1/1/2009','6/1/2012')) for stk in ['AAPL','GOOG','MSFT','DELL']))
#网络错误，得不到数据
#Panel的每一项都是一个DataFrame.
#+END_SRC

* 第六章  数据加载、存储与文件格式

需要补充

输入输出一般分为下面几类：读取文本文件和其他更高效的磁盘存储格式，加载数据库中的数据。利用Web API操作网络资源。 
1、读写文本格式的数据
自己感觉读写文件有时候“需要运气”，经常需要手工调整。因为其简单的文件交互语法、直观的数据结构，以及诸如元组打包解包之类的便利功能，Python在文本和文件处理方面已经成为一门招人喜欢的语言。pandas提供了一些用于将表格型数据读取为DataFrame对象的函数。见下表：

下面大致介绍一下这些函数在文本数据转换为DataFrame时的一些技术。可以分为一下几类：
索引：将一个或者多个列当作返回的DataFrame处理，以及是否从文件、用户获取列名。
类型推断和数据转换：包括用户定义值的转换、缺失值标记列表等。
日期解析：包括组合功能，比如将分散在多个列中的日期时间信息组合成结果中的单个列。
迭代：支持对大文件进行逐块迭代。
不规整数据问题：跳过一些行、页脚、注释或者其他不要的东西
pandas读取文件会自动推断数据类型，不用指定。
read_csv为例
用names重新规定列名，用index_col指定索引，也可以将多个列组合作为层次化索引。可以编写正则表达式规定分隔符。用skiprows跳过某些行。pandas会用NA、-1.#IND、NULL等进行标记。用na_values用来不同的NA标记值。下面是read_csv/read_table参数：


逐块读取文本文件
处理很大的文件时，或找出大文件中的参数集以便于后续处理时，可能只想读取一部分或者逐块对文件进行迭代。nrows指定读取多少行。要逐块读取文件，需要设置chunksize（行数）。
将数据写出到文本格式
用to_csv方法写出到csv文件中。参数sep标明分隔符。na_rep标明空白字符串的代替值。index header标明是否写出行列标签，默认是写出。用cols限制并以指定顺序写出某些列。
Series也有to_csv方法。用一些整理工作（无header行，第一列作索引）就能用read_csv读取为Series，当然还有一个更方便的from_csv，Series.from_csv。
手工处理分隔符格式
有些奇葩文件需要进行处理以后再读。Python内置的csv模块可以读取任何单字符分隔符文件。将打开的文件传递给csv.reader。csv文件的形式有很多，只需定义csv.Dialect的一个子类即可定义新格式（如专门的分隔符、字符串引用约定、行结束符等）：


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import os
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt
import pandas.io.data  as web
import csv

f = open('ex6.csv')
reader = csv.reader(f)
for line in reader:
    print line
lines = list(csv.reader(open('ex7.csv')))
header,values = lines[0],lines[1:]
print header
print values
#下面的 * 应该是取出值的意思
data_dict = {h:v for h,v in zip(header,zip(*values))}
print data_dict
class my_dialect(csv.Dialect):
    lineterminator = '\n'
    delimiter = ';'
    quotechar = '"'

reader = csv.reader(f,dialect=my_dialect)
#csv语支的参数也可以用参数的形式给出
reader = csv.reader(f,delimiter = '|')
#+END_SRC



对于那些使用复杂分隔符或多字符分隔符的文件，csv文件就无能为力了。这种情况下用split或者re.split进行拆分合整理工作。要手工输出分隔符文件，可以使用csv.writer。它接受一个已打开且可写的文件对象以及跟cav.reader相同的那些语支和格式化选项。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import os
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt
import pandas.io.data  as web
import csv

with open('mydata.csv','w') as f:
    writer = csv.writer(f,lineterminator = '\n')
    writer.writerow(('one','two','three'))
    writer.writerow(('1','2','3'))

#+END_SRC

JSON数据
除空值null和一些其他的细微差别（如列表末尾不允许存在多余的逗号）之外，JSON非常接近有效Python代码。基本数据类型有对象（字典）、数组（列表）、字符串、数值、布尔值以及NULL。对象中所有的键都必须是字符串（非常重要）。用json模块，json.loads可以将字符串转换成Python形式，即可以将对象读取为python字典。
相反的，json.dumps可以将python对象转换为json格式。
XML和HTML：Web信息收集
lxml可以读取xml和html格式数据并处理。这部分用到的时候再研究。
2、二进制数据文件
实现数据的二进制格式最简单的方法之一是使用Python内置的pickle序列化。为了使用方便，pandas对象都有一个用于将数据以pickle形式保存到磁盘上的save方法。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import os
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt
import pandas.io.data as web
import csv

frame = pd.read_csv('ex1.csv')
print frame
frame.save('frame_pickle') #存储为二进制文件
ok = pd.load('frame_pickle') #load函数
#ok1 = pd.read_table('frame_pickle') #不能用read_table函数
#print ok1
print ok
#+END_SRC


pickle，作者建议用作短期存储，因为会遇到解析版本问题。
使用HDF5格式
很多工具都能实现高效读写磁盘上以二进制格式存储的科学数据。HDF5就是一个流行工业级库，是一个C库，有Java、Python、MATLAB等多种接口。这部分暂时不看。
读取Excel数据
支持excel2003及更高版本的excel文件。用pandas中的ExcelFile类即可，需要安装xlrd和openpyxl包。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import os
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt
import pandas.io.data as web
import csv

xls_file = pd.ExcelFile('ex1.xlsx')
table = xls_file.parse('ex1')
print table
#+END_SRC


使用HTML和Web API
许多网站都有一些通过JSON或其他格式提供数据的公共API。用requests包等可以实现。这部分暂时不看。
使用数据库
数据从SQL中加载到DataFrame比较简单，此外pandas还有一些能够简化该过程的函数。作者是用一款SQLite数据库，用sqlite3驱动器。作者还举了MongoDB中数据的例子。暂时不看。


*  第七章 数据规整化：清理、转换、合并、重塑（一）

数据分析和建模的大量编程工作都是在数据准备上的（深表同意）：加载、清理、转换以及重塑。pandas和Python标准库提供了一组高级的、灵活的、高效的核心函数和算法，他们能够轻松地将数据规整化为正确的形式。 
1、合并数据集
pandas对象中的数据可以通过一些内置的方式进行合并
pandas.merge可以根据一个或者多个键值连接起来，就是SQL中的数据库连接工作。
pandas.concat可以沿着一条轴将多个对象堆叠在一起
实例方法combine_first可以讲重复数据编接在一起 ，用一个对象中的值填充另一个对象中的缺失值（注：译者说就是数据库中的外连接）。
DataFrame有一个join实例方法，它能更方便地实现按索引合并。还可以用作合并多个带有相同或者相似索引的
由于太常用，给出一些例子。
数据库风格的DataFrame合并


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

#数据集的合并（merge）或者连接（join）运算是通过一个或者多个键将行链接起来。这是关系型数据库的核心。
df1 = DataFrame({'key':['b','b','a','c','a','a','b'],'data1':range(7)})
df2 = DataFrame({'key':['a','b','d'],'data2':range(3)})
print df1
print df2
#没有指定用哪些列进行合并时，默认用重复的列名进行合并，并且只保留合并列中的交集，其他舍去
#即merge默认的是“内连接”
print pd.merge(df1,df2) 
#不过，最好显示指定一下：
print pd.merge(df1,df2,on = 'key')
#如果两个对象列明不同，也可以分别指定，当然，原则是这两列得有相同的值
df3 = DataFrame({'lkey':['b','b','a','c','a','a','b'],'data1':range(7)})
df4 = DataFrame({'rkey':['a','b','d'],'data2':range(3)})
print pd.merge(df3,df4,left_on = 'lkey',right_on = 'rkey')
#如果两列没有相同值，返回一个空DataFrame
print pd.merge(df3,df4,left_on = 'lkey',right_on = 'data2')
#merge选项有inner、left、right、outer几种，分别表示 内、左、右、外连接
print pd.merge(df1,df2,how = 'outer')
#下面看多对多（即两个对象中每个键值对应不同的值）
df1 = DataFrame({'key':list('bbacab'),'data1':range(6)})
df2 = DataFrame({'key':list('ababd'),'data2':range(5)})
#下面是多对多的合并，结果是笛卡尔积也就是针对一个键值，两个对象对应值的所有组合
print pd.merge(df1,df2,on = 'key',how = 'left')
#对多个键进行合并，传入一个由列名组成的列表即可
left = DataFrame({'key1':['foo','foo','bar'],'key2':['one','two','one'],'lval':[1,2,3]})
right = DataFrame({'key1':['foo','foo','bar','bar'],'key2':['one','one','one','two'],'rval':[4,5,6,7]})
#多个键进行合并就是将多个键组合成元组，当作单个键值使用（实际上并不是这么回事）
#注意要“不忘初心”，根据键值是对其他列的值进行合并
print pd.merge(left,right,on = ['key1','key2'],how = 'outer')
#警告：列与列合并时，会把DataFrame的索引丢弃
#下面处理重复列名的问题，这里的重复列名是说，依据一列进行合并时两个对象剩下的列中有的列名字重复
#pandas会自动添加后缀
print pd.merge(left,right,on = 'key1')
#后缀可以通过suffixes选项来指定
print pd.merge(left,right,on = 'key1',suffixes = ('_left','_right'))
>>>
   data1 key
0      0   b
1      1   b
2      2   a
3      3   c
4      4   a
5      5   a
6      6   b
   data2 key
0      0   a
1      1   b
2      2   d
   data1 key  data2
0      2   a      0
1      4   a      0
2      5   a      0
3      0   b      1
4      1   b      1
5      6   b      1
   data1 key  data2
0      2   a      0
1      4   a      0
2      5   a      0
3      0   b      1
4      1   b      1
5      6   b      1
   data1 lkey  data2 rkey
0      2    a      0    a
1      4    a      0    a
2      5    a      0    a
3      0    b      1    b
4      1    b      1    b
5      6    b      1    b
Empty DataFrame
Columns: array([data1, lkey, data2, rkey], dtype=object)
Index: array([], dtype=int64)
   data1 key  data2
0      2   a      0
1      4   a      0
2      5   a      0
3      0   b      1
4      1   b      1
5      6   b      1
6      3   c    NaN
7    NaN   d      2
    data1 key  data2
0       2   a      0
1       2   a      2
2       4   a      0
3       4   a      2
4       0   b      1
5       0   b      3
6       1   b      1
7       1   b      3
8       5   b      1
9       5   b      3
10      3   c    NaN
  key1 key2  lval  rval
0  bar  one     3     6
1  bar  two   NaN     7
2  foo  one     1     4
3  foo  one     1     5
4  foo  two     2   NaN
  key1 key2_x  lval key2_y  rval
0  bar    one     3    one     6
1  bar    one     3    two     7
2  foo    one     1    one     4
3  foo    one     1    one     5
4  foo    two     2    one     4
5  foo    two     2    one     5
  key1 key2_left  lval key2_right  rval
0  bar       one     3        one     6
1  bar       one     3        two     7
2  foo       one     1        one     4
3  foo       one     1        one     5
4  foo       two     2        one     4
5  foo       two     2        one     5
[Finished in 0.7s]

#+END_SRC


merge的选项有：


索引上的合并


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

#索引上的合并
#DataFrame中连接键有时候在索引中。这时可以传入left_index = True或者right_index = True
left1 = DataFrame({'key':list('abaabc'),'value':range(6)})
right1 = DataFrame({'group_val':[3.5,7],'index':['a','b']})
print right1
#注意上面的right1的索引值和ledt1中的值是同类型的，也就是说相当于对右边的进行转置并且索引跟随改变再进行合并
print pd.merge(left1,right1,left_on = 'key',right_index = True,how = 'inner')
#对于层次化索引，事情就有点复杂了
lefth = DataFrame({'key1':['Ohio','Ohio','Ohio','Nevada','Nevada'],
    'key2':[2000,2001,2002,2001,2002],'data':np.arange(5.)})
righth = DataFrame(np.arange(12.).reshape((6,2)),index = [['Nevada','Nevada','Ohio','Ohio','Ohio','Ohio',],
    [2001,2000,2000,2000,2001,2002]],columns = ['event1','event2'])
print lefth
print righth
#这种情况下，必须指明用作合并键的多个列（注意对重复索引值的处理）
#注意得到的结果的index是跟左边对象的index一致
print pd.merge(lefth,righth,left_on = ['key1','key2'],right_index = True,how = 'outer')
#同时使用合并双方的索引也没问题
left2 = DataFrame([[1.,2.],[3.,4.],[5.,6.]],index = ['a','c','e'],columns = ['Ohio','Nevada'])
right2 = DataFrame([[7.,8.],[9.,10.],[11.,12.],[13,14]],index = ['b','c','d','e'],columns = ['Missouri','Alabama'])
print left2
print right2
#注意下面的方式，利用index进行合并
print pd.merge(left2,right2,how = 'outer',left_index = True,right_index = True)
#DataFrame有一个join实例方法，它能更方便地实现按索引合并。还可以用作合并多个带有相同或者相似索引的
#DataFrame对象，而不管有没有重叠的列
print left2.join(right2,how = 'outer')
#由于一些历史原因，DataFrame的join方法是在连接键上做左连接。它还支持参数DataFrame的索引跟
#调用者DataFrame的某个列之间的连接（这个方法有点像merge中的left_index这样的参数）
print left1.join(right1,on = 'key') #这个函数现在已经跟书上的不一样了
#最后，对于简单的索引合并，还可以向join传入多个DataFrame
another = DataFrame([[7.,8.],[9.,10.],[11.,12.],[16.,17.]],index = ['a','c','e','f'],columns = ['New York','Oregon'])
print left2.join([right2,another],how = 'outer')
>>>
   group_val index
0        3.5     a
1        7.0     b
Empty DataFrame
Columns: array([key, value, group_val, index], dtype=object)
Index: array([], dtype=int64)
   data    key1  key2
0     0    Ohio  2000
1     1    Ohio  2001
2     2    Ohio  2002
3     3  Nevada  2001
4     4  Nevada  2002
             event1  event2
Nevada 2001       0       1
       2000       2       3
Ohio   2000       4       5
       2000       6       7
       2001       8       9
       2002      10      11
   data    key1  key2  event1  event2
4   NaN  Nevada  2000       2       3
3     3  Nevada  2001       0       1
4     4  Nevada  2002     NaN     NaN
0     0    Ohio  2000       4       5
0     0    Ohio  2000       6       7
1     1    Ohio  2001       8       9
2     2    Ohio  2002      10      11
   Ohio  Nevada
a     1       2
c     3       4
e     5       6
   Missouri  Alabama
b         7        8
c         9       10
d        11       12
e        13       14
   Ohio  Nevada  Missouri  Alabama
a     1       2       NaN      NaN
b   NaN     NaN         7        8
c     3       4         9       10
d   NaN     NaN        11       12
e     5       6        13       14
   Ohio  Nevada  Missouri  Alabama
a     1       2       NaN      NaN
b   NaN     NaN         7        8
c     3       4         9       10
d   NaN     NaN        11       12
e     5       6        13       14
  key  value  group_val index
0   a      0        NaN   NaN
1   b      1        NaN   NaN
2   a      2        NaN   NaN
3   a      3        NaN   NaN
4   b      4        NaN   NaN
5   c      5        NaN   NaN
   Ohio  Nevada  Missouri  Alabama  New York  Oregon
a     1       2       NaN      NaN         7       8
b   NaN     NaN         7        8       NaN     NaN
c     3       4         9       10         9      10
d   NaN     NaN        11       12       NaN     NaN
e     5       6        13       14        11      12
f   NaN     NaN       NaN      NaN        16      17
[Finished in 0.8s]
#+END_SRC

#+END_SRC


下面是轴向连接


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

#另一种合并运算为连接（concatenation），绑定（binding）或者堆叠（stacking）。
#Numpy有一个用于合并原始Numpy数组的concatenation函数：
arr = np.arange(12).reshape((3,4))
print arr
print np.concatenate([arr,arr],axis = 1)

#+END_SRC

对于pandas对象，需要考虑：
如果各对象其他轴上的索引不同，那些轴应该是并集还是交集？
结果对象中的分组需要各不相同吗？
用于连接的轴重要吗？
下面介绍concat函数：


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

#另一种合并运算为连接（concatenation），绑定（binding）或者堆叠（stacking）。
#Numpy有一个用于合并原始Numpy数组的concatenation函数：
arr = np.arange(12).reshape((3,4))
print arr
print np.concatenate([arr,arr],axis = 1)

s1 = Series([0,1],index = ['a','b'])
s2 = Series([2,3,4],index = ['c','d','e'])
s3 = Series([5,6],index = ['f','g'])
print pd.concat([s1,s2,s3])
#注意下面的方式，产生的是一个DataFrame，index是所有index合并起来，列是每个Series占一列，其他位置N啊N
print pd.concat([s1,s2,s3],axis = 1)
#如果Series有重复值的情况下
s4 = pd.concat([s1 * 5,s3])
print s4
#下面的inner是取交集
print pd.concat([s1,s4],axis = 1,join = 'inner')
#通过join_axes指定要在“其他轴”上使用的索引
print pd.concat([s1,s4],axis = 1,join_axes = [['a','c','b','e']])
#现在有个问题，参与连接的各个部分在最后的结果中不能区分，可以设置层次化索引解决此问题
result = pd.concat([s1,s2,s3],keys = ['one','two','three'])
print result
print result.unstack()
#如果沿着axis=1进行合并，则当然的key成为DataFrame的列头(列名)：
result1 = pd.concat([s1,s2,s3],axis = 1,keys = ['one','two','three'])
print result1
print result1.columns
#下面看DataFrame的合并方式，行列数量不同也能合并，比R语言好
df1 = DataFrame(np.arange(6).reshape(3,2),index = ['a','b','c'],columns = ['one','two'])
df2 = DataFrame(5 + np.arange(4).reshape(2,2),index = ['a','c'],columns = ['three','four'])
print pd.concat([df1,df2])#默认将行合并
print pd.concat([df1,df2],axis = 1,keys = ['level1','level2'])
#下面的这种合并方式更加科学,字典的形式
print pd.concat({'level1':df1,'level2':df2},axis = 0)
print pd.concat([df1,df2],axis = 1,keys = ['level1','level2'],names = ['upper','lower'])
#最后需要考虑的问题是，跟当前分析工作无关的DataFrame行索引,也就是说，原来的行索引没有意义了
df1 = DataFrame(np.random.randn(3,4),columns = [list('abcd')])
df2 = DataFrame(np.random.randn(2,3),columns = ['b','d','a'])
#只要加上ignore_index = True 即可
print pd.concat([df1,df2],ignore_index = True)
>>>
    a    0
    b    1
    c    2
    d    3
    e    4
    f    5
    g    6
        0   1   2
    a   0 NaN NaN
    b   1 NaN NaN
    c NaN   2 NaN
    d NaN   3 NaN
    e NaN   4 NaN
    f NaN NaN   5
    g NaN NaN   6
    a    0
    b    5
    f    5
    g    6
       0  1
    a  0  0
    b  1  5
        0   1
    a   0   0
    c NaN NaN
    b   1   5
    e NaN NaN
    one    a    0
           b    1
    two    c    2
           d    3
           e    4
    three  f    5
           g    6
            a   b   c   d   e   f   g
    one     0   1 NaN NaN NaN NaN NaN
    two   NaN NaN   2   3   4 NaN NaN
    three NaN NaN NaN NaN NaN   5   6
       one  two  three
    a    0  NaN    NaN
    b    1  NaN    NaN
    c  NaN    2    NaN
    d  NaN    3    NaN
    e  NaN    4    NaN
    f  NaN  NaN      5
    g  NaN  NaN      6
    array([one, two, three], dtype=object)
       four  one  three  two
    a   NaN    0    NaN    1
    b   NaN    2    NaN    3
    c   NaN    4    NaN    5
    a     6  NaN      5  NaN
    c     8  NaN      7  NaN
       level1       level2      
          one  two   three  four
    a       0    1       5     6
    b       2    3     NaN   NaN
    c       4    5       7     8
              four  one  three  two
    level1 a   NaN    0    NaN    1
           b   NaN    2    NaN    3
           c   NaN    4    NaN    5
    level2 a     6  NaN      5  NaN
           c     8  NaN      7  NaN
    upper  level1       level2      
    lower     one  two   three  four
    a           0    1       5     6
    b           2    3     NaN   NaN
    c           4    5       7     8
              a         b         c         d
    0  2.277611  0.597990  2.128480 -0.467747
    1  2.450508 -0.682617  1.129313  1.174447
    2 -0.106422  0.590667  1.015706  0.712673
    3 -1.323742  0.060791       NaN  1.095113
    4  0.586082 -0.849976       NaN -0.320739
    [Finished in 1.9s]

#+END_SRC

concat函数的参数如下：

合并重叠数据
还有一种数据是不能简单通过merge、concatenation解决的。比如，有可能部分或者全部索引重叠的两个数据集。


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

a = Series([np.nan,2.5,np.nan,3.5,4.5,np.nan],
    index = ['f','e','d','c','b','a'])
b = Series(np.arange(len(a),dtype = np.float64),
    index = ['f','e','d','c','b','a'])
b[-1] = np.nan
print a,'\n'
print b,'\n'
#print a + b #注意这里的自动对齐
#c用来按照索引取a、b的值：
c = np.where(pd.isnull(a),b,a)
print c,'\n'
#numpy中也有这样一个方法combine_first
print b[:-2].combine_first(a[2:]) #注意两者都不为空时，保留b的值

#对于DataFrame而言，combine_first也是做同样的事，可以看作用参数对象中的数据
#为调用者对象的确实数据“打补丁”
df1 = DataFrame({'a':[1.,np.nan,5.,np.nan],
    'b':[np.nan,2.,np.nan,6.],
    'c':range(2,18,4)})
df2 = DataFrame({'a':[5.,4.,np.nan,3.,7.],
    'b':[np.nan,3.,4.,6.,8.]})
#要特别注意下面的应用，df1比df2 少一行，运行以后df1就比原来多了一行，这有时候对数据处理是个隐藏bug啊！
print df1.combine_first(df2)
>>>
f    NaN
e    2.5
d    NaN
c    3.5
b    4.5
a    NaN 
    f     0
    e     1
    d     2
    c     3
    b     4
    a   NaN
    f    0.0
    e    2.5
    d    2.0
    c    3.5
    b    4.5
    a    NaN
    a    NaN
    b    4.5
    c    3.0
    d    2.0
    e    1.0
    f    0.0
       a   b   c
    0  1 NaN   2
    1  4   2   6
    2  5   4  10
    3  3   6  14
    4  7   8 NaN
    [Finished in 0.9s]

2、重塑和轴向旋转

#-*- encoding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

#reshape（重塑）、pivot（轴向旋转）可以对表格型数据进行基础运算
#重塑层次化索引
#stack:将数据的列“旋转”为行
#unstack：将数据的行“旋转”为列
data = DataFrame(np.arange(6).reshape((2,3)),index = pd.Index(['Ohio','Colorado'],name = 'state'),
    columns = pd.Index(['one','two','three'],name = 'number'))
print data
result = data.stack()
print result #这里就是将列名作为了层次化索引(内层索引)，得到了一个Series
print result.unstack() #将层次化索引转换为二维表，得到DataFrame
#默认情况下，unstack处理的是内层的索引，若想别的层次，传入编号或者名称即可，注意最外一层编号为0
result1 = result.unstack(0)
print result1
print result1.stack(0),'\n'  #默认，列为内层
print result1.unstack(1) ,'\n'  #列为外层
#下面看有缺失值的情况,unstack()会标示出缺失值
s1 = Series([0,1,2,3],index = [list('abcd')])
s2 = Series([4,5,6],index = ['c','d','e'])
data2 = pd.concat([s1,s2],keys = ['one','two'])
print data2
print data2.unstack(),'\n'
#stack会滤除缺失数据
print data2.unstack().stack(),'\n'
print data2.unstack().stack(dropna = False) ,'\n' #保留缺失值
#对DataFrame进行unstack时，作为旋转轴的级别成为结果中最低的,弄到最内层
df = DataFrame({'left':result,'right':result + 5},columns = pd.Index(['left','right'],name = 'side'))
print 'df is \n',df
print 'df.unstack is \n',df.unstack('state')
print 'df.unstack.stack \n',df.unstack('state').stack('side')
#+END_SRC


    将“长格式”转换为“宽格式”


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

#时间序列中的数据通常是以所谓“长格式”（long）或“堆叠格式”（stacked）存储在数据库和csv中
#由于没有找到数据，自己动手写一点
ldata = DataFrame({'date':['03-31','03-31','03-31','06-30','06-30','06-30'],
    'item':['real','infl','unemp','real','infl','unemp'],'value':['2710.','000.','5.8','2778.','2.34','5.1']})
print 'ldata is \n',ldata
#下面就是将data、item作为行、列名，value填充进二维表
pivoted = ldata.pivot('date','item','value')
print 'pivoted is \n',pivoted
ldata['value2'] = np.random.randn(len(ldata))
print 'ldata is \n',ldata
#看一下下面的结果,得到的列就有了层次化列表
pivoted = ldata.pivot('date','item')
print pivoted
print 'pivoted is \n',pivoted['value'],'\n'
#换一种试试,下面的就将value2填充，value就丢弃了
pivoted1 = ldata.pivot('date','item','value2')
print pivoted1
#注意，pivot其实只是一个“快捷方式而已”，用set_index创建层次化索引，再用unstack重塑
unstacked = ldata.set_index(['date','item']).unstack('item') #unstack标明展开的轴
print unstacked
#+END_SRC

*  第七章 数据规整化：清理、转换、合并、重塑（二）

3、数据转换
介绍完数据的重排之后，下面介绍数据的过滤、清理、以及其他转换工作。
去重


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

#DataFrame去重
data = DataFrame({'k1':['one']*3 + ['two'] * 4,
    'k2':[1,1,2,3,3,4,4,]})
#print data
print data.duplicated() #返回一个布尔型Series，重复的为True，不重复的为False
#得到去重之后的DataFrame，应该意识到这是非常常用的
print data.drop_duplicates().reset_index(drop = True)
#可以选定需要去重的列
print data.drop_duplicates(['k1']) #默认保留第一次出现的行
print data.drop_duplicates(['k1'],take_last = True) #设定保留最后一个出现的行
>>>
0    False
1     True
2    False
3    False
4     True
5    False
6     True
    k1  k2
0  one   1
1  one   2
2  two   3
3  two   4
    k1  k2
0  one   1
3  two   3
    k1  k2
2  one   2
6  two   4
[Finished in 0.7s]
#+END_SRC


利用函数或者映射进行数据转换


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

data = DataFrame({'food':['bacon','pulled pork','bacon','Pastrami','corned beef','Bacon','pastrami',
    'honey ham','nova lox'],'ounces':[4,3,12,6,7.5,8,3,5,6]})
print data
#假如你想添加一列表示该肉类食物来源的动物类型，我们先编写一个肉类到动物的映射。
meat_to_animal = {
    'bacon':'pig',
    'pulled pork':'pig',
    'pastrami':'cow',
    'corned beef':'cow',
    'honey ham':'pig',
    'nova lox':'salmon'
}
#Series的map方法可以接受一个函数或含有映射关系的字典型对象，但是这里有个问题：有些大写了，
#有些没有。因此需要先转换大小写（注意数据清理过程）,感觉这方法很实用
data['animal'] = data['food'].map(str.lower).map(meat_to_animal)
print data
#下面看一下map用来执行函数，即将data['food']的每个元素应用到隐含函数
print data['food'].map(lambda x:meat_to_animal[x.lower()])
>>>
          food  ounces
0        bacon     4.0
1  pulled pork     3.0
2        bacon    12.0
3     Pastrami     6.0
4  corned beef     7.5
5        Bacon     8.0
6     pastrami     3.0
7    honey ham     5.0
8     nova lox     6.0
          food  ounces  animal
0        bacon     4.0     pig
1  pulled pork     3.0     pig
2        bacon    12.0     pig
3     Pastrami     6.0     cow
4  corned beef     7.5     cow
5        Bacon     8.0     pig
6     pastrami     3.0     cow
7    honey ham     5.0     pig
8     nova lox     6.0  salmon
0       pig
1       pig
2       pig
3       cow
4       cow
5       pig
6       cow
7       pig
8    salmon
Name: food
[Finished in 0.8s]

#+END_SRC

替换值


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

#下面看replace函数
data = Series([1.,-999.,2.,-999.,-1000.,3.])
print data
#用replace替换-999、-1000，注意Series可以直接用，相当于矢量化了
print data.replace([-999,-1000],np.nan)
#下面看一下numpy，不能直接用replace和map
#data1 = np.arange(10)
#print data1.replace(0,np.nan)
#print data1.map(lambda x: x + 1)
print data.replace([-999,-1000],[np.nan,0])
print data.replace({-999:np.nan,-1000:0})
>>>
0       1
1    -999
2       2
3    -999
4   -1000
5       3
0     1
1   NaN
2     2
3   NaN
4   NaN
5     3
0     1
1   NaN
2     2
3   NaN
4     0
5     3
0     1
1   NaN
2     2
3   NaN
4     0
5     3
[Finished in 0.8s]

#+END_SRC

重命名轴索引
跟Series的值一样，轴标签可以通过函数或者映射进行转换，从而得到一个新对象，轴还可以被就地修改，而无需新建一个数据结构。


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

data = DataFrame(np.arange(12).reshape((3,4)),index = ['Ohio','Colorado','New York'],
    columns = ['one','two','three','four'])
print data
#轴标签的map方法
print data.index.map(str.upper)
#就地修改
data.index = data.index.map(str.upper)
print data
#下面用rename得到一个副本
print data.rename(index = str.title,columns = str.upper)
#rename可以结合字典对象进行更新
print data.rename(index = {'OHIO':'INDIANA'},columns = {'three':'peekaboo'})
#rename可以将DataFrame的索引和标签进行复制和赋值
#就地修改
_ = data.rename(index = {'OHIO':'INDIANA'},inplace = True)
print data
print '\n',type(_)
print _
>>>
          one  two  three  four
Ohio        0    1      2     3
Colorado    4    5      6     7
New York    8    9     10    11
[OHIO COLORADO NEW YORK]
          one  two  three  four
OHIO        0    1      2     3
COLORADO    4    5      6     7
NEW YORK    8    9     10    11
          ONE  TWO  THREE  FOUR
Ohio        0    1      2     3
Colorado    4    5      6     7
New York    8    9     10    11
          one  two  peekaboo  four
INDIANA     0    1         2     3
COLORADO    4    5         6     7
NEW YORK    8    9        10    11
          one  two  three  four
INDIANA     0    1      2     3
COLORADO    4    5      6     7
NEW YORK    8    9     10    11
    <class 'pandas.core.frame.DataFrame'>
              one  two  three  four
    INDIANA     0    1      2     3
    COLORADO    4    5      6     7
    NEW YORK    8    9     10    11
    [Finished in 0.8s]

#+END_SRC

离散化和面元划分
为了便于分析，连续数据常常被离散化或拆分为面元（bin），即分组。


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

ages = [20,22,25,27,21,23,37,31,61,45,41,32]
bins = [18,25,35,60,100]
#用的是cut函数
cats = pd.cut(ages,bins)
print cats
#返回的是一个特殊的Categorical对象，可以看作是表示面元名称的字符串。
#它含有一个表示不同分类名称的levels数组以及一个labels属性：
print cats.labels  #是分组的序号，标示为第几组
print cats.levels
print pd.value_counts(cats)
#得到的是几个“区间”，不包括左，包括右，可用right = False包括左，不包括右
print pd.cut(ages,[18,26,36,61,100],right = False)
#可以设置自己的面元名称，设置label是即可
group_names = ['Youth','YoungAdult','MiddleAged','Senior']
print pd.cut(ages,bins,labels = group_names)
#当然可以为cut传入面元的数量而不是具体的分界点，会自动均匀分布
data = np.random.randn(20)
print data
#下面标识分为4组，精度为2位
print pd.cut(data,4,precision = 2)
#qcut函数是一个类似于cut的函数，可以根据样本分位数对数据进行面元划分。根据数据，cut可能无法
#是各个面元数量数据点相同，qcut使用的是样本分位数，因此可以得大小基本相等的面元。
data = np.random.randn(1000)
cats = pd.qcut(data,4) #四份位数进行切割
print cats
print pd.value_counts(cats)
#当然可以设置自定义的分位数（0到1的值）
print pd.qcut(data,[0,0.1,0.5,0.9,1.])
>>>
Categorical: 
array([(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], (18, 25],
       (35, 60], (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]], dtype=object)
Levels (4): Index([(18, 25], (25, 35], (35, 60], (60, 100]], dtype=object)
[0 0 0 1 0 0 2 1 3 2 2 1]
array([(18, 25], (25, 35], (35, 60], (60, 100]], dtype=object)
(18, 25]     5
(35, 60]     3
(25, 35]     3
(60, 100]    1
Categorical: 
array([[18, 26), [18, 26), [18, 26), [26, 36), [18, 26), [18, 26),
       [36, 61), [26, 36), [61, 100), [36, 61), [36, 61), [26, 36)], dtype=object)
Levels (4): Index([[18, 26), [26, 36), [36, 61), [61, 100)], dtype=object)
Categorical: 
array([Youth, Youth, Youth, YoungAdult, Youth, Youth, MiddleAged,
       YoungAdult, Senior, MiddleAged, MiddleAged, YoungAdult], dtype=object)
Levels (4): Index([Youth, YoungAdult, MiddleAged, Senior], dtype=object)
Categorical: 
array([(-0.5, 0.66], (0.66, 1.82], (0.66, 1.82], (-0.5, 0.66],
       (-1.67, -0.5], (0.66, 1.82], (-0.5, 0.66], (-1.67, -0.5],
       (0.66, 1.82], (-1.67, -0.5], (-1.67, -0.5], (-1.67, -0.5],
       (-1.67, -0.5], (-0.5, 0.66], (-0.5, 0.66], (-0.5, 0.66],
       (-0.5, 0.66], (1.82, 2.98], (-0.5, 0.66], (-0.5, 0.66]], dtype=object)
Levels (4): Index([(-1.67, -0.5], (-0.5, 0.66], (0.66, 1.82],
                   (1.82, 2.98]], dtype=object)
[-3.161, -0.624]    250
(0.69, 2.982]       250
(0.0578, 0.69]      250
(-0.624, 0.0578]    250
[Finished in 0.7s]

#+END_SRC

检测和过滤异常值
异常值（outlier）的过滤或变换运算在很大程度上其实就是数组运算。


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

np.random.seed(12345)
data = DataFrame(np.random.randn(1000,4))
print data.describe()
#假设想要找出某些列中绝对值大小超过3的值
col = data[3]
#print col
print col[np.abs(col) > 3]
#找出全部含有超过3或-3的值的行
print data[(np.abs(data) > 3).any(1)]
#对上面的这样的值限制在-3到3内
data[np.abs(data) > 3] = np.sign(data) * 3
print data.describe()
>>>
                 0            1            2            3
count  1000.000000  1000.000000  1000.000000  1000.000000
mean     -0.067684     0.067924     0.025598    -0.002298
std       0.998035     0.992106     1.006835     0.996794
min      -3.428254    -3.548824    -3.184377    -3.745356
25%      -0.774890    -0.591841    -0.641675    -0.644144
50%      -0.116401     0.101143     0.002073    -0.013611
75%       0.616366     0.780282     0.680391     0.654328
max       3.366626     2.653656     3.260383     3.927528
97     3.927528
305   -3.399312
400   -3.745356
Name: 3
            0         1         2         3
5   -0.539741  0.476985  3.248944 -1.021228
97  -0.774363  0.552936  0.106061  3.927528
102 -0.655054 -0.565230  3.176873  0.959533
305 -2.315555  0.457246 -0.025907 -3.399312
324  0.050188  1.951312  3.260383  0.963301
400  0.146326  0.508391 -0.196713 -3.745356
499 -0.293333 -0.242459 -3.056990  1.918403
523 -3.428254 -0.296336 -0.439938 -0.867165
586  0.275144  1.179227 -3.184377  1.369891
808 -0.362528 -3.548824  1.553205 -2.186301
900  3.366626 -2.372214  0.851010  1.332846
                 0            1            2            3
count  1000.000000  1000.000000  1000.000000  1000.000000
mean     -0.067623     0.068473     0.025153    -0.002081
std       0.995485     0.990253     1.003977     0.989736
min      -3.000000    -3.000000    -3.000000    -3.000000
25%      -0.774890    -0.591841    -0.641675    -0.644144
50%      -0.116401     0.101143     0.002073    -0.013611
75%       0.616366     0.780282     0.680391     0.654328
max       3.000000     2.653656     3.000000     3.000000
[Finished in 0.8s]

#+END_SRC

排列和随机采样
下面是随机选取一个DataFrame的一些行，做法就是随机产生行号，然后进行选取即可。


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

df = DataFrame(np.arange(5 * 4).reshape(5,4))
sampler = np.random.permutation(5)  #返回一个随机排列
print df
print sampler
#然后可以在基于ix的索引操作或者take函数中使用该数组
print df.take(sampler)
#作者这里说了非替换式采样，我理解就是不重复采样吧！
#下面是进行截取
print df.take(np.random.permutation(len(df))[:3])
bag = np.array([5,7,-1,6,4])
sampler = np.random.randint(0,len(bag),size = 10)
print sampler
draws = bag.take(sampler)
print draws
>>>
    0   1   2   3
0   0   1   2   3
1   4   5   6   7
2   8   9  10  11
3  12  13  14  15
4  16  17  18  19
[3 2 1 0 4]
    0   1   2   3
3  12  13  14  15
2   8   9  10  11
1   4   5   6   7
0   0   1   2   3
4  16  17  18  19
    0   1   2   3
4  16  17  18  19
0   0   1   2   3
3  12  13  14  15
[1 0 1 3 4 3 3 2 0 2]
[ 7  5  7  6  4  6  6 -1  5 -1]
[Finished in 0.7s]

#+END_SRC

计算指标/哑变量
另一种常用的用于统计建模或机器学习的转换方式是：将分类变量（categorical variable）转换为“哑变量矩阵”(dummy matrix)或“指标矩阵”（indicator matrix）。如果DataFrame的某一列有k各不同的值，可以派生出一个k列的矩阵或者DataFrame（值为1和0）。这样的做法在下一章（第八章）的地图的例子中有体现（谁让我先看的第八章，当时还在想这个办法好，原来根源在这里）。
movie_id
title
genres
1
Toy Story (1995
Animation|Children's|Comedy
2
Jumanji (1995
Adventure|Children's|Fantasy
3
Grumpier Old Men (1995
Comedy|Romance
4
Waiting to Exhale (1995
Comedy|Drama
5
Father of the Bride Part II (1995
Comedy


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

df = DataFrame({'key':['b','b','a','c','a','b'],'data1' : range(6)})
print df
print pd.get_dummies(df['key'])  #得到哑变量DataFrame
#有时候，你想给指标DataFrame的列加上一个前缀，一边进行合并。
#这个功能好,但是注意是给指标DataFrame的列名加的前缀
dummies = pd.get_dummies(df['key'],prefix = 'key')
print dummies
df_with_dummy = df[['data1']].join(dummies)  #按行索引合并
print df_with_dummy
#这里说一个隐藏的trick，df['data1']得到一个Series，而df[['data1']]得到一个DataFrame
print type(df['data1'])    #Series而已，列名丢掉
print type(df[['data1']])  #DataFrame 是有列名的

#下面看如果DataFrame的某行属于多个分类怎么办，利用ch02中的MovieLens数据。
names = ['movie_id','title','genres']
movies = pd.read_table('E:\\movies.dat',sep = '::',header = None,names = names)
print movies[:10]
#要为genre添加指标变量的时候需要先进性数据规整。
#首先把所有genres提取出来
genre_iter = (set(x.split('|')) for x in movies.genres)
genres = sorted(set.union(*genre_iter))
dummies = DataFrame(np.zeros((len(movies),len(genres))),columns = genres)
#接下来，迭代每一部电影并将dummies各行的项设置为1
for i,gen in enumerate(movies.genres):
    dummies.ix[i,gen.split('|')] = 1
#然后与movies合并起来
movies_windic = movies.join(dummies.add_prefix('Genre_'))
print movies_windic.ix[0]
#但是对于河大的数据，这种方法构建指标非常慢。肯定需要编写一个能够利用DataFrame内部机制的更低级的函数才行
#一个对统计应用的秘诀是：结合get_dummies和诸如cut之类的离散化函数
values = np.random.rand(10)
print values
bins = [0,0.2,0.4,0.6,0.8,1]
print pd.get_dummies(pd.cut(values,bins))

#+END_SRC

4、字符串操作
Python有简单易用的字符串和文本处理功能。大部分文本运算直接做成了字符串对象的内置方法。当然还能用正则表达式。pandas对此进行了加强，能够对数组数据应用字符串表达式和正则表达式，而且能处理烦人的缺失数据。
字符串对象方法


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

#字符串对象方法
#对于大部分的字符串而言，内置的方法已经能够满足要求了
val = 'a,b, guido'
print val.split(',')  #返回的是一个列表
pieces = [x.strip() for x in val.split(',')]  #strip函数修剪空白字符
print pieces
#利用加法可以把字符串连接起来,注意下面的赋值方式
first,second,third = pieces
print first +'::' + second +'::'+ third
#上面的不实用，下面是一种更快的风格
print '::'.join(pieces)
#另一种方法是字串定位，常用的有 in、index、find
print 'guido' in val  #返回布尔型，是否在字符串中
print val.index(',')  #返回第一次出现的位置，找不到返回异常
print val.find(':')   #返回第一次出现字符的位置，找不到返回-1，可以指定从哪个位置开始和结束
print val.count(',')  #返回个数
print val.replace(',','::')
print val.replace(',','') #传入''用来删除字符 
>>>
['a', 'b', ' guido']
['a', 'b', 'guido']
a::b::guido
a::b::guido
True
1
-1
2
a::b:: guido
ab guido
[Finished in 0.6s]
#上面这些都能用正则表达式实现

#+END_SRC

Python内置的字符串方法有：


正则表达式
正则表达式（regex）提供了一种灵活的在文本中搜索、匹配字符串的模式。用的是re模块。re模块的函数分为3类：模式匹配、替换、拆分。关于正则表达式的总结，参考一下：http://www.cnblogs.com/huxi/archive/2010/07/04/1771073.html （谢谢作者）。


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame
import re

text = "foo  bar\t baz  \tqux"
print re.split('\s+',text) #这条语句是先编译正则表达式 \s+ (多个空白字符)，然后再调用split
regex = re.compile('\s+')
print regex.split(text)
#下面是找到匹配regex的所有模式
print regex.findall(text)
#注意:想转义字符\不起作用，即作为一个单独字符，可以直接在前面加r，原生字符串
text1 = r'foo \t'
print text1
#如果想对许多字符串都应用同一条正则表达式，应该先compile节省时间
#findall 返回字符串中所有匹配项，而search则只返回第一个匹配项。match更加严格，它只匹配字符串的首部
text = """Dave dave@google.com
Steve steve@gmail.com
Rob rob@gmail.com
Ryan ryan@yahoo.com
"""
pattern = r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,4}'
#下面第二个参数的作用是正则对大小写不敏感
regex = re.compile(pattern,flags = re.IGNORECASE)
print regex.findall(text)  #返回一个list
#search返回第一个邮件地址，返回的是一种特殊特殊对象，这个对象只能告诉我们模式在原始字符串中的起始和结束位置
m = regex.search(text)
print m
print text[m.start():m.end()]
#regex.match返回None，因为它只匹配出现在字符串开头的模式，也就是说，无法指定要开始和结束的匹配位置
print regex.match(text)
#还有一个sub方法，会将匹配到的模式替换为指定字符串，并返回新字符串
print regex.sub('REDACTED',text)
#另外，如果想将找出的模式分段，用圆括号括起来即可
pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\.([A-Z]{2,4})'
regex = re.compile(pattern,flags = re.IGNORECASE)
m = regex.match('wesm@bright.net')  #返回一个match对象
print m
print m.groups()  #弄成元组的形式输出
print regex.findall(text) #返回一个列表，每一项都是一个元组
print regex.sub(r'Username: \1, Domain: \2, Suffix: \3',text)  #sub可以利用\1 \2 \3访问被替换的字符串
#看下面的小例子
regex = re.compile(r"""
    (?P<username>[A-Z0-9._%+-]+)
    @
    (?P<domain>[A-Z0-9.-]+)
    \.
    (?P<suffix>[A-Z]{2,4})""",flags = re.IGNORECASE|re.VERBOSE)

#这样可以得到一个简单的字典
m = regex.match('wesm@bright.net')
print m.groupdict()
>>>
['foo', 'bar', 'baz', 'qux']
['foo', 'bar', 'baz', 'qux']
['  ', '\t ', '  \t']
foo \t
['dave@google.com', 'steve@gmail.com', 'rob@gmail.com', 'ryan@yahoo.com']
<_sre.SRE_Match object at 0x03343758>
dave@google.com
None
Dave REDACTED
Steve REDACTED
Rob REDACTED
Ryan REDACTED
    <_sre.SRE_Match object at 0x03342A70>
    ('wesm', 'bright', 'net')
    [('dave', 'google', 'com'), ('steve', 'gmail', 'com'), ('rob', 'gmail', 'com'), ('ryan', 'yahoo', 'com')]
    Dave Username: dave, Domain: google, Suffix: com
    Steve Username: steve, Domain: gmail, Suffix: com
    Rob Username: rob, Domain: gmail, Suffix: com
    Ryan Username: ryan, Domain: yahoo, Suffix: com
    {'username': 'wesm', 'domain': 'bright', 'suffix': 'net'}
    [Finished in 0.8s]

#+END_SRC

    正则表达式的方法有：

pandas中矢量化字符串函数


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame
import re

data = {'Dave':'dave@google.com','Steve':'steve@gmail.com','Rob':'rob@gmail.com','Web':np.nan}
data = Series(data)
print data
print data.isnull()
#通过map，所有字符串和正则都能传入各个值（通过lambda或者其他函数），但是如果存在NA就会报错。
#然而，Series有些跳过NA的方法。通过Series的str属性可以访问这些方法。
print '\n',data.str.contains('gmail'),'\n' #查看是否每个包含gmail
pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\.([A-Z]{2,4})'
print data.str.findall(pattern,flags = re.IGNORECASE),'\n'
#print data.str.replace('@','')  #这里的replace可以矢量化应用到每个元素
#有两个办法可以实现矢量化的元素获取操作：要么使用str.get,要么在str属性上用索引
matches = data.str.match(pattern,flags = re.IGNORECASE)
print matches,'\n'
print matches.str.get(1),'\n'
print matches.str[0],'\n'
#可以这样进行截取
print data.str[:5],'\n'
#下面这样只是选取前两个
print data[:2]
>>>
Dave     dave@google.com
Rob        rob@gmail.com
Steve    steve@gmail.com
Web                  NaN
Dave     False
Rob      False
Steve    False
Web       True
    Dave     False
    Rob       True
    Steve     True
    Web        NaN
    Dave     [('dave', 'google', 'com')]
    Rob        [('rob', 'gmail', 'com')]
    Steve    [('steve', 'gmail', 'com')]
    Web                              NaN
    Dave     ('dave', 'google', 'com')
    Rob        ('rob', 'gmail', 'com')
    Steve    ('steve', 'gmail', 'com')
    Web                            NaN
    Dave     google
    Rob       gmail
    Steve     gmail
    Web         NaN
    Dave      dave
    Rob        rob
    Steve    steve
    Web        NaN
    Dave     dave@
    Rob      rob@g
    Steve    steve
    Web        NaN
    Dave    dave@google.com
    Rob       rob@gmail.com
    [Finished in 0.7s]

#+END_SRC

下面矢量化的字符串方法，比较重要。


*  第七章 数据规整化：清理、转换、合并、重塑（三）

5、示例：usda食品数据库
下面是一个具体的例子，书中最重要的就是例子。 


#+BEGIN_SRC python

  #-*- encoding: utf-8 -*-
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  from pandas import Series,DataFrame
  import re
  import json

  #加载下面30M+的数据
  db = json.load(open('E:\\foods-2011-10-03.json'))
  #print len(db)
  #print type(db)  #得到的db是个list,每个条目都是含有某种食物全部数据的字典
  #print db[0]  #这一条非常长
  #print db[0].keys()
  #nutrients 是keys中的一个key，它对应的值是有关食物营养成分的一个字典列表，很长……
  #print db[0]['nutrients'][0]
  #下面将营养成分做成DataFrame
  nutrients = DataFrame(db[0]['nutrients'])  #将字典列表直接做成DataFrame
  #print nutrients.head()
  #print type(db[0]['nutrients'])
  info_keys = ['description','group','id','manufacturer']
  info = DataFrame(db,columns = info_keys)
  #print info
  #查看分类分布情况
  #print pd.value_counts(info.group)
  #现在，为了将所有的营养数据进行分析，需要将所有营养成分整合到一个大表中，下面分几个步骤来完成
  nutrients = []

  for rec in db:
      fnuts = DataFrame(rec['nutrients'])
      fnuts['id'] = rec['id']  #广播
      nutrients.append(fnuts)
  nutrients = pd.concat(nutrients,ignore_index = True) #将列表连接起来，相当于rbind，把行对其连接在一起

  #去重,这是数据处理的重要步骤
  print nutrients.duplicated().sum()
  nutrients = nutrients.drop_duplicates()
  #由于nutrients与info有重复的名字，所以需要重命名一下info
  #注意下面这样的命名方式
  col_mapping = {'description':'food',
  'group':'fgroup'}
  #rename函数返回的是副本，需要copy = False
  info = info.rename(columns = col_mapping,copy = False)
  #print info.columns #查看一下列名
  col_mapping = {'description':'nutrient','group':'nutgroup'}
  nutrients = nutrients.rename(columns = col_mapping,copy = False)
  #print nutrients.columns 
  #做完上面这些，显然我们需要将两个DataFrame合并起来
  print nutrients.ix[:10,:]
  #print info.id
  ndata = pd.merge(nutrients,info,on = 'id',how = 'outer')
  print ndata
  print ndata.ix[3000]
  #注意下面的处理方式很nice
  result = ndata.groupby(['nutrient','fgroup'])['value'].quantile(0.5)
  print result
  result['Zinc, Zn'].order().plot(kind = 'barh')
  plt.show()
  #只要稍微动动脑子（作者不止一次说过了……额），就可以发现各营养成分最为丰富的食物是什么了
  by_nuttriend = ndata.groupby(['nutgroup','nutrient'])
  print by_nuttriend.head()
  #注意下面取出最大值的方式
  get_maximum = lambda x:x.xs(x.value.idxmax())
  get_minimum = lambda x:x.xs(x.value.idxmin())
  max_foods = by_nuttriend.apply(get_maximum)[['value','food']]
  #让food小一点
  max_foods.food = max_foods.food.str[:50]
  print max_foods.head()
  print max_foods.ix['Amino Acids']['food']
  >>>
  14179
                         nutrient     nutgroup units    value    id
  0                       Protein  Composition     g    25.18  1008
  1             Total lipid (fat)  Composition     g    29.20  1008
  2   Carbohydrate, by difference  Composition     g     3.06  1008
  3                           Ash        Other     g     3.28  1008
  4                        Energy       Energy  kcal   376.00  1008
  5                         Water  Composition     g    39.28  1008
  6                        Energy       Energy    kJ  1573.00  1008
  7          Fiber, total dietary  Composition     g     0.00  1008
  8                   Calcium, Ca     Elements    mg   673.00  1008
  9                      Iron, Fe     Elements    mg     0.64  1008
  10                Magnesium, Mg     Elements    mg    22.00  1008
  <class 'pandas.core.frame.DataFrame'>
  Int64Index: 375176 entries, 0 to 375175
  Data columns:
  nutrient        375176  non-null values
  nutgroup        375176  non-null values
  units           375176  non-null values
  value           375176  non-null values
  id              375176  non-null values
  food            375176  non-null values
  fgroup          375176  non-null values
  manufacturer    293054  non-null values
  dtypes: float64(1), int64(1), object(6)
  nutrient                 Glycine
  nutgroup             Amino Acids
  units                          g
  value                      0.073
  id                          1077
  food            Spearmint, fresh
  fgroup          Spices and Herbs
  manufacturer                    
  Name: 3000
  nutrient          fgroup                           
  Adjusted Protein  Sweets                               12.900
                    Vegetables and Vegetable Products     2.180
  Alanine           Baby Foods                            0.085
                    Baked Products                        0.248
                    Beef Products                         1.550
                    Beverages                             0.003
                    Breakfast Cereals                     0.311
                    Cereal Grains and Pasta               0.373
                    Dairy and Egg Products                0.271
                    Ethnic Foods                          1.290
                    Fast Foods                            0.514
                    Fats and Oils                         0.000
                    Finfish and Shellfish Products        1.218
                    Fruits and Fruit Juices               0.027
                    Lamb, Veal, and Game Products         1.408
  ...
  Zinc, Zn  Finfish and Shellfish Products       0.67
            Fruits and Fruit Juices              0.10
            Lamb, Veal, and Game Products        3.94
            Legumes and Legume Products          1.14
            Meals, Entrees, and Sidedishes       0.63
            Nut and Seed Products                3.29
            Pork Products                        2.32
            Poultry Products                     2.50
            Restaurant Foods                     0.80
            Sausages and Luncheon Meats          2.13
            Snacks                               1.47
            Soups, Sauces, and Gravies           0.20
            Spices and Herbs                     2.75
            Sweets                               0.36
            Vegetables and Vegetable Products    0.33
  Length: 2246
  <class 'pandas.core.frame.DataFrame'>
  MultiIndex: 467 entries, (u'Amino Acids', u'Alanine', 48) to (u'Vitamins', u'Vitamin K (phylloquinone)', 395)
  Data columns:
  nutrient        467  non-null values
  nutgroup        467  non-null values
  units           467  non-null values
  value           467  non-null values
  id              467  non-null values
  food            467  non-null values
  fgroup          467  non-null values
  manufacturer    444  non-null values
  dtypes: float64(1), int64(1), object(6)
                              value                                          food
  nutgroup    nutrient                                                           
  Amino Acids Alanine         8.009             Gelatins, dry powder, unsweetened
              Arginine        7.436                  Seeds, sesame flour, low-fat
              Aspartic acid  10.203                           Soy protein isolate
              Cystine         1.307  Seeds, cottonseed flour, low fat (glandless)
              Glutamic acid  17.452                           Soy protein isolate
  nutrient
  Alanine                           Gelatins, dry powder, unsweetened
  Arginine                               Seeds, sesame flour, low-fat
  Aspartic acid                                   Soy protein isolate
  Cystine                Seeds, cottonseed flour, low fat (glandless)
  Glutamic acid                                   Soy protein isolate
  Glycine                           Gelatins, dry powder, unsweetened
  Histidine                Whale, beluga, meat, dried (Alaska Native)
  Hydroxyproline    KENTUCKY FRIED CHICKEN, Fried Chicken, ORIGINA...
  Isoleucine        Soy protein isolate, PROTEIN TECHNOLOGIES INTE...
  Leucine           Soy protein isolate, PROTEIN TECHNOLOGIES INTE...
  Lysine            Seal, bearded (Oogruk), meat, dried (Alaska Na...
  Methionine                    Fish, cod, Atlantic, dried and salted
  Phenylalanine     Soy protein isolate, PROTEIN TECHNOLOGIES INTE...
  Proline                           Gelatins, dry powder, unsweetened
  Serine            Soy protein isolate, PROTEIN TECHNOLOGIES INTE...
  Threonine         Soy protein isolate, PROTEIN TECHNOLOGIES INTE...
  Tryptophan         Sea lion, Steller, meat with fat (Alaska Native)
  Tyrosine          Soy protein isolate, PROTEIN TECHNOLOGIES INTE...
  Valine            Soy protein isolate, PROTEIN TECHNOLOGIES INTE...
  Name: food
  [Finished in 14.1s]
#+END_SRC

*  第八章 绘图和可视化

python有许多可视化工具，本书主要讲解matplotlib。matplotlib是用于创建出版质量图表的桌面绘图包（主要是2D方面）。matplotlib的目的是为了构建一个MATLAB式的绘图接口。本书中的大部分图都是用它生成的。除了图形界面显示，还可以把图片保存为pdf、svg、jpg、png、gif等形式。 
1、matplotlib API入门
Ipython可以用close（）关闭界面。
Figure和Subplot
matplotlib的图像都位于Figure对象中。用plt.figure创建一个新的Figure。


#+BEGIN_SRC python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#plt.plot(np.arange(10))
fig = plt.figure()
#plt.show()
#figsize 有一些重要的选项，特别是figsize，规定的是图片保存到磁盘时具有一定大小的纵横比。
#plt.gcf()即可得到当前Figure的引用
ax1 = fig.add_subplot(2,2,1)
ax2 = fig.add_subplot(2,2,2)
ax3 = fig.add_subplot(2,2,3)
plt.plot(np.random.randn(50).cumsum(),'k--')


#fig.add_subplot 返回的对象是AxesSubplot对象，下面调用就可以了
_ = ax1.hist(np.random.randn(100),bins = 20,color = 'k',alpha = 0.3)
ax2.scatter(np.arange(30),np.arange(30) + 3 * np.random.randn(30))
plt.show()

#由于Figure 和 subplot是一件非常常见的任务，于是出现了更为方便的方法（plt.subplots ),它可以创建一个新的Figure，
#并返回一个含有已创建的subplot对象的Numpy数组
fig,axes = plt.subplots(2,3)

#print fig
print axes[0][0]
#axes[0][0].hist(np.random.randn(100),bins = 20,color = 'k',alpha = 0.3)
plt.show()
#这是非常实用的，因为可以轻松地对axes数组进行索引，就好像一个是一个二维数组一样，例如
#axes[0,1].还可以通过sharex和sharey指定subplot具有相同的x轴和y轴。在比较相同范围的数据时，这是
#非常实用的，否则matplotlib会自动缩放各图表的界限。

#+END_SRC

看一下subplots的作用：

pyplot.subplots的选项还有：

上面的**fig_k可以有很多的参数，文档中有更多的内容。
调整subplot周围的间距
默认情况下，matplotlib会在subplot外围留下一定的边距，并在subplot之间留下一定的间距。间距和图像的高度和宽度有关，会自动调整。利用Figure的subplots——adjust方法可以修改间距，因此，它是一个顶级函数。


#+BEGIN_SRC python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

subplots_adjust(left = None,bottom = None,right = None,top = None,wspace = None,hspace = None)
#wspace和space用于控制宽度和高度的百分比，可以用做subplot之间的间距，下面是个例子：
fig,ax = plt.subplots(2,2,sharex = True,sharey = True)
for i in range(2):
     for j in range(2):
          ax[i,j].hist(np.random.randn(500),bins = 50,color = 'k',alpha = 0.5)
plt.subplots_adjust(wspace = 0.5,hspace = 0.5)
plt.show()
#matplotlib不会检查标签的重叠（确实是这样）。
#+END_SRC

#+BEGIN_SRC python



# -*- encoding: UTF-8 -*- 
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt



fig,ax = plt.subplots(2,2)

#cecece是白色……
ax[0,0].plot(np.arange(10),linestyle = '--',color = '#CECECE')

#线上面还可以添加一些标记（marker),以强调实际的数据点。由于matplotlib创建的是连续的线形图,因此有时可能不太容易看到真实点的位置，标记可以放到格式字符串中，但是标记类型和线性必须在颜色的后面
ax[0,1].plot(np.random.randn(30).cumsum(),'ko--')
ax[1,0].plot(np.random.randn(30).cumsum(),color = 'k',linestyle = '--',marker = 'o')
#在线型图中，非实际数据点默认是按照线性插值的，可以通过drawstyle选项修改这一点。
data = np.random.randn(30).cumsum()
ax[1,1].plot(data,'ko--')
ax[1,1].plot(data,'k--',drawstyle = 'steps-post')
plt.show()

#+END_SRC

注意上面的drawstyle选项可以规定点与点之间的连接方式，或者说是插值方式，结果为：

设置标题、轴标签、刻度以及刻度标签

#+BEGIN_SRC python

# -*- encoding: UTF-8 -*- 
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import numpy.random as npr

fig = plt.figure()
ax = fig.add_subplot(1,1,1)
ax.plot(npr.randn(1000).cumsum())

#要想修改x的刻度，最简单的方法就是使用set_xticks和set_xticklabels.前者告诉matplotlib将
#刻度放在数据范围中的哪些位置，默认情况下，这些位置就是刻度标签。但是可以使用set_xticklabels将#任何其他的值用作标签
ticks = ax.set_xticks([0,250,500,700,900,1000])
#下面的totation是规定旋转角度
labels = ax.set_xticklabels(['a','b','c','d','e','f'],rotation = 30,fontsize = 'small')
#可以为x轴设置名称
ax.set_xlabel('Stages')

plt.show()
#+END_SRC



图例

#+BEGIN_SRC python

# -*- encoding: UTF-8 -*- 
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import numpy.random as npr
from datetime import datetime

#添加图例
fig = plt.figure()
ax = fig.add_subplot(1,1,1)
ax.plot(npr.randn(1000).cumsum(),'k',label = 'one')
ax.plot(npr.randn(1000).cumsum(),'k--',label = 'two')
ax.plot(npr.randn(1000).cumsum(),'k.',label = 'three')
ax.legend(loc = 'best')
plt.show()
#+END_SRC



注解与绘图

#+BEGIN_SRC python

# -*- encoding: UTF-8 -*- 
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import numpy.random as npr
from datetime import datetime


fig = plt.figure()
ax = fig.add_subplot(1,1,1)

data = pd.read_csv('E:\\spx.csv',index_col = 0,parse_dates = True)
spx = data['SPX']
spx.plot(ax = ax,style = 'k-')

crisis_data = [
(datetime(2007,10,11),'Peak of bull market'),
(datetime(2008,3,12),'Bear Stearns Fails'),
(datetime(2008,9,15),'Lehman Bankruptcy')
]

for date,label in crisis_data:
    ax.annotate(label,xy = (date,spx.asof(date) + 50),
        xytext = (date,spx.asof(date) + 200),
        arrowprops = dict(facecolor = 'black'),
        horizontalalignment = 'left',verticalalignment = 'top')
ax.set_xlim(['1/1/2007','1/1/2011'])
ax.set_ylim([600,1800])

ax.set_title('Important dates in 2008-2009 finacial crisis')
plt.show()
#更多关于注解的示例，请看文档

#图形的绘制要麻烦些，有一些常见的图形的对象，这些对象成为块（patch）
#如Rectangle 和 Circle，完整的块位于matplotlib.patches
#要绘制图形，需要创建一个块对象shp，然后通过ax.add_patch(shp)将其添加到subplot中

fig = plt.figure()
ax = fig.add_subplot(1,1,1)

rect = plt.Rectangle((0.2,0.75),0.4,0.15,color = 'k',alpha = 0.3)
circ = plt.Circle((0.7,0.2),0.15,color = 'b',alpha = 0.3)
pgon = plt.Polygon([[0.15,0.15],[0.35,0.4],[0.2,0.6]],color = 'g',alpha = 0.5)

ax.add_patch(rect)
ax.add_patch(circ)
ax.add_patch(pgon)

plt.show()
#+END_SRC




将图表保存到文件

#+BEGIN_SRC python

# -*- encoding: UTF-8 -*- 
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import numpy.random as npr
from datetime import datetime
from io import StringIO

#将图标保存到文件
#savefig函数可以保存图形文件，不同的扩展名保存为不同的格式
fig = plt.figure()
ax = fig.add_subplot(1,1,1)

rect = plt.Rectangle((0.2,0.75),0.4,0.15,color = 'k',alpha = 0.3)
circ = plt.Circle((0.7,0.2),0.15,color = 'b',alpha = 0.3)
pgon = plt.Polygon([[0.15,0.15],[0.35,0.4],[0.2,0.6]],color = 'g',alpha = 0.5)

ax.add_patch(rect)
ax.add_patch(circ)
ax.add_patch(pgon)

#注意下面的dpi（每英寸点数）和bbox_inches（可以剪除当前图标周围的空白部分）（确实有效）
#plt.savefig('pic.jpg',dpi = 100,bbox_inches = 'tight')

#不一定save到文件中，也可以写入任何文件型对象，比如StringIO：

buffer = StringIO()
plt.savefig(buffer)
plot_data = buffer.getvalue()

#这对Web上提供动态生成的图片是很实用的

#plt.show()

#+END_SRC


savefig的一些选项：

matplotlib配置
matplotlib的一些属性是可以设置的，比如图像大小、subplot边距、配色方案、字体大小、网格类型等。有两种方式进行操作。第一种是Python变成方式，即利用rc方法。比如：
plt.rc('figure',figsize = (10,10))
rc的第一个参数是希望自定义的对象，比如‘figure’、‘axes’、‘xtick’、‘ytick’、‘grid’、‘legend’等。其后可以跟上一系列的关键字参数。最简单的就是写成一个字典：

#+BEGIN_SRC python
font_options = {'family':'monospace',
                         'weight':'bold',
                         'size':'small'}
plt.rc('font',**font_options)
#+END_SRC

matplotlibrc是配置文件，定义好以后每次加载就会用设置的参数。
2、pandas中的绘图函数
matplotlib是一种比较低级的工具，需要将各种组件组合好：数据展示（线型图、柱状图等）、图例、标题、刻度标签以及注解。这是因为制作一张图表一般需要用到多个对象。在pandas中，会省事不少。pandas能够利用DataFrame的对象特点创建标准图表的高级绘图方法。作者说pandas在线文档时最好的学习工具，书上的代码可能过时了。
线型图


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

s = Series(np.random.randn(10).cumsum(),index = np.arange(0,100,10))
#该Series对象的索引会被传给matplotlib，并绘制X轴。
#可以用use_index = False 禁用该功能
s.plot(use_index = False)

#X轴的刻度和界限可以通过xticks和xlim选项进行调节，Y轴通过xticks和ylim调节

plt.show()
#+END_SRC



#+BEGIN_SRC python
#pandas的大部分方法都有一个可选的ax参数，可以是一个subplot对象。这可以
#使在网格中更为灵活地处理subplot的位置。
#DataFrame的plot方法会在一个subplot中为各列绘制线型图，并自动添加图例
df = DataFrame(np.random.randn(10,4).cumsum(0),
    columns = ['A','B','C','D'],
    index = np.arange(0,100,10))
df.plot()
plt.show()


#+END_SRC

下面把参数贴一下：


DataFrame还有一些对列进行处理的参数：

自下面开始就有一些专门的图形，绘制的时候可以与R语言进行对比：http://www.cnblogs.com/batteryhp/p/4733474.html。
柱状图


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame


#生成的线形图中代码加上kind = ‘bar’（垂直柱图） 或者 （水平）kind = ‘barh’（水平柱图）
#Series和DataFrame的索引被用作X（bar）或者Y（barh）的刻度

fig,axes = plt.subplots(2,1)
data = Series(np.random.randn(16),index = list('abcdefghijklmnop'))

data.plot(kind = 'barh',ax = axes[0],color = 'k',alpha = 0.7)
data.plot(kind = 'bar',ax = axes[1],color = 'k',alpha = 0.7)

#DataFrame会按照行对数据进行分组
df = DataFrame(np.random.randn(6,4),index = ['one','two','three','four','five','six'],
    columns = pd.Index(['A','B','C','D'],name = 'Genus')) 
#注意这里的name会被用作图例的标题，因为，这本来就是列的名字
print df
df.plot(kind = 'bar')
plt.show()
#这里的stacked是标明画累计柱图
df.plot(kind = 'bar',stacked = True,alpha = 0.5)
plt.show()

#Series的value_counts可以用来显示Series中各值的频数（实验证明）
s = Series([1,2,2,3,4,4,4,5,5,5])
s.value_counts().plot(kind = 'bar')
plt.show()

#+END_SRC





下面看一个例子：


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

#下面是一个例子：做一张堆积柱状图来显示每天各种聚会规模的数据点百分比
tips = pd.read_csv('E:\\tips.csv')
party_counts = pd.crosstab(tips.day,tips.size)
print party_counts
party_counts = party_counts.ix[:,2:5]
#然后进行归一化是各行和为1
party_pcts = party_counts.div(party_counts.sum(1).astype(float),axis = 0)
print party_pcts
party_pcts.plot(kind = 'bar',stacked = True)
plt.show()


#+END_SRC

直方图和密度图


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame
#绘制小费百分比直方图
tips = pd.read_csv('E:\\tips.csv')
tips['tip_pct'] = tips['tip'] / tips['total_bill']
#bins规定一共分多少个组
tips['tip_pct'].hist(bins = 50)
plt.show()

#与此相关的是密度图：他是通过计算“可能会产生观测数据的连续概率分布的估计”
#而产生的。一般的过程将该分布金思维一组核（诸如正态之类的较为简单的分布）。
#此时的密度图称为KDE图。kind = ‘kde’即可。
tips['tip_pct'].plot(kind = 'kde')
plt.show()

#显然，直方图和密度图经常会在一起出现
comp1 = np.random.normal(0,1,size = 200)
comp2 = np.random.normal(10,2,size = 200)
values = Series(np.concatenate([comp1,comp2]))
print values
values.hist(bins = 100,alpha = 0.3,color = 'k',normed = True)
values.plot(kind = 'kde',style = 'k--')
plt.show()
#+END_SRC





散布图
散布图（scantter plot）是观察两个一维数据序列之间的关系的有效手段。matplotlib中的scantter方法是绘制散布图的主要方法。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

#下面加载macrodata中的数据集，选择其中几列并计算对数差
macro = pd.read_csv('E:\\macrodata.csv')
data = macro[['cpi','m1','tbilrate','unemp']]
#这里的diff函数是用来计算相邻两数只差，对每一列，后一个数减前一个数
trans_data = np.log(data).diff().dropna()
#print np.log(data).head()
#print np.log(data).diff().head()
print trans_data.head()

plt.scatter(trans_data['m1'],trans_data['unemp'])
plt.title('Changes in log %s vs. log %s'%('m1','unemp'))
plt.show()
#画散布图矩阵式很有意义的pandas提供了scantter_matrix函数来创建散步矩阵
#关于 diagonal 参数，是为了不让对角线上的图形（自己和自己的散布图）显示为一条直线而设置的关于这种数据的某些图形显示
#比如 diagonal = 'kde'就是画密度图且核为kde，若diagonal='hist'，则为直方图
pd.scatter_matrix(trans_data,diagonal = 'kde',color = 'k',alpha = 0.3)
pd.scatter_matrix(trans_data,diagonal = 'hist',color = 'k',alpha = 0.3)
plt.show()
#+END_SRC





绘制地图：图形化显示海地地震危机数据
这是一个例子。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame
from mpl_toolkits.basemap import Basemap

#下面的例子应该是比较综合的
data = pd.read_csv('E:\\Haiti.csv')
#print data
#下面处理一下数据,下面的为日期，纬度、经度
#print data[['INCIDENT DATE','LATITUDE','LONGITUDE']][:10]
#print data['CATEGORY'][:6]   #这些代表消息的类型
#数据中很有可能有异常值、缺失值，下面看一下
#print data.describe()
#清除错误信息并移除缺失分类信息是“一件简单的事情”
data = data[(data.LATITUDE > 18) & (data.LATITUDE < 20) & (data.LONGITUDE > -75) &
           (data.LONGITUDE < -70) & data.CATEGORY.notnull()]

#我们想根据分类对数据做一些分析或者图形化工作，但是各个分类字段中可能含有多个分类。此外，各个分类信息
#不仅有一个编码，还有一个英语（法语）名称。因此需要对数据进行规整化处理。下面编写两（三）个
#函数，一个用于获取所有分类的列表，一个用于将各个分类信息拆分为编码和英语明名称

#sptrip 是删除空白字符，'\n'等;注意作者这种隐式循环写法
def to_cat_list(catstr):
    stripped = (x.strip() for x in catstr.split(','))
    return [x for x in stripped if x]
def get_all_categoties(cat_series):
    cat_sets = (set(to_cat_list(x)) for x in cat_series)
    return sorted(set.union(*cat_sets))
def get_english(cat):
    code,names = cat.split('.')
    if '|' in names:
        names = names.split('|')[1]
    return code,names.strip()

#下面进行一下ceshi
#print get_english('2.Urgences logistiques | Vital Lines')
#接下来做了一个将编码跟名称映射起来的字典，这是因为我们等会要用编码进行分析。
#下面将所有组合弄出来
all_cats = get_all_categoties(data.CATEGORY)
#print data.CATEGORY[:10]
#print all_cats
#生成器表达式
#生成字典
english_mapping = dict(get_english(x) for x in all_cats)
#print english_mapping['2a']
#print english_mapping['6c']
#根据分类选取记录的方式有很多，其中之一就是添加指标（或者哑变量）列，每个分类一列。
#为此，首先抽取出唯一的分类编码，并构造一个全零DataFrame（列为分类编码，索引跟data的索引一样)
def get_code(seq):
    return [x.split('.')[0] for x in seq if x]
#下面是将所有的key取出来
all_codes = get_code(all_cats)
#print all_codes
code_index = pd.Index(np.unique(all_codes))
#print code_index
dummy_frame = DataFrame(np.zeros((len(data),len(code_index))),index = data.index,columns = code_index)
#print len(data)
#print dummy_frame.ix[:,:6]
#下面将各行中适当的项设置为1，然后再与data进行连接：

for row,cat in zip(data.index,data.CATEGORY):
    codes = get_code(to_cat_list(cat))
    dummy_frame.ix[row,codes] = 1
#添加前缀，并且合并一下
data = data.join(dummy_frame.add_prefix('category_'))
#print data
#接下来开始画图吧，我们希望把数据绘制在海地的地图上。basemap数据集是matplotloib的一个插件
#使得能够用Python在地图上绘制2D数据。basemap提供了许多不同的地球投影以及一种将地球上的经纬度
#坐标投影转换为二维matplotlib图的方式。
#“经过一遍又一遍的尝试”，作者编写了下面的函数，绘制出一张简单的黑白地图。

def basic_haiti_map(ax = None,lllat = 17.25,urlat = 20.25,lllon = -75,urlon = -71):
    #创建极球面投影的Basemap实例。
    m  = Basemap(ax = ax,projection = 'stere',
        lon_0 = (urlon + lllon) / 2,
        lat_0 = (urlat + lllat) / 2,
        llcrnrlat = lllat,urcrnrlat = urlat,
        llcrnrlon = lllon,urcrnrlon = urlon,
        resolution = 'f' )

#+END_SRC

由于window下安装geos不成功，这部分等ubuntu装好了再接着写。
4、Python图形化工具生态系统
介绍几个其他的绘图工具。
Chaco
特点：静态图 + 交互图形，非常适合用复杂的图形化方法表示数据的内部关系。
对交互支持的好的多，交互式GUI是个不错选择。
mayavi
这是一个基于开源C++图形库VTK的3D图形工具包。可以集成到Ipython交互使用。
其他库
其他库或者应用还有：PyQwt、Veusz、gnuplotpy、biggles等，
大部库都在向基于Web的技术发展，并逐渐远离桌面图形技术。
图形化工具的未来
基于Web技术（如Javascript）的图形化是必然的发展趋势，现在已经有不
少了，higncharts等
*  第九章 数据聚合与分组运算（一）

对数据进行分组并对各组应用一个函数，是数据分析的重要环节。数据准备好之后，通常的任务就是计算分组统计或生成透视表。groupby函数能高效处理数据，对数据进行切片、切块、摘要等操作。可以看出这跟SQL关系密切，但是可用的函数有很多。在本章中，可以学到： 
根据一个或多个键（可以是函数、数组或DataFrame列名）拆分pandas对象
计算分组摘要统计，如计数、平均值、标准差、，或自定义函数
对DataFrame的列应用各种各样的函数
应用组内转换或其他运算，如规格化、线性回归、排名或选取子集等
计算透视表和交叉表
执行分位数分析以及其他分组分析
对时间数据的聚合也称重采样（resampling），在第十章介绍。
1、GroupBy技术
很多数据处理过程都经历“拆分-应用-合并”的过程。即根据一个或多个键进行分组、每一个应用函数、再进行合并。

分组键有多种形式：
列表或数组，长度与待分组的轴一样
表示DataFrame某个列明的值
字典或Series，给出待分组轴上的值与分组名之间的对应关系
函数，用于处理轴索引或索引中的各个标签
下面开始写例子。
简单实例


#+BEGIN_SRC python
#-*- encoding: utf-8 –*-
#分组实例
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

df =DataFrame({'key1':list('aabba'),'key2':['one','two','one','two','one'],
    'data1':np.random.randn(5),'data2':np.random.randn(5)})
print df,'\n'
#根据key1进行分组，并计算data1的均值。
#注意下面的方式，取出来进行分组，而不是在DataFrame中分组，这种方式很灵活
#可以看到这是一个GroupBy对象,具备了应用函数的基础
#这个过程是将Seri进行聚合，产生了新的Series
grouped = df['data1'].groupby(df['key1'])
print grouped,'\n'
print grouped.mean(),'\n'
means = df['data1'].groupby([df['key1'],df['key2']]).mean()
print means,'\n' #得到一个层次化索引的DataFrame
print means.unstack(),'\n'
#上面的分组键均为Series，实际上，分组键可以是任何长度适当的数组,很灵活
states = np.array(['Ohio','California','California','Ohio','Ohio'])
years = np.array([2005,2005,2006,2005,2006])
print df['data1'].groupby([states,years]).mean(),'\n'
#还可以用列名（可以是字符串、数字或其他python对象）用作分组键
print df.groupby('key1').mean(),'\n'  #这里将数值型的列都进行了mean,非数值型的忽略
print df.groupby(['key1','key2']).mean(),'\n'
#groupby以后可以应用一个很有用的size方法
print df.groupby(['key1','key2']).size(),'\n' #截止翻译版为止，分组键中的缺失值被排除在外
>>>
      data1     data2 key1 key2
0  1.489789 -1.548474    a  one
1 -1.000447 -0.187066    a  two
2  0.254255 -0.960017    b  one
3  1.279892  1.124993    b  two
4 -0.366753  0.139047    a  one 
    <pandas.core.groupby.SeriesGroupBy object at 0x03A895B0>
    key1
    a       0.040863
    b       0.767073
    key1  key2
    a     one     0.561518
          two    -1.000447
    b     one     0.254255
          two     1.279892
    key2       one       two
    key1                    
    a     0.561518 -1.000447
    b     0.254255  1.279892
    California  2005   -1.000447
                2006    0.254255
    Ohio        2005    1.384841
                2006   -0.366753
             data1     data2
    key1                    
    a     0.040863 -0.532165
    b     0.767073  0.082488
                  data1     data2
    key1 key2                    
    a    one   0.561518 -0.704714
         two  -1.000447 -0.187066
    b    one   0.254255 -0.960017
         two   1.279892  1.124993
    key1  key2
    a     one     2
          two     1
    b     one     1
          two     1
    [Finished in 0.7s]

#+END_SRC

对分组进行迭代


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

df =DataFrame({'key1':list('aabba'),'key2':['one','two','one','two','one'],
    'data1':np.random.randn(5),'data2':np.random.randn(5)})

#对分组进行迭代,groupby对象支持迭代
#下面是迭代？……不过就是将分组的结果分别赋值给两个量?不是这样的,下面的循环会打印两个one

print df.groupby('key1')
for name,group in df.groupby('key1'):
    print 'one'
    print name
    print group,'\n'

#多重键的情况，元组的第一个元素将会是由键值组成的元组,下面会打印四个two
#也就是说，下面的三个print是一个组合，打印key值这一点挺好
for (k1,k2),group in df.groupby(['key1','key2']):
    print 'two'
    print k1,k2
    print group,'\n'
#当然，可以对数据片段进行操作
#转换为字典，应该是比较有用的一个转换方式
print list(df.groupby('key1')),'\n'
pieces = dict(list(df.groupby('key1')))
#注意下面的字典中的每个值仍然是一个“含有名称的DataFrame”，可能不严谨，但是就是这意思
print pieces['a'],'\n'
print type(pieces['a'])
print pieces['a'][['data1','data2']],'\n'
#groupby默认在axis = 0上进行分组，可以设置在任何轴上分组
#下面用dtype对列进行分组
print df.dtypes,'\n'
grouped = df.groupby(df.dtypes,axis = 1)
print grouped,'\n'
print dict(list(grouped)) #有点像把不同数值类型的列选出来
>>>
<pandas.core.groupby.DataFrameGroupBy object at 0x0333CEB0>
one
a
      data1     data2 key1 key2
0 -0.984933  0.392220    a  one
1 -2.104506  4.120798    a  two
4 -0.267432 -1.825800    a  one 
    one
    b
          data1     data2 key1 key2
    2  0.476850 -1.738739    b  one
    3 -0.863738 -0.458431    b  two
    two
    a one
          data1    data2 key1 key2
    0 -0.984933  0.39222    a  one
    4 -0.267432 -1.82580    a  one
    two
    a two
          data1     data2 key1 key2
    1 -2.104506  4.120798    a  two
    two
    b one
         data1     data2 key1 key2
    2  0.47685 -1.738739    b  one
    two
    b two
          data1     data2 key1 key2
    3 -0.863738 -0.458431    b  two
    [('a',       data1     data2 key1 key2
    0 -0.984933  0.392220    a  one
    1 -2.104506  4.120798    a  two
    4 -0.267432 -1.825800    a  one), ('b',       data1     data2 key1 key2
    2  0.476850 -1.738739    b  one
    3 -0.863738 -0.458431    b  two)]
          data1     data2 key1 key2
    0 -0.984933  0.392220    a  one
    1 -2.104506  4.120798    a  two
    4 -0.267432 -1.825800    a  one
    <class 'pandas.core.frame.DataFrame'>
          data1     data2
    0 -0.984933  0.392220
    1 -2.104506  4.120798
    4 -0.267432 -1.825800
    data1    float64
    data2    float64
    key1      object
    key2      object
    <pandas.core.groupby.DataFrameGroupBy object at 0x033F0190>
    {dtype('object'):   key1 key2
    0    a  one
    1    a  two
    2    b  one
    3    b  two
    4    a  one, dtype('float64'):       data1     data2
    0 -0.984933  0.392220
    1 -2.104506  4.120798
    2  0.476850 -1.738739
    3 -0.863738 -0.458431
    4 -0.267432 -1.825800}
    [Finished in 0.7s]

#+END_SRC

选取一个或一组列


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

df =DataFrame({'key1':list('aabba'),'key2':['one','two','one','two','one'],
    'data1':np.random.randn(5),'data2':np.random.randn(5)})
print df,'\n'

#对于由DataFrame产生的GroupBy对象，如果用一个或一组列名进行索引，就能实现选取部分列进行聚合的目的，即
#下面语法效果相同
print df.groupby('key1')['data1']  #又一次选取方式的区分，这条语句返回Series，下一条返回DataFrame
print df.groupby('key1')[['data1']]
#下面的
print df['data1'].groupby(df['key1'])
print df[['data1']].groupby(df['key1']),'\n'
#尤其对于大数据集，可能只是对部分列进行聚合。比如，想计算data2的均值并返回DataFrame
print df.groupby(['key1','key2'])[['data2']].mean(),'\n'
>>>
      data1     data2 key1 key2
0 -1.381889  0.919518    a  one
1 -0.186802  1.265642    a  two
2 -0.173303  0.866173    b  one
3  0.015841 -0.601375    b  two
4 -0.281338 -0.319804    a  one 
 
<pandas.core.groupby.SeriesGroupBy object at 0x039EB970>
<pandas.core.groupby.DataFrameGroupBy object at 0x039EB930>
<pandas.core.groupby.SeriesGroupBy object at 0x039EB930>
<pandas.core.groupby.DataFrameGroupBy object at 0x039EB950> 
                  data2
    key1 key2          
    a    one   0.299857
         two   1.265642
    b    one   0.866173
         two  -0.601375
    [Finished in 0.7s]

#+END_SRC

通过字典或Series进行分组


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

people = DataFrame(np.random.randn(5,5),columns = ['a','b','c','d','e'],index = ['Joe','Steve','Wes','Jim','Travis'])
people.ix[2:3,['b','c']] = np.nan #加点NaN
print people,'\n'
#假设已经知道列的分组方式，现在需要利用这个信息进行分组统计：
mapping = {'a':'red','b':'red','c':'blue','d':'blue','e':'red','f':'orange'}
#下面为groupby传入一个已知信息的字典
by_column = people.groupby(mapping,axis = 1)
print by_column.sum(),'\n'  #注意得到的名字是 red 和 blue
#Series也有这样的功能，被看作一个固定大小的映射，可以用Series作为分组键，pandas会自动检查对齐
map_series = Series(mapping)
print map_series,'\n'
print people.groupby(map_series,axis = 1).count()
>>>
               a         b         c         d         e
Joe    -0.344808  0.716334  1.092892  0.824548  0.206477
Steve   0.457156 -0.207056 -0.447555 -0.378811 -0.581657
Wes    -0.739237       NaN       NaN -1.168591  0.876174
Jim     0.116797 -1.888764  2.072722  0.029644  0.919705
Travis -0.482019  1.479823  0.706617  0.697408 -0.914512 
                blue       red
    Joe     1.917440  0.578003
    Steve  -0.826367 -0.331557
    Wes    -1.168591  0.136937
    Jim     2.102366 -0.852261
    Travis  1.404025  0.083292
    a       red
    b       red
    c      blue
    d      blue
    e       red
    f    orange
            blue  red
    Joe        2    3
    Steve      2    3
    Wes        1    2
    Jim        2    3
    Travis     2    3
    [Finished in 0.6s]

     
#+END_SRC

利用函数进行分组


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

#相较于字典或Series，python函数在定义分组映射关系时可以更有创意且更为抽象。
#函数会在各个索引值上调用一次，并根据结果进行分组。
people = DataFrame(np.random.randn(5,5),columns = ['a','b','c','d','e'],index = ['Joe','Steve','Wes','Jim','Travis'])
print people.groupby(len).sum()  #名字长度相同的人进行加和
#将函数、数组、字典、Series混用也ok，因为最终都会转换为数组
key_list = ['one','one','one','two','two']
print people.groupby([len,key_list]).min()
>>>
          a         b         c         d         e
3  0.528550  0.245731  1.187483 -1.086821  0.042086
5 -2.579143  0.152800 -0.911028  0.328152  0.627507
6  2.328199 -1.091351 -1.198069  0.571550  0.794774
              a         b         c         d         e
3 one -0.444315  0.559996 -1.486260  0.090243 -1.131864
  two -0.601314 -1.389457  1.616836 -1.366003  1.495320
5 one -2.579143  0.152800 -0.911028  0.328152  0.627507
6 two  2.328199 -1.091351 -1.198069  0.571550  0.794774
[Finished in 1.5s]

#+END_SRC

根据索引级别分组


#+BEGIN_SRC python
#-*- encoding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

#层次化索引数据集最方便的地方就在于它能够根据索引级别进行聚合。要实现该目的，只要通过level关键字传入级别编号或名称即可
columns = pd.MultiIndex.from_arrays([['US','US','US','JP','JP'],[1,3,5,1,3]],names = ['cty','tenor'])
hier_df = DataFrame(np.random.randn(4,5),columns = columns)
print hier_df,'\n'
print hier_df.groupby(level = 'cty',axis = 1).count(),'\n'
print hier_df.groupby(level = 'tenor',axis = 1).count(),'\n'
print hier_df.groupby(level = ['cty','tenor'],axis = 1).count()
>>>
cty          US                            JP          
tenor         1         3         5         1         3
0      0.211478  0.076928 -1.225755  0.080232  1.472201
1      0.159280  0.504315  0.741466  2.263926  0.771153
2     -0.759615  0.550016 -1.476229  1.838213 -0.509156
3      0.987656  0.238239  0.537588 -0.126640  0.252719 
    cty  JP  US
    0     2   3
    1     2   3
    2     2   3
    3     2   3
    tenor  1  3  5
    0      2  2  1
    1      2  2  1
    2      2  2  1
    3      2  2  1
    cty    JP     US      
    tenor   1  3   1  3  5
    0       1  1   1  1  1
    1       1  1   1  1  1
    2       1  1   1  1  1
    3       1  1   1  1  1
    [Finished in 1.2s]

#+END_SRC

2、数据聚合
这里的数据聚合是说任何能够从数组产生标量值的过程。之前的例子已经用到了一些，比如mean()、count()、min()、max()等。常见的聚合运算都有就地计算数据集统计信息的优化实现。当然并不止这些，可以用自己定义的运算，还可以调用分组对象上已经定义好的任何方法。例如，quantile可以计算Series或DataFrame列的样本分位数。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt


df =DataFrame({'key1':list('aabba'),'key2':['one','two','one','two','one'],
    'data1':np.random.randn(5),'data2':np.random.randn(5)})
#print df
grouped = df.groupby('key1')
#注意下面的quantile并没有直接实现于GroupBy，它是一个Series方法，故而能用，
#就是说，此过程实际上是groupby对df进行高效切片，然后对每个切片应用quantile
print grouped['data1'].quantile(0.9),'\n'
#对于自己定义的聚合函数，只需将其传入aggregate或agg即可
#注意下面是对每列都应用
def peak_to_peak(arr):
    return arr.max() - arr.min()
print grouped.agg(peak_to_peak),'\n'
#有些方法（describe）也是可以应用的。
print grouped.describe()
#自定义函数比经过优化的函数要慢得多，这是因为在构造中间分组数据块时存在非常大的开销（函数调用、数据重排等）
#下面说明更高级的聚合功能，用的是R语言reshape2包中的数据集tips，这数据是从R中自己导出来的
tips = pd.read_csv('E:\\tips.csv')
#增加小费占比一列
tips['tip_pct'] = tips['tip'] / tips['total_bill']
print tips.head()
>>>
key1
a       0.970028
b       0.642314 
             data1     data2
    key1                    
    a     1.502016  1.583056
    b     0.495911  0.384405
                   data1     data2
    key1                          
    a    count  3.000000  3.000000
         mean   0.304136  0.822614
         std    0.802148  0.792578
         min   -0.284158  0.007541
         25%   -0.152726  0.438623
         50%   -0.021293  0.869705
         75%    0.598283  1.230151
         max    1.217858  1.590597
    b    count  2.000000  2.000000
         mean   0.443950  0.425535
         std    0.350662  0.271816
         min    0.195994  0.233332
         25%    0.319972  0.329433
         50%    0.443950  0.425535
         75%    0.567927  0.521636
         max    0.691905  0.617737
       total_bill   tip     sex smoker  day    time  size   tip_pct
    0       16.99  1.01  Female  False  Sun  Dinner     2  0.059447
    1       10.34  1.66    Male  False  Sun  Dinner     3  0.160542
    2       21.01  3.50    Male  False  Sun  Dinner     3  0.166587
    3       23.68  3.31    Male  False  Sun  Dinner     2  0.139780
    4       24.59  3.61  Female  False  Sun  Dinner     4  0.146808
    [Finished in 0.7s]


#+END_SRC

经过优化的GroupBy的方法，按作者的意思，这些函数是快的。

面向列的多函数应用
有时候需要对不同的列应用不同的函数，或者对一列应用不同的函数。下面是例子。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt

#下面说明更高级的聚合功能，用的是R语言reshape2包中的数据集tips，这数据是从R中自己导出来的
tips = pd.read_csv('E:\\tips.csv')
#增加小费占比一列
tips['tip_pct'] = tips['tip'] / tips['total_bill']
print tips.head(),'\n'
grouped = tips.groupby(['sex','smoker'])
grouped_pct = grouped['tip_pct']
print grouped_pct.agg('mean'),'\n'
#若传入一组函数或函数名，得到的DataFrame的列就会以相应的函数命名
def peak_to_peak(arr):
    return arr.max() - arr.min()
#对比例这一列应用三个函数
print grouped_pct.agg(['mean','std',peak_to_peak]),'\n'
#上面有个问题就是列名是自动给出的，以函数名为列名，若传入元组
#（name，function）组成的列表，就会自动将第一个元素作为列名
print grouped_pct.agg([('foo','mean'),('bar',np.std)]),'\n'  #注意np.std不能加引号
#还可以对多列应用同一函数
functions = ['count','mean','max']
result = grouped['tip_pct','total_bill'].agg(functions)  #对两列都应用functions
print result,'\n'  #得到的结果的列名是层次化索引，可以直接用外层索引选取数据
print result['tip_pct'],'\n'
ftuples = [('DDD','mean'),('AAA',np.var)]
print grouped['tip_pct','total_bill'].agg(ftuples),'\n'
#如果想对不同的列应用不同的函数，具体的办法是向agg传入一个从列映射到函数的字典
print grouped.agg({'tip':np.max,'size':sum}),'\n'  #sum这样的函数可以加引号或者不加
print grouped.agg({'tip':['min','max','mean','std'],'size':sum})
>>>
   total_bill   tip     sex smoker  day    time  size   tip_pct
0       16.99  1.01  Female  False  Sun  Dinner     2  0.059447
1       10.34  1.66    Male  False  Sun  Dinner     3  0.160542
2       21.01  3.50    Male  False  Sun  Dinner     3  0.166587
3       23.68  3.31    Male  False  Sun  Dinner     2  0.139780
4       24.59  3.61  Female  False  Sun  Dinner     4  0.146808 
    sex     smoker
    Female  False     0.156921
            True      0.182150
    Male    False     0.160669
            True      0.152771
    Name: tip_pct
                       mean       std  peak_to_peak
    sex    smoker                                  
    Female False   0.156921  0.036421      0.195876
           True    0.182150  0.071595      0.360233
    Male   False   0.160669  0.041849      0.220186
           True    0.152771  0.090588      0.674707
                        foo       bar
    sex    smoker                    
    Female False   0.156921  0.036421
           True    0.182150  0.071595
    Male   False   0.160669  0.041849
           True    0.152771  0.090588
                   tip_pct                      total_bill                  
                     count      mean       max       count       mean    max
    sex    smoker                                                           
    Female False        54  0.156921  0.252672          54  18.105185  35.83
           True         33  0.182150  0.416667          33  17.977879  44.30
    Male   False        97  0.160669  0.291990          97  19.791237  48.33
           True         60  0.152771  0.710345          60  22.284500  50.81
                   count      mean       max
    sex    smoker                           
    Female False      54  0.156921  0.252672
           True       33  0.182150  0.416667
    Male   False      97  0.160669  0.291990
           True       60  0.152771  0.710345
                    tip_pct            total_bill           
                        DDD       AAA         DDD        AAA
    sex    smoker                                           
    Female False   0.156921  0.001327   18.105185  53.092422
           True    0.182150  0.005126   17.977879  84.451517
    Male   False   0.160669  0.001751   19.791237  76.152961
           True    0.152771  0.008206   22.284500  98.244673
                   size   tip
    sex    smoker            
    Female False    140   5.2
           True      74   6.5
    Male   False    263   9.0
           True     150  10.0
                    tip                            size
                    min   max      mean       std   sum
    sex    smoker                                      
    Female False   1.00   5.2  2.773519  1.128425   140
           True    1.00   6.5  2.931515  1.219916    74
    Male   False   1.25   9.0  3.113402  1.489559   263
           True    1.00  10.0  3.051167  1.500120   150
    [Finished in 0.7s]

#+END_SRC

以‘无索引’的方式返回聚合数据
到目前为止，示例中的聚合数据都是由唯一的分组键组成的索引（可能还是层次化的）。由于并不是总需要如此，可以向groupby传入as_index = False禁用该功能。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt

tips = pd.read_csv('E:\\tips.csv')
#增加小费占比一列
tips['tip_pct'] = tips['tip'] / tips['total_bill']
print tips.head(),'\n'
print tips.groupby(['sex','smoker'],as_index = False).mean() #这里的形式可能有时候更好用
>>>
   total_bill   tip     sex smoker  day    time  size   tip_pct
0       16.99  1.01  Female  False  Sun  Dinner     2  0.059447
1       10.34  1.66    Male  False  Sun  Dinner     3  0.160542
2       21.01  3.50    Male  False  Sun  Dinner     3  0.166587
3       23.68  3.31    Male  False  Sun  Dinner     2  0.139780
4       24.59  3.61  Female  False  Sun  Dinner     4  0.146808 
          sex smoker  total_bill       tip      size   tip_pct
    0  Female  False   18.105185  2.773519  2.592593  0.156921
    1  Female   True   17.977879  2.931515  2.242424  0.182150
    2    Male  False   19.791237  3.113402  2.711340  0.160669
    3    Male   True   22.284500  3.051167  2.500000  0.152771
    [Finished in 0.6s]

#+END_SRC

3、分组级运算和转换
聚合只是分组运算中的一种，它是数据转换的一个特例。也就是说，它只是接受能够将一维数组简化为标量值的函数。本节将介绍transform和apply方法，能够执行更多其他的分组运算。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt

#下面为DataFrame添加一个用于存放各索引分组平均值的列。一个办法是先聚合在合并：
df =DataFrame({'key1':list('aabba'),'key2':['one','two','one','two','one'],
    'data1':np.random.randn(5),'data2':np.random.randn(5)})
#print df,'\n'
k1_means = df.groupby('key1').mean().add_prefix('mean_')
print k1_means,'\n'
#下面用左边的key1作为连接键，right_index是将右边的行索引作为连接键
print pd.merge(df,k1_means,left_on = 'key1',right_index = True)
#上面的方法虽然也行，但是不灵活。可以看作利用mean函数对数据的两列进行转换。

people = DataFrame(np.random.randn(5,5),columns = ['a','b','c','d','e'],index = ['Joe','Steve','Wes','Jim','Travis'])
print people,'\n'
key = ['one','two','one','two','one']
print people.groupby(key).mean(),'\n'
#看下面神奇的事情
print people.groupby(key).transform(np.mean),'\n'
#不难看出，transform会将一个函数应用到各个分组并将结果放置到适当的位置，
#如果各分组产生的是一个标量值，则改值就会被广播出去
#下面的例子很说明问题，很灵活
def demean(arr):
    return arr - arr.mean()
demeaned = people.groupby(key).transform(demean)
print demeaned,'\n'
#下面检查一下demeaned各组均值是否为0
print demeaned.groupby(key).mean()
>>>
      mean_data1  mean_data2
key1                        
a      -0.729610   -0.141770
b      -0.174505    0.484952 

      data1     data2 key1 key2  mean_data1  mean_data2
0 -2.082417  0.752055    a  one   -0.729610   -0.141770
1 -0.563339 -0.915167    a  two   -0.729610   -0.141770
4  0.456927 -0.262198    a  one   -0.729610   -0.141770
2 -0.173514  1.695344    b  one   -0.174505    0.484952
3 -0.175496 -0.725440    b  two   -0.174505    0.484952
               a         b         c         d         e
Joe    -1.109408 -0.379178 -0.666847  2.003109 -1.331988
Steve   0.316630 -1.801337 -0.479510  0.305003  1.641795
Wes     0.338475 -0.613742 -0.623375 -0.423722 -0.529741
Jim     0.206591 -0.876095  0.297528 -0.177179  0.208701
Travis -1.307377  0.144524  0.236289  0.382082  0.497277 

           a         b         c         d         e
one -0.69277 -0.282799 -0.351311  0.653823 -0.454817
two  0.26161 -1.338716 -0.090991  0.063912  0.925248 

              a         b         c         d         e
Joe    -0.69277 -0.282799 -0.351311  0.653823 -0.454817
Steve   0.26161 -1.338716 -0.090991  0.063912  0.925248
Wes    -0.69277 -0.282799 -0.351311  0.653823 -0.454817
Jim     0.26161 -1.338716 -0.090991  0.063912  0.925248
Travis -0.69277 -0.282799 -0.351311  0.653823 -0.454817 

               a         b         c         d         e
Joe    -0.416638 -0.096379 -0.315536  1.349286 -0.877171
Steve   0.055020 -0.462621 -0.388519  0.241091  0.716547
Wes     1.031245 -0.330943 -0.272064 -1.077544 -0.074924
Jim    -0.055020  0.462621  0.388519 -0.241091 -0.716547
Travis -0.614607  0.427322  0.587599 -0.271741  0.952094 

     a             b             c             d  e
one  0 -1.850372e-17 -3.700743e-17  1.850372e-17  0
two  0 -5.551115e-17  0.000000e+00  0.000000e+00  0
[Finished in 0.7s]

#+END_SRC

apply:一般性的“拆分-应用-合并”
本节就是说apply函数很重要，是最一般化的GroupBy方法。跟aggregate一样，transform也是一个有着严格条件的特殊函数：传入的函数只能产生两种结果，要么是可以广播的标量，要么是产生一个相同大小的结果数组。apply函数将对象拆分为多个片段，对各个片段调用传入的函数，并尝试将各片段合到一起。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt

tips = pd.read_csv('E:\\tips.csv')
#增加小费占比一列
tips['tip_pct'] = tips['tip'] / tips['total_bill']
#print tips.head(),'\n'
#下面找出指定列的最大的几个值，然后将所在行选出来
def top(df,n = 5,column = 'tip_pct'):
    return df.sort_index(by = column)[-n:]
print top(tips,n = 6),'\n'
#如果对smoker分组并用该函数调用apply
print tips.groupby('smoker').apply(top),'\n'
#上面实际上是在各个片段上调用了top，然后用pd.concat进行了连接，并以分组名称进行了标记，于是就形成了层次化索引
#当然可以向top函数传入参数
print tips.groupby(['smoker','day']).apply(top,n = 1,column = 'total_bill')
#需要说明的是：apply很强大，需要发挥想象力，它只需返回一个pandas对象或者标量值即可
#之前曾经这么做过：
result = tips.groupby('smoker')['tip_pct'].describe()
print result,'\n'
print result.unstack('smoker'),'\n'
#下面的方式,效果一样
f = lambda x : x.describe()
print tips.groupby('smoker')['tip_pct'].apply(f),'\n'
#对所有列都行
print tips.groupby('smoker').apply(f),'\n'
#看的出，上面自动生成了层次化索引，可以将分组键去掉
print tips.groupby('smoker',group_keys = False).apply(top),'\n'
#下面看得出，重新设置索引会去掉原来所有索引，并重置索引
print tips.groupby('smoker').apply(top).reset_index(drop = True),'\n'
#下面看的出来，as_index在这里并不管用
print tips.groupby('smoker',as_index = False).apply(top),'\n'
#下面看的出来，as_index在这里并不管用
print tips.groupby(['sex','smoker'],as_index = False).apply(top),'\n'

#+END_SRC

分位数和桶分析
    第七章中的cut和qcut函数可以对数据进行拆分，现在将其与groupby集合起来会轻松实现对数据集的桶（bucket）（嗯，注意名字）或分位数（quantile）分析了。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt

frame = DataFrame({'data1':np.random.randn(1000),'data2':np.random.randn(1000)})
print frame.head(),'\n'
factor = pd.cut(frame.data1,4)
print factor[:10],'\n'
#cut返回的对象可直接用于groupby（很合理）
def get_stats(group):
    return {'min':group.min(),'max':group.max(),'count':group.count(),'mean':group.mean()}
grouped = frame.data2.groupby(factor)
print grouped.apply(get_stats),'\n'
print grouped.apply(get_stats).unstack(),'\n'
#上面的桶是区间大小相等的桶，要想得到数据量相等的桶，用qcut即可。
grouping = pd.qcut(frame.data1,10)
#print grouping
#labels = False 标明只返回各个值所在的分组编号，而不是所在的各个分组，感觉这样更好
grouping = pd.qcut(frame.data1,10,labels = False)
#print grouping,'\n'
grouped = frame.data2.groupby(grouping)
print grouped.apply(get_stats).unstack()
#+END_SRC

*  第九章 数据聚合与分组运算（二）

第三节中的四个示例。（ps：新开一篇是为了展现对例子的重视。）
3.1用特定于分组的值填充缺失值
对于缺失值的清理工作，可以用dropna进行删除，有时候需要进行填充（或者平滑化）。这时候用的是fillna。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt

s = Series(np.random.randn(6))
s[::2] = np.nan
print s,'\n'
print s.fillna(s.mean()),'\n' #不是就地的，是产生一个副本
#假如需要对不同的分组填充不同的值。只需要groupby然后应用fillna即可。
states = ['Ohio','New York','Vermont','Florida','Oregon','Nevada','California','Idaho']
group_key = ['East'] * 4 + ['West'] * 4
data = Series(np.random.randn(8),index = states)
data[['Vermont','Nevada','Idaho']] = np.nan
print data,'\n'
print data.groupby(group_key).mean(),'\n'
#下面用均值填充NA
fill_mean = lambda g:g.fillna(g.mean())
print data.groupby(group_key).apply(fill_mean),'\n'
#当然，可以自己定义填充值：我刚开始就是这么想的，用字典传入值即可
fill_values = {'East':0.5,'West':0.4}
#注意下面的用法：注意g.name 是分组的名称
fill_func = lambda g:g.fillna(fill_values[g.name])
print data.groupby(group_key).apply(fill_func),'\n'
#看一下名字，第一列是分组名，第二列是函数调用后得到的结果，而这个结果也是分组名
p = lambda x: x.name
print data.groupby(group_key).apply(p)
>>>
0         NaN
1   -0.054305
2         NaN
3    1.157882
4         NaN
5   -2.037833 
    0   -0.311418
    1   -0.054305
    2   -0.311418
    3    1.157882
    4   -0.311418
    5   -2.037833
    Ohio         -0.432424
    New York     -0.572222
    Vermont            NaN
    Florida      -1.938769
    Oregon        0.417424
    Nevada             NaN
    California    1.170923
    Idaho              NaN
    East   -0.981139
    West    0.794173
    Ohio         -0.432424
    New York     -0.572222
    Vermont      -0.981139
    Florida      -1.938769
    Oregon        0.417424
    Nevada        0.794173
    California    1.170923
    Idaho         0.794173
    Ohio         -0.432424
    New York     -0.572222
    Vermont       0.500000
    Florida      -1.938769
    Oregon        0.417424
    Nevada        0.400000
    California    1.170923
    Idaho         0.400000
    East    East
    West    West
    [Finished in 1.5s]

#+END_SRC

3.2随机采样和排列
假如想从一个大数据集中抽样完成蒙特卡洛模拟或其他工作。“抽取”的方式有很多，但是效率是不一样的。一个办法是，选取np.random.permutation(N)的前K个元素。下面是个更有趣的例子：


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt

#红桃（Hearts）、黑桃（Spades)、梅花（Clubs）、方片（Diamonds）
suits = ['H','S','C','D']
card_val = (range(1,11) + [10] * 3) * 4
#print card_val
base_names = ['A'] + range(2,11) + ['J','K','Q']
cards = []
#注意下面的生成方式，很简洁
#extend将一个列表添加到已有列表中，与append不同
for suit in ['H','S','C','D']:
    cards.extend(str(num) + suit for num in base_names)
print cards,'\n'
deck = Series(card_val,index = cards)
print deck[:13],'\n'
#从排中抽取5张，注意抽取方式,是一种随机选取5个的方式，即先选出一个排列，再从中拿出5个
def draw(deck,n = 5):
    return deck.take(np.random.permutation(len(deck))[:n])
print draw(deck),'\n'
#假如想从每种花色中随机抽取两张。先分组，再对每个组应用draw函数进行抽取
get_suit = lambda card:card[-1]
print deck.groupby(get_suit).apply(draw,2)
#另一种方法
print deck.groupby(get_suit,group_keys = False).apply(draw,2)
>>>
['AH', '2H', '3H', '4H', '5H', '6H', '7H', '8H', '9H', '10H', 'JH', 'KH', 'QH', 'AS', '2S', '3S', '4S', '5S', '6S', '7S', '8S', '9S', '10S', 'JS', 'KS', 'QS', 'AC', '2C', '3C', '4C', '5C', '6C', '7C', '8C', '9C', '10C', 'JC', 'KC', 'QC', 'AD', '2D', '3D', '4D', '5D', '6D', '7D', '8D', '9D', '10D', 'JD', 'KD', 'QD'] 
    AH      1
    2H      2
    3H      3
    4H      4
    5H      5
    6H      6
    7H      7
    8H      8
    9H      9
    10H    10
    JH     10
    KH     10
    QH     10
    7H      7
    10S    10
    4H      4
    JS     10
    6D      6
    C  AC      1
       10C    10
    D  7D      7
       10D    10
    H  6H      6
       10H    10
    S  2S      2
       4S      4
    6C      6
    QC     10
    QD     10
    10D    10
    6H      6
    5H      5
    7S      7
    2S      2
    [Finished in 1.3s]

#+END_SRC

3.3分组加权平均数和相关系数
根据groupby的“拆分-应用-合并”范式。DataFrame的列与列之间或两个Series之间的运算（比如分组加权平均）成为一种标准化作业（有道理）。看下面一个例子：


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt

df = DataFrame({'category':list('aaaabbbb'),'data':np.random.randn(8),'weights':np.random.randn(8)})
print df,'\n'
#下面以category分组并计算加权平均
grouped = df.groupby('category')
get_wavg = lambda g:np.average(g['data'],weights = g['weights'])
print grouped.apply(get_wavg),'\n'
#看下面的例子
close_px = pd.read_csv('E:\\stock_px.csv',parse_dates = True,index_col = 0)
print close_px[-4:],'\n'
#下面做一个比较有趣的任务：计算一个由日收益率（通过百分比计算）与SPX之间的年度相关系数
#组成的DataFrame,下面是一个实现办法,下面的pct_change是计算每列下一个数值相对于上一个值的百分比变化，所以，第一个肯定为NaN
rets = close_px.pct_change().dropna()
#print rets[-4:]
spx_corr = lambda x:x.corrwith(x['SPX'])
#注意下面隐式的函数，作者好高明
by_year = rets.groupby(lambda x:x.year)
#对每一小块的所有列和SPX列计算相关系数
print by_year.apply(spx_corr),'\n'
#当然，还可以计算列与列之间的相关系数
print by_year.apply(lambda g:g['AAPL'].corr(g['MSFT']))
>>>
  category      data   weights
0        a  0.287761 -1.015669
1        a -0.751835 -1.439559
2        a -0.879771  1.111463
3        a  2.252593 -0.482481
4        b  0.431053 -1.250926
5        b  0.240771 -1.259915
6        b  0.090695 -1.024591
7        b -0.602894  2.140364 
    category
    a           0.697948
    b           1.595552
                  AAPL   MSFT    XOM      SPX
    2011-10-11  400.29  27.00  76.27  1195.54
    2011-10-12  402.19  26.96  77.16  1207.25
    2011-10-13  408.43  27.18  76.37  1203.66
    2011-10-14  422.00  27.27  78.11  1224.58
              AAPL      MSFT       XOM  SPX
    2003  0.541124  0.745174  0.661265    1
    2004  0.374283  0.588531  0.557742    1
    2005  0.467540  0.562374  0.631010    1
    2006  0.428267  0.406126  0.518514    1
    2007  0.508118  0.658770  0.786264    1
    2008  0.681434  0.804626  0.828303    1
    2009  0.707103  0.654902  0.797921    1
    2010  0.710105  0.730118  0.839057    1
    2011  0.691931  0.800996  0.859975    1
    2003    0.480868
    2004    0.259024
    2005    0.300093
    2006    0.161735
    2007    0.417738
    2008    0.611901
    2009    0.432738
    2010    0.571946
    2011    0.581987
    [Finished in 1.7s]

#+END_SRC

3.4面向分组的线性回归
可以利用groupby进行更复杂的分析，只要返回的是pandas对象或者标量值即可。例如，定义下面的函数对每块进行最小二乘回归。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt
import statsmodels.api as sm

close_px = pd.read_csv('E:\\stock_px.csv',parse_dates = True,index_col = 0)
rets = close_px.pct_change().dropna()
#注意下面隐式的函数，作者好高明
by_year = rets.groupby(lambda x:x.year)

def regress(data,yvar,xvars):
    Y = data[yvar]
    X = data[xvars]
    X['intercept'] = 1
    result = sm.OLS(Y,X).fit()
    return result.params
print by_year.apply(regress,'AAPL',['SPX'])

#+END_SRC

    4、透视表和交叉表
    透视表很有用，能够比较轻松的完成groupby和更复杂的工作，DataFrame有pivot_table方法，顶级函数pands.pivot_table。除此之外，margins = True添加分项小计。 


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

tips = pd.read_csv('E:\\tips.csv')
tips['tip_pct'] = tips['tip'] / tips['total_bill']
print tips.head(),'\n'
#透视表默认的函数是mean
print tips.pivot_table(rows = ['sex','smoker']),'\n'  #这样的工作用groupby也能轻易完成
#print tips.groupby(['sex','smoker']).mean()
#现在用tip_pct和size进行聚合，而且想根据day进行分组，将smoker放在列上，把day放到行上
print tips.pivot_table(['tip_pct','size'],rows = ['sex','day'],cols = 'smoker'),'\n' #这个用groupby就比较费力了 
#可以添加选项  margins = True添加分项小计。这将会添加All行和列,这里的all行或者列将不考虑差别，算整体的值
print tips.pivot_table(['tip_pct','size'],rows = ['sex','day'],cols = 'smoker',margins = True),'\n'
#想应用别的函数，只要将函数传入aggfunc即可
print tips.pivot_table('tip_pct',rows = ['sex','smoker'],cols = 'day',aggfunc = len,margins = True),'\n'
#如果出现NA，经常出现，可以设置一个fill_value
print tips.pivot_table('size',rows = ['time','sex','smoker'],cols = 'day',aggfunc = 'sum',fill_value = 0)
>>>
   total_bill   tip     sex smoker  day    time  size   tip_pct
0       16.99  1.01  Female  False  Sun  Dinner     2  0.059447
1       10.34  1.66    Male  False  Sun  Dinner     3  0.160542
2       21.01  3.50    Male  False  Sun  Dinner     3  0.166587
3       23.68  3.31    Male  False  Sun  Dinner     2  0.139780
4       24.59  3.61  Female  False  Sun  Dinner     4  0.146808 
                       size       tip   tip_pct  total_bill
    sex    smoker                                          
    Female False   2.592593  2.773519  0.156921   18.105185
           True    2.242424  2.931515  0.182150   17.977879
    Male   False   2.711340  3.113402  0.160669   19.791237
           True    2.500000  3.051167  0.152771   22.284500
                  tip_pct                size          
    smoker          False     True      False     True 
    sex    day                                         
    Female Fri   0.165296  0.209129  2.500000  2.000000
           Sat   0.147993  0.163817  2.307692  2.200000
           Sun   0.165710  0.237075  3.071429  2.500000
           Thur  0.155971  0.163073  2.480000  2.428571
    Male   Fri   0.138005  0.144730  2.000000  2.125000
           Sat   0.162132  0.139067  2.656250  2.629630
           Sun   0.158291  0.173964  2.883721  2.600000
           Thur  0.165706  0.164417  2.500000  2.300000
                     size                       tip_pct                    
    smoker          False      True       All     False      True       All
    sex    day                                                             
    Female Fri   2.500000  2.000000  2.111111  0.165296  0.209129  0.199388
           Sat   2.307692  2.200000  2.250000  0.147993  0.163817  0.156470
           Sun   3.071429  2.500000  2.944444  0.165710  0.237075  0.181569
           Thur  2.480000  2.428571  2.468750  0.155971  0.163073  0.157525
    Male   Fri   2.000000  2.125000  2.100000  0.138005  0.144730  0.143385
           Sat   2.656250  2.629630  2.644068  0.162132  0.139067  0.151577
           Sun   2.883721  2.600000  2.810345  0.158291  0.173964  0.162344
           Thur  2.500000  2.300000  2.433333  0.165706  0.164417  0.165276
    All          2.668874  2.408602  2.569672  0.159328  0.163196  0.160803
    day            Fri  Sat  Sun  Thur  All
    sex    smoker                          
    Female False     2   13   14    25   54
           True      7   15    4     7   33
    Male   False     2   32   43    20   97
           True      8   27   15    10   60
    All             19   87   76    62  244
    day                   Fri  Sat  Sun  Thur
    time   sex    smoker                     
    Dinner Female False     2   30   43     2
                  True      8   33   10     0
           Male   False     4   85  124     0
                  True     12   71   39     0
    Lunch  Female False     3    0    0    60
                  True      6    0    0    17
           Male   False     0    0    0    50
                  True      5    0    0    23
    [Finished in 0.8s]

#+END_SRC

pivot_table的参数，已经全部用到过：

交叉表：crosstab
crosstab是一种计算分组频率（显然应该是频数）的特殊透视表，下面的例子很典型。也就是说这是生成列联表的函数。


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

tips = pd.read_csv('E:\\tips.csv')
tips['tip_pct'] = tips['tip'] / tips['total_bill']

data = DataFrame({'Sample':range(1,11,1),'Gender':['F','M','F','M','M','M','F','F','M','F'],'Handedness':['R','L','R','R','L','R','R','L','R','R']})
print pd.crosstab(data.Gender,data.Handedness,margins = True),'\n'
#crosstab的前两个参数可以是数组、Series或数组列表
print pd.crosstab([tips.time,tips.day],tips.smoker,margins = True),'\n'
>>>
Handedness  L  R  All
Gender               
F           1  4    5
M           2  3    5
All         3  7   10 
    smoker       False  True  All
    time   day                   
    Dinner Fri       3     9   12
           Sat      45    42   87
           Sun      57    19   76
           Thur      1     0    1
    Lunch  Fri       1     6    7
           Thur     44    17   61
    All            151    93  244
    [Finished in 0.7s]

#+END_SRC

5、2012联邦选举委员会数据库
这是一个例子，美国选举委员会有关政治精选竞选方面的数据。
根据职业和雇主统计赞助信息


#+BEGIN_SRC python
#-*- encoding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame

#加载数据,150M+,用时8.7s
fec = pd.read_csv('E:\\P00000001-ALL.csv')
print fec,'\n'
print fec.ix[123456],'\n'
#下面介绍几种不同的分析方法
#通过unique，你可以获取全部的候选人名单
unique_cands = fec.cand_nm.unique()
print unique_cands,'\n'
#下面将候选人和党派对应起来,额，写了半天，奥巴马是Democrat（民主党），其他人都是共和党……
parties = {'Bachmann, Michelle':'Republican',
'Cain, Herman':'Republican',
'Gingrich, Newt':'Republican',
'Huntsman, Jon':'Republican',
'Johnson, Gary Earl':'Republican',
'McCotter, Thaddeus G':'Republican',
'Obama, Barack':'Democrat',
'Paul, Ron':'Republican',
'Pawlenty, Timothy':'Republican',
'Perry, Rick':'Republican',
"Roemer, Charles E. 'Buddy' III":'Republican',
'Romney, Mitt':'Republican',
'Santorum, Rick':'Republican'}
#为其添加新列
fec['party'] = fec.cand_nm.map(parties)
print fec['party'].value_counts(),'\n'
#注意，这份数据既包括赞助也包括退款
print (fec.contb_receipt_amt > 0).value_counts(),'\n'
#为了简便，这里将只研究正出资额的部分
fec = fec[fec.contb_receipt_amt > 0]
#专门准备两个子集盛放奥巴马和Mitt Romney
fec_mrbo = fec[fec.cand_nm.isin(['Obama, Barack','Romney, Mitt'])]
#根据职业和雇主统计赞助信息，例如律师倾向于赞助民主党，企业主倾向于自主共和党
#下面看一下职业
print fec.contbr_occupation.value_counts()[:10],'\n'
#下面将这些职业进行一些处理（将一个职业信息映射到另一个）
occ_mapping = {
    'INFORMATION REQUESTED PER BEST EFFORTS':'NOT PROVIDED',
    'INFORMATION REQUESTED':'NOT PROVIDED',
    'INFORMATION REQUESTED (BEST EFFORTS)':'NOT PROVIDED',
    'C.E.O':'CEO'
}
#下面用了一个dict.get，下面的get第一个x是dict的键，映射到返回对应的key，第二个是没有映射到返回的内容，如果没有提供映射的话，返回x
f = lambda x:occ_mapping.get(x,x)
fec.contbr_occupation = fec.contbr_occupation.map(f)
#对雇主的信息也这样处理一下
emp_mapping = {
    'INFORMATION REQUESTED PER BEST EFFORTS':'NOT PROVIDED',
    'INFORMATION REQUESTED':'NOT PROVIDED',
    'SELF':'SELF-EMPLOYED',
    'SELF EMPLOYED':'SELF-EMPLOYED'
}
f = lambda x:emp_mapping.get(x,x)
fec.contbr_employer = fec.contbr_employer.map(f)
#下面可以通过pivot_table根据党派和职业对数据进行聚合，然后过滤掉出资总额不足200万美元的数据
by_occupation = fec.pivot_table('contb_receipt_amt',rows = 'contbr_occupation',cols = 'party',aggfunc = sum)
print by_occupation.head(),'\n'  #这个数据一定要看一下
over_2mm = by_occupation[by_occupation.sum(1) > 2000000]
print over_2mm
over_2mm.plot(kind = 'barh')
plt.show()
#你可能还想了解一下对OBAMA和ROMNEY总出资额最高的职业和企业，想法是先分组，然后再选取
def get_top_amounts(group,key,n = 5):
    totals = group.groupby(key)['contb_receipt_amt'].sum()
    return totals.order(ascending = False)[:n]  #作者书上写错了
grouped = fec_mrbo.groupby('cand_nm')
#下面的语句是说，grouped对象可以被进一步groupby
print grouped.apply(get_top_amounts,'contbr_occupation',n = 7),'\n'
print fec_mrbo.groupby(['cand_nm','contbr_occupation'])['contb_receipt_amt'].sum(),'\n' #不知道这里为啥不对……，为什么跟前面的语句结果不一样？……
#print fec_mrbo.pivot_table('contb_receipt_amt',rows = ['cand_nm','contbr_occupation'],aggfunc = 'sum')
print grouped.apply(get_top_amounts,'contbr_employer',n = 10)
>>>
<class 'pandas.core.frame.DataFrame'>
Int64Index: 1001731 entries, 0 to 1001730
Data columns:
cmte_id              1001731  non-null values
cand_id              1001731  non-null values
cand_nm              1001731  non-null values
contbr_nm            1001731  non-null values
contbr_city          1001716  non-null values
contbr_st            1001727  non-null values
contbr_zip           1001620  non-null values
contbr_employer      994314  non-null values
contbr_occupation    994433  non-null values
contb_receipt_amt    1001731  non-null values
contb_receipt_dt     1001731  non-null values
receipt_desc         14166  non-null values
memo_cd              92482  non-null values
memo_text            97770  non-null values
form_tp              1001731  non-null values
file_num             1001731  non-null values
dtypes: float64(1), int64(1), object(14) 
    cmte_id                             C00431445
    cand_id                             P80003338
    cand_nm                         Obama, Barack
    contbr_nm                         ELLMAN, IRA
    contbr_city                             TEMPE
    contbr_st                                  AZ
    contbr_zip                          852816719
    contbr_employer      ARIZONA STATE UNIVERSITY
    contbr_occupation                   PROFESSOR
    contb_receipt_amt                          50
    contb_receipt_dt                    01-DEC-11
    receipt_desc                              NaN
    memo_cd                                   NaN
    memo_text                                 NaN
    form_tp                                 SA17A
    file_num                               772372
    Name: 123456
    [Bachmann, Michelle Romney, Mitt Obama, Barack
    Roemer, Charles E. 'Buddy' III Pawlenty, Timothy Johnson, Gary Earl
    Paul, Ron Santorum, Rick Cain, Herman Gingrich, Newt McCotter, Thaddeus G
    Huntsman, Jon Perry, Rick]
    Democrat      593746
    Republican    407985
    True     991475
    False     10256
    RETIRED                                   233990
    INFORMATION REQUESTED                      35107
    ATTORNEY                                   34286
    HOMEMAKER                                  29931
    PHYSICIAN                                  23432
    INFORMATION REQUESTED PER BEST EFFORTS     21138
    ENGINEER                                   14334
    TEACHER                                    13990
    CONSULTANT                                 13273
    PROFESSOR                                  12555
    party                                Democrat  Republican
    contbr_occupation                                        
       MIXED-MEDIA ARTIST / STORYTELLER       100         NaN
    AREA VICE PRESIDENT                      250         NaN
    RESEARCH ASSOCIATE                       100         NaN
    TEACHER                                  500         NaN
    THERAPIST                               3900         NaN
    party                 Democrat       Republican
    contbr_occupation                              
    ATTORNEY           11141982.97   7477194.430000
    C.E.O.                 1690.00   2592983.110000
    CEO                 2074284.79   1640758.410000
    CONSULTANT          2459912.71   2544725.450000
    ENGINEER             951525.55   1818373.700000
    EXECUTIVE           1355161.05   4138850.090000
    HOMEMAKER           4248875.80  13634275.780000
    INVESTOR             884133.00   2431768.920000
    LAWYER              3160478.87    391224.320000
    MANAGER              762883.22   1444532.370000
    NOT PROVIDED        4866973.96  20565473.010000
    OWNER               1001567.36   2408286.920000
    PHYSICIAN           3735124.94   3594320.240000
    PRESIDENT           1878509.95   4720923.760000
    PROFESSOR           2165071.08    296702.730000
    REAL ESTATE          528902.09   1625902.250000
    RETIRED            25305116.38  23561244.489999
    SELF-EMPLOYED        672393.40   1640252.540000
    cand_nm        contbr_occupation                     
    Obama, Barack  RETIRED                                   25305116.38
                   ATTORNEY                                  11141982.97
                   INFORMATION REQUESTED                      4866973.96
                   HOMEMAKER                                  4248875.80
                   PHYSICIAN                                  3735124.94
                   LAWYER                                     3160478.87
                   CONSULTANT                                 2459912.71
    Romney, Mitt   RETIRED                                   11508473.59
                   INFORMATION REQUESTED PER BEST EFFORTS    11396894.84
                   HOMEMAKER                                  8147446.22
                   ATTORNEY                                   5364718.82
                   PRESIDENT                                  2491244.89
                   EXECUTIVE                                  2300947.03
                   C.E.O.                                     1968386.11
    Name: contb_receipt_amt
    cand_nm        contbr_occupation                  
    Obama, Barack     MIXED-MEDIA ARTIST / STORYTELLER     100
                    AREA VICE PRESIDENT                    250
                    RESEARCH ASSOCIATE                     100
                    TEACHER                                500
                    THERAPIST                             3900
                   -                                      5000
                   .NET PROGRAMMER                         481
                   07/13/1972                               98
                   12K ADVOCATE                            150
                   13D                                     721
                   1SG RETIRED                             210
                   1ST ASSISTANT DIRECTOR 2ND UNIT          35
                   1ST GRADE TEACHER                       435
                   1ST VP WEALTH MANAGEMENT                559
                   22ND CENTURY REALTY                     500
    ...
    Romney, Mitt  WRITER/ MUSIC PRODUCER      100
                  WRITER/AUTHOR              2500
                  WRITER/EDITOR               350
                  WRITER/INVESTOR              25
                  WRITER/MEDIA PRODUCER       300
                  WRITER/PRODUCER             225
                  WRITER/TRAINER               35
                  WUNDERMAN                  1000
                  YACHT BUILDER              2500
                  YACHT CAPTAIN               500
                  YACHT CONSTRUCTION         2500
                  YOGA INSTRUCTOR             500
                  YOGA TEACHER               2500
                  YOUTH CARE WORKER            25
                  YOUTH OUTREACH DIRECTOR    1000
    Name: contb_receipt_amt, Length: 35991
    cand_nm        contbr_employer                       
    Obama, Barack  RETIRED                                   22694358.85
                   SELF-EMPLOYED                             17080985.96
                   NOT EMPLOYED                               8586308.70
                   INFORMATION REQUESTED                      5053480.37
                   HOMEMAKER                                  2605408.54
                   SELF                                       1076531.20
                   SELF EMPLOYED                               469290.00
                   STUDENT                                     318831.45
                   VOLUNTEER                                   257104.00
                   MICROSOFT                                   215585.36
    Romney, Mitt   INFORMATION REQUESTED PER BEST EFFORTS    12059527.24
                   RETIRED                                   11506225.71
                   HOMEMAKER                                  8147196.22
                   SELF-EMPLOYED                              7409860.98
                   STUDENT                                     496490.94
                   CREDIT SUISSE                               281150.00
                   MORGAN STANLEY                              267266.00
                   GOLDMAN SACH & CO.                          238250.00
                   BARCLAYS CAPITAL                            162750.00
                   H.I.G. CAPITAL                              139500.00
    Name: contb_receipt_amt
    [Finished in 16.6s]

#+END_SRC
下面是上面代码中的图：


对出资额分组
还可以对该数据做另一种非常实用的分析：利用cut将数据分散到各个面元中。


#+BEGIN_SRC python
#部分代码
bins = np.array([0,1,10,100,1000,10000,100000,1000000,10000000])
labels = pd.cut(fec_mrbo.contb_receipt_amt,bins)
print labels,'\n'
#然后根据候选人姓名以及面元标签对数据进行分组
grouped = fec_mrbo.groupby(['cand_nm',labels])
print grouped.size().unstack(0),'\n'  #可以看出两个候选人不同面元捐款的数量
#还可以对出资额求和并在面元内规格化，以便图形化显示两位候选人各种赞助的比例
bucket_sums = grouped.contb_receipt_amt.sum().unstack(0)
print bucket_sums,'\n'
normed_sums = bucket_sums.div(bucket_sums.sum(axis = 1),axis = 0)
print normed_sums,'\n'
#排除最大的两个面元并作图：
normed_sums[:-2].plot(kind = 'barh',stacked = True)
plt.show()
>>>
Categorical: contb_receipt_amt
array([(10, 100], (100, 1000], (100, 1000], ..., (1, 10], (10, 100],
       (100, 1000]], dtype=object)
Levels (8): Index([(0, 1], (1, 10], (10, 100], (100, 1000],
                   (1000, 10000], (10000, 100000], (100000, 1000000],
                   (1000000, 10000000]], dtype=object) 
    cand_nm              Obama, Barack  Romney, Mitt
    contb_receipt_amt                               
    (0, 1]                         493            77
    (1, 10]                      40070          3681
    (10, 100]                   372280         31853
    (100, 1000]                 153991         43357
    (1000, 10000]                22284         26186
    (10000, 100000]                  2             1
    (100000, 1000000]                3           NaN
    (1000000, 10000000]              4           NaN
    cand_nm              Obama, Barack  Romney, Mitt
    contb_receipt_amt                               
    (0, 1]                      318.24         77.00
    (1, 10]                  337267.62      29819.66
    (10, 100]              20288981.41    1987783.76
    (100, 1000]            54798531.46   22363381.69
    (1000, 10000]          51753705.67   63942145.42
    (10000, 100000]           59100.00      12700.00
    (100000, 1000000]       1490683.08           NaN
    (1000000, 10000000]     7148839.76           NaN
    cand_nm              Obama, Barack  Romney, Mitt
    contb_receipt_amt                               
    (0, 1]                    0.805182      0.194818
    (1, 10]                   0.918767      0.081233
    (10, 100]                 0.910769      0.089231
    (100, 1000]               0.710176      0.289824
    (1000, 10000]             0.447326      0.552674
    (10000, 100000]           0.823120      0.176880
    (100000, 1000000]         1.000000           NaN
    (1000000, 10000000]       1.000000           NaN
    [Finished in 221.9s]

#+END_SRC

        下面是最后的图形：

根据州统计赞助信息
这部分由于地图模块装不上就先放一下。

#+BEGIN_SRC python

#部分代码
grouped = fec_mrbo.groupby(['cand_nm','contbr_st'])
totals = grouped.contb_receipt_amt.sum().unstack(0).fillna(0)
totals = totals[totals.sum(1) > 100000]
print totals[:10],'\n'
percent = totals.div(totals.sum(1),axis = 0)
print percent[:10]
>>>
cand_nm    Obama, Barack  Romney, Mitt
contbr_st                             
AK             281840.15      86204.24
AL             543123.48     527303.51
AR             359247.28     105556.00
AZ            1506476.98    1888436.23
CA           23824984.24   11237636.60
CO            2132429.49    1506714.12
CT            2068291.26    3499475.45
DC            4373538.80    1025137.50
DE             336669.14      82712.00
FL            7318178.58    8338458.81 
    cand_nm    Obama, Barack  Romney, Mitt
    contbr_st                             
    AK              0.765778      0.234222
    AL              0.507390      0.492610
    AR              0.772902      0.227098
    AZ              0.443745      0.556255
    CA              0.679498      0.320502
    CO              0.585970      0.414030
    CT              0.371476      0.628524
    DC              0.810113      0.189887
    DE              0.802776      0.197224
    FL              0.467417      0.532583
    [Finished in 18.1s]
#+END_SRC

*  第十章 时间序列（一）


时间序列是很重要的。时间序列（time series）数据是一种重要的结构化数据格式。时间序列的意义取决于具体的应用场景，主要有以下几种：
时间戳（timestamp），特定的时刻
固定时期（period），如2015年全年
时间间隔（interval），由起始和结束时间戳表示。就是说，时期可以是时间间隔的特例。
实验或过程时间，每个时间点都是相对于特定起始时间的一个度量。例如，自从放入烤箱时起，每秒钟饼干的直径。
pandas提供了一组标准的时间序列处理工具和数据算法。因此可以高效处理非常大的时间序列，轻松进行切片/切块、聚合、对定期/不定期的时间序列进行重采样等。也就是说，大部分都对金融和经济数据尤为有用，当然也可以用它们来分析服务器日志数据。
1、日期和时间数据类型及工具
Python标准库中包含用于日期（date）、时间（time）数据的数据类型。而且还有日历方面的功能。主要会用到datetime、time、calendar模块。


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from datetime import datetime

now = datetime.now()
#datetime以毫秒形式储存时间
print now,now.year,now.month,now.day,now.microsecond,'\n'
#print datetime(2015,12,17,20,00,01,555555) #设置一个时间
#datetime.timedelta表示两个datetime对象之间的时间差
#换句话说，datetime格式可以相相减
delta = datetime(2011,1,7) - datetime(2008,6,24,8,15)
print delta
#把注意下面是days And seconds
print dt.timedelta(926,56700)
print delta.days
print delta.seconds
#下面是错误的
#print delta.minutes
start = datetime(2011,1,7)
#参数分别为days,seconds,microseconds(微秒),milliseconds（毫秒）,minutes,hours,weeks,除了微秒小数自动四舍五入之外，其他的都能自动转换为其他度量
print start + dt.timedelta(1,20,0.5,5,10,10,0)
>>>
2015-12-17 20:24:21.829000 2015 12 17 829000 
    926 days, 15:45:00
    926 days, 15:45:00
    926
    56700
    2011-01-08 10:10:20.005001
    [Finished in 0.6s]

#+END_SRC

datetime中的数据类型有：

字符串和datetime的相互转换
利用str或者strftime方法（传入一个格式化字符串），datetime对象和pandas中timestamp对象就可以转换为字符串：


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from datetime import datetime
from dateutil.parser import parse

stamp = datetime(2011,1,3)
print str(stamp),'\n'
#看一下下面的字符，很有意思，自己不小心打错了，运行仍然是正确的
print stamp.strftime('&Y-%m-%d')
print stamp.strftime('%Y-%m-%d'),'\n'
value = '2011-01-03'
print datetime.strptime(value,'%Y-%m-%d') #注意这是datetime函数的函数，不是模块的函数
datestrs = ['7/6/2011','8/6/2011']
print [datetime.strptime(x,'%m/%d/%Y') for x in datestrs]
#上面将字符串转化为最常用的格式，但是米次都自己写出来有点麻烦，可以用dateutil这个第三方包中的parser.parse方法
print parse('2011-01-03')
#dateutil可以几乎解析所有能够理解的日期表示形式（很可惜中文不行）
#这个应该是很实用的
print parse('2011/01/03')
print parse('Jan 31,1997 10:45 PM')
#国际通用格式中，日出现在月的前面，传入dayfirst = True即可
print parse('6/12/2011',dayfirst = True),'\n'
#pandas通常是用于处理成组日期的，不管这些日期是DataFrame的行还是列。
print pd.to_datetime(datestrs),'\n'
idx = pd.to_datetime(datestrs + [None])
print idx
print idx[2] #这里应该是NaT（Not a Time）
print pd.isnull(idx)
#parse是一个不完美的工具，比如下面
print parse('42')
>>>
2011-01-03 00:00:00 
    &Y-01-03
    2011-01-03
    2011-01-03 00:00:00
    [datetime.datetime(2011, 7, 6, 0, 0), datetime.datetime(2011, 8, 6, 0, 0)]
    2011-01-03 00:00:00
    2011-01-03 00:00:00
    1997-01-31 22:45:00
    2011-12-06 00:00:00
    <class 'pandas.tseries.index.DatetimeIndex'>
    [2011-07-06 00:00:00, 2011-08-06 00:00:00]
    Length: 2, Freq: None, Timezone: None
    <class 'pandas.tseries.index.DatetimeIndex'>
    [2011-07-06 00:00:00, ..., NaT]
    Length: 3, Freq: None, Timezone: None
    0001-255-255 00:00:00
    [False False  True]
    2042-12-17 00:00:00
    [Finished in 0.6s]

#+END_SRC

#+RESULTS:

下面是日期的一些格式：


datetime对象还有一些特定于当前环境（位于不同国家或使用不同语言系统）的格式化选项。估计用的少？

2、时间序列基础
pandas最基本的时间序列类型就是以时间戳（通常用Python字符串或datatime对象表示）为索引的Series。


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse

dates = [datetime(2011,1,2),datetime(2011,1,5),datetime(2011,1,7),
datetime(2011,1,8),datetime(2011,1,10),datetime(2011,1,12)]
#print dates
ts = Series(np.random.randn(6),index = dates)
print ts,'\n'
#这些datetime对象实际上是被放在一个DatetimeIndex中的。现在，变量ts就成为了TimeSeries了。
print type(ts)
print ts.index,'\n'
#没必要显示使用TimeSeries的构造函数。当创建一个带有DatetimeIndex的Series时，pandas就会知道该对象是一个时间序列
print ts + ts[::2]
#pandas用NumPy的datetime64数据类型以纳秒形式存储时间戳：
print ts.index.dtype
#DatetimeIndex中的各个标量值是pandas的Timestamp
stamp = ts.index[0]
print stamp
#只要有需要，TimeStamp可以随时自动转换为datetime对象。此外，还可以存储频率信息，且知道如何执行时区转换以及其他操作
>>>
2011-01-02   -1.267108
2011-01-05   -0.450098
2011-01-07    0.784850
2011-01-08    0.024722
2011-01-10    0.638663
2011-01-12    0.246022 
    <class 'pandas.core.series.TimeSeries'>
    <class 'pandas.tseries.index.DatetimeIndex'>
    [2011-01-02 00:00:00, ..., 2011-01-12 00:00:00]
    Length: 6, Freq: None, Timezone: None
    2011-01-02   -2.534216
    2011-01-05         NaN
    2011-01-07    1.569701
    2011-01-08         NaN
    2011-01-10    1.277326
    2011-01-12         NaN
    datetime64[ns]
    2011-01-02 00:00:00
    [Finished in 0.7s]

#+END_SRC

索引、选取、子集构造
TimeSeries是Series的一个子类，所以在索引以及数据选取方面跟Series一样。


#+BEGIN_SRC python
stamp = ts.index[2]
print ts[stamp],'\n'
#还有更方便的用法，传入可以被解释为日期的字符串
print ts['1/10/2011']
print ts['20110110'],'\n'
#对于较长的时间序列，只需传入“年”或“年月”即可轻松选取数据切片
long_ts = Series(np.random.randn(1000),
    index = pd.date_range('1/1/2000',periods = 1000))
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time

print long_ts,'\n'
print long_ts['2001'],'\n'
print long_ts['2001-05'],'\n'
#通过日期进行切片的方式只对规则Series有效：
print ts[datetime(2011,1,7):],'\n'
#由于大部分时间序列数据都是按照时间先后排序的，因此你可以用不存在于该时间序列中的时间戳对其进行切片（即范围查询）
#就是说，本来1/6/2011不在index中，却可以用来当作范围
print ts['1/6/2011':'1/11/2011'],'\n' #这里可以传入字符串日期、datetime或者Timestamp

print 'This is time and localtime'
print "time.time(): %f " %  time.time()
print time.localtime( time.time() )
print time.asctime( time.localtime(time.time()) )
ltime=time.localtime(int(time.time()))   #time.time()不能直接运用strftime进行转换
print time.strftime("%Y-%m-%d %H:%M:%S", ltime)
#time asctime() 函数接受时间元组并返回一个可读的形式为"Tue Dec 11 18:07:14 2008"
print 'over','\n'

#还有一个等价方法截取两个日期之间的TimeSeries.
print ts.truncate(after = '1/9/2011'),'\n'

#上面这些对DataFrame也有效
dates = pd.date_range('1/1/2000',periods = 100,freq = 'W-WED') #这里的freq是按照星期进行增加
long_df = DataFrame(np.random.randn(100,4),index = dates,columns = ['Colorado','Texas','New York','Ohio'])
print long_df.ix['2001-05']
>>>
0.0751316698811 
-0.622706612554
-0.622706612554 
    2000-01-01   -1.646726
    2000-01-02    1.531423
    2000-01-03    0.251503
    2000-01-04    0.938951
    2000-01-05    0.647967
    2000-01-06    0.696173
    2000-01-07   -1.372519
    2000-01-08   -1.398277
    2000-01-09   -0.679975
    2000-01-10   -0.801375
    2000-01-11   -0.241165
    2000-01-12   -0.332811
    2000-01-13   -0.337774
    2000-01-14    0.826756
    2000-01-15   -0.279239
    ...
    2002-09-12   -0.097634
    2002-09-13    2.222456
    2002-09-14    0.042517
    2002-09-15    0.266974
    2002-09-16    0.038329
    2002-09-17   -1.524744
    2002-09-18    1.476706
    2002-09-19    0.108336
    2002-09-20    0.016759
    2002-09-21   -0.072676
    2002-09-22   -0.960545
    2002-09-23    0.520699
    2002-09-24   -1.188202
    2002-09-25    1.669166
    2002-09-26   -0.043997
    Freq: D, Length: 1000
    2001-01-01   -0.168866
    2001-01-02   -0.273377
    2001-01-03    0.094258
    2001-01-04   -0.979666
    2001-01-05    0.947706
    2001-01-06    0.666709
    2001-01-07    0.451145
    2001-01-08   -0.301992
    2001-01-09    0.272385
    2001-01-10   -0.255775
    2001-01-11   -0.321916
    2001-01-12    1.894119
    2001-01-13    0.582272
    2001-01-14   -1.102707
    2001-01-15    0.019423
    ...
    2001-12-17   -0.243563
    2001-12-18    1.757564
    2001-12-19   -0.145106
    2001-12-20   -0.579629
    2001-12-21   -0.431069
    2001-12-22    0.480805
    2001-12-23   -0.651905
    2001-12-24    0.702051
    2001-12-25   -0.384549
    2001-12-26   -1.077664
    2001-12-27   -0.972768
    2001-12-28    1.001220
    2001-12-29    0.418016
    2001-12-30    0.567361
    2001-12-31   -0.811610
    Freq: D, Length: 365
    2001-05-01   -0.071521
    2001-05-02    0.402344
    2001-05-03   -0.568929
    2001-05-04    0.227754
    2001-05-05    0.194631
    2001-05-06   -0.407669
    2001-05-07   -1.407606
    2001-05-08   -0.804147
    2001-05-09    0.050445
    2001-05-10   -0.604275
    2001-05-11    0.270760
    2001-05-12    0.000804
    2001-05-13   -0.348938
    2001-05-14   -1.626158
    2001-05-15    0.084629
    2001-05-16   -0.376655
    2001-05-17    1.913789
    2001-05-18    2.497594
    2001-05-19    0.818446
    2001-05-20    0.067115
    2001-05-21   -0.993827
    2001-05-22    0.940616
    2001-05-23   -0.951763
    2001-05-24   -0.806228
    2001-05-25    0.441872
    2001-05-26    0.067010
    2001-05-27   -1.903360
    2001-05-28   -0.400990
    2001-05-29    0.257146
    2001-05-30    0.785503
    2001-05-31   -1.129024
    Freq: D
    2011-01-07    0.075132
    2011-01-08   -0.985630
    2011-01-10   -0.622707
    2011-01-12   -1.356095
    2011-01-07    0.075132
    2011-01-08   -0.985630
    2011-01-10   -0.622707
    This is time and localtime
    time.time(): 1450362054.149000 
    time.struct_time(tm_year=2015, tm_mon=12, tm_mday=17, tm_hour=22, tm_min=20, tm_sec=54, tm_wday=3, tm_yday=351, tm_isdst=0)
    Thu Dec 17 22:20:54 2015
    2015-12-17 22:20:54
    over
    2011-01-02   -0.772858
    2011-01-05   -0.908074
    2011-01-07    0.075132
    2011-01-08   -0.985630
                Colorado     Texas  New York      Ohio
    2001-05-02  0.303341  0.026978 -0.036389  0.463034
    2001-05-09 -1.573227 -0.283074 -0.882382 -1.207936
    2001-05-16  1.520804 -0.838297  0.725690  1.240092
    2001-05-23  1.297194 -0.516198 -0.022075 -0.876630
    2001-05-30 -1.629426  1.022547 -0.131823 -0.621269
    [Finished in 0.7s]

#+END_SRC

带有重复索引的时间序列


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time

#注意下面的DatetimeIndex生成方式
dates = pd.DatetimeIndex(['1/1/2000','1/2/2000','1/2/2000','1/2/2000','1/3/2000'])
dup_ts = Series(np.arange(5),index = dates)
print dup_ts,'\n'
#通过检唯一的测is_unique属性，我们就可以知道它不是
print dup_ts.index.is_unique,'\n'
#此时若索引，得到的可能是标量值，也可能是切片
print dup_ts['1/2/2000'],'\n'
print dup_ts['1/3/2000']
#假如你想要对具有非
#唯一时间戳的数据进行聚合一个办法是使用groupby，并传入level = 0
grouped = dup_ts.groupby(level = 0)
print grouped.mean(),'\n'
print grouped.count()
>>>
2000-01-01    0
2000-01-02    1
2000-01-02    2
2000-01-02    3
2000-01-03    4 
    False
    2000-01-02    1
    2000-01-02    2
    2000-01-02    3
    4
    2000-01-01    0
    2000-01-02    2
    2000-01-03    4
    2000-01-01    1
    2000-01-02    3
    2000-01-03    1
    [Finished in 1.3s]

#+END_SRC

3、日期的范围、频率以及移动
有时候需要用相对固定的频率对数据进行分析，比如每月、每天等。幸运的是，pandas有一整套标准时间序列频率以及用于重采样、频率推断、生成固定频率日期范围的工具。


#+BEGIN_SRC python
#定义列表
dates = [datetime(2011,1,2),datetime(2011,1,5),datetime(2011,1,7),
datetime(2011,1,8),datetime(2011,1,10),datetime(2011,1,12)]
#print dates
ts = Series(np.random.randn(6),index = dates)
#print ts
#下面进行重采样，得到具有固定时间频率（每天）的时间序列，当让这样的话就会产生缺失值
print ts.resample('D')

#+END_SRC


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time

#定义列表
dates = [datetime(2011,1,2),datetime(2011,1,5),datetime(2011,1,7),
datetime(2011,1,8),datetime(2011,1,10),datetime(2011,1,12)]
#print dates
ts = Series(np.random.randn(6),index = dates)
#print ts
#下面进行重采样，得到具有固定时间频率（每天）的时间序列，当让这样的话就会产生缺失值
print ts.resample('D') #频率的转换（或重采样）主题较大，后面再说
>>>
2011-01-02   -0.956627
2011-01-03         NaN
2011-01-04         NaN
2011-01-05    0.130565
2011-01-06         NaN
2011-01-07    0.090270
2011-01-08    0.753881
2011-01-09         NaN
2011-01-10   -0.733514
2011-01-11         NaN
2011-01-12   -0.200039
Freq: D
[Finished in 1.2s]

#+END_SRC

生成日期范围


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time

#pandas.date_range会生成指定长度的DatetimeIndex
index = pd.date_range('4/1/2015','6/1/2015')
print index,'\n'
#默认情况下，date_range产生按天计算的时间点，当然可以传入开始或结束日期，还得传入一个表示一段时间的数字
print pd.date_range('1/1/2016',periods = 31),'\n'
#开始和结束定义了日期索引的严格边界，如果你想要生成一个由每月最后一个工作日组成的日期索引，可以传入‘BM’（business end of month）
#这样就只会包含时间间隔内（或者放好在时间边界上）符合频率要求的日期：
print pd.date_range('12/18/2015','1/1/2016',freq = 'BM'),'\n'
#date_range默认保留起始和结束时间戳信息
print pd.date_range('5/2/2015 12:12:12',periods = 5)
#有时，虽然起始和结束带有时间信息，但是可以用normalize = True把它们吧变为00:00:00
print pd.date_range('5/2/2015 12:12:12',periods = 5,normalize = True)
>>>
<class 'pandas.tseries.index.DatetimeIndex'>
[2015-04-01 00:00:00, ..., 2015-06-01 00:00:00]
Length: 62, Freq: D, Timezone: None 
    <class 'pandas.tseries.index.DatetimeIndex'>
    [2016-01-01 00:00:00, ..., 2016-01-31 00:00:00]
    Length: 31, Freq: D, Timezone: None
    <class 'pandas.tseries.index.DatetimeIndex'>
    [2015-12-31 00:00:00]
    Length: 1, Freq: BM, Timezone: None
    <class 'pandas.tseries.index.DatetimeIndex'>
    [2015-05-02 12:12:12, ..., 2015-05-06 12:12:12]
    Length: 5, Freq: D, Timezone: None
    <class 'pandas.tseries.index.DatetimeIndex'>
    [2015-05-02 00:00:00, ..., 2015-05-06 00:00:00]
    Length: 5, Freq: D, Timezone: None
    [Finished in 1.1s]

#+END_SRC

频率和日期偏移量
    有些频率所描述的时间点并不是均匀分隔的。例如'M'和'BM'就取决于每月的天数，对于后者，还要考虑月末是不是周末，将这些成为锚点偏移量（anchored offset）。pandas还允许自定义一些日期逻辑，但是暂且不表。


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute

#pandas中的频率是由一个基础频率和一个乘数组成的。基础的频率由字符串表示，比如‘M’表示月，‘H’表示小时
#对于每个基础频率，都有一个被称为日期偏移量（date offset）的对象与之对应。
hour = Hour()
print hour #感觉这个形式比较霸气
#传入整数可以自定义偏移量倍数
four_hours = Hour(4)
print four_hours
#一般而言，并不需要显示创建偏移量，只需创建时间序列时传入'H'或者'4h'即可
print pd.date_range('1/1/2016','1/2/2016',freq = '4h'),'\n'
#偏移量可以拼接
print Hour(1) + Minute(30)
#传入频率字符串（'2h30min'）,这种字符串可以被高效地解析为等效的表达式
print pd.date_range('1/1/2016',periods = 10,freq = '1h30min'),'\n'
#有些频率所描述的时间点并不是均匀分隔的。例如'M'和'BM'就取决于每月的天数，对于后者，还要考虑月末是不是周末，将这些成为锚点偏移量（anchored offset）
#WOM(Week Of Month)日期是一个非常常用的频率，以WOM开头，能产生诸如“每月第三个星期五”之类的信息
rng = pd.date_range('1/1/2016','9/1/2016',freq = 'WOM-3FRI')
print rng
>>>
<1 Hour>
<4 Hours>
<class 'pandas.tseries.index.DatetimeIndex'>
[2016-01-01 00:00:00, ..., 2016-01-02 00:00:00]
Length: 7, Freq: 4H, Timezone: None 
    <90 Minutes>
    <class 'pandas.tseries.index.DatetimeIndex'>
    [2016-01-01 00:00:00, ..., 2016-01-01 13:30:00]
    Length: 10, Freq: 90T, Timezone: None
    <class 'pandas.tseries.index.DatetimeIndex'>
    [2016-01-15 00:00:00, ..., 2016-08-19 00:00:00]
    Length: 8, Freq: WOM-3FRI, Timezone: None
    [Finished in 1.1s]

#+END_SRC

    下面是一些常用的基础频率，很多很详细。

    

移动（超前和滞后）数据
移动（shifting）指的是沿着时间轴将数据前移或后移。Series和DataFrame都有一个shift方法用于执行单纯的前移或后移操作，保持索引不变。


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute

ts = Series(np.random.randn(4),index = pd.date_range('1/1/2016',periods = 4,freq = 'M'))
print ts
print ts.shift(2)
print ts.shift(-2),'\n'
#可以看到，shift通常用于计算一个时间序列或多个时间序列（如DataFrame列）中的百分比变化。
print ts / ts.shift(1) - 1
#单纯的移位操作不会修改索引，所以部分数据会被丢弃，如果频率已知，则可以将其传给shift以实现对时间戳进行位移而不是只对数据移位
print ts.shift(2,freq = 'M')  #时间戳移动，而数据不动
#当然也可以自己定义移动的频率
print ts.shift(3,freq = 'D'),'\n'  #时间的移动不是上下移动，而是将时间列的每个值进行移动
print ts.shift(1,freq = '3D')
print ts.shift(1,freq = '90T')
>>>
2016-01-31    0.721445
2016-02-29   -0.568200
2016-03-31   -0.945288
2016-04-30    0.198176
Freq: M
2016-01-31         NaN
2016-02-29         NaN
2016-03-31    0.721445
2016-04-30   -0.568200
Freq: M
2016-01-31   -0.945288
2016-02-29    0.198176
2016-03-31         NaN
2016-04-30         NaN
Freq: M 
    2016-01-31         NaN
    2016-02-29   -1.787585
    2016-03-31    0.663653
    2016-04-30   -1.209646
    Freq: M
    2016-03-31    0.721445
    2016-04-30   -0.568200
    2016-05-31   -0.945288
    2016-06-30    0.198176
    Freq: M
    2016-02-03    0.721445
    2016-03-03   -0.568200
    2016-04-03   -0.945288
    2016-05-03    0.198176
    2016-02-03    0.721445
    2016-03-03   -0.568200
    2016-04-03   -0.945288
    2016-05-03    0.198176
    2016-01-31 01:30:00    0.721445
    2016-02-29 01:30:00   -0.568200
    2016-03-31 01:30:00   -0.945288
    2016-04-30 01:30:00    0.198176
    [Finished in 0.7s]

#+END_SRC

通过偏移量对日期进行位移
pandas的日期偏移量还可以用在datetime或Timestemp对象上。


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd

now = datetime(2011,11,29)
print type(now)
print now + Day(3),'\n'
#如果加的是锚点偏移量，第一次增量会将原日期向前滚动到符合频率规则的下一个日期
#如果本来就是锚点，那么下一个就是下一个锚点
print now + MonthEnd(),'\n'
print now + MonthEnd(2),'\n'
#通过锚点偏移量的rollforward和rollback方法，可显示地将日期向前或向后“滚动”
offset = MonthEnd()
print offset.rollforward(now),'\n'
print offset.rollback(now),'\n'
#日期偏移量还有一个巧妙的用法，即结合groupby使用这两个“滚动”方法
ts = Series(np.random.randn(20),index = pd.date_range('1/15/2000',periods = 20,freq = '4d'))
print ts,'\n'
#注意下面的方式，很隐晦
print ts.groupby(offset.rollforward).mean(),'\n'
#当然，更简单快速的方式是使用resample
print ts.resample('M',how = 'mean')
>>>
<type 'datetime.datetime'>
2011-12-02 00:00:00 
2011-11-30 00:00:00 
2011-12-31 00:00:00 
2011-11-30 00:00:00 
2011-10-31 00:00:00 
2000-01-15   -1.234284
2000-01-19   -1.078641
2000-01-23   -0.727257
2000-01-27   -0.943798
2000-01-31    0.050586
2000-02-04    0.019833
2000-02-08   -1.407244
2000-02-12   -0.446414
2000-02-16   -0.521847
2000-02-20    0.066200
2000-02-24    1.604580
2000-02-28   -0.714762
2000-03-03    1.743459
2000-03-07    1.675388
2000-03-11    0.104701
2000-03-15    0.124533
2000-03-19   -1.113306
2000-03-23   -1.442906
2000-03-27   -0.489818
2000-03-31    0.344161
Freq: 4D 
2000-01-31   -0.786679
2000-02-29   -0.199950
2000-03-31    0.118276 
 
2000-01-31   -0.786679
2000-02-29   -0.199950
2000-03-31    0.118276
Freq: M
[Finished in 0.7s]

#+END_SRC
4、时区处理
时间序列最让人不爽的就是对时区的处理。很多人已经用协调世界时（UTC，格林尼治时间接替者，目前是国际标准）来处理时间序列。时区就是以UTC偏移量的形式表示的。
Python中，时区信息来自第三方库pytz，它可以使Python可以使用Olson数据库。pandas包装了pytz功能。因此不用记忆API，只要记得时区名称即可。时区名可以在文档中找到。



#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz

print pytz.common_timezones[-5:]
#要从pytz中获取时区对象，使用pytz.timezone即可
tz = pytz.timezone('US/Eastern')
print tz #这里的输出已经和课本上不一样，估计是进行了简化，使得更方便了
>>>
['US/Eastern', 'US/Hawaii', 'US/Mountain', 'US/Pacific', 'UTC']
US/Eastern
[Finished in 0.7s]

#+END_SRC

本地化和转换
默认情况下，pandas中的序列是单纯的（naive[too young too simple!navie!]）时区。


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz

rng = pd.date_range('3/9/2012 9:30',periods = 6,freq = 'D')
ts = Series(np.random.randn(len(rng)),index = rng)
print ts,'\n'
print ts.index.tz,'\n'  #默认的时区字段为None
#在生成日期范围的时候还可以加上一个时区集
print pd.date_range('3/9/2012',periods = 10,freq = 'D',tz = 'UTC'),'\n'
#从单纯到本地化的转换是通过tz_localize方法处理的：
ts_utc = ts.tz_localize('US/Pacific')  #转换为美国太平洋时间
print ts_utc,'\n'
print ts_utc.index,'\n'
#一旦被转换为某个特定时期，就可以用tz_convert将其转换到其他时区了
print ts_utc.tz_convert('US/Eastern'),'\n'
#tz_localize和tz_convert是DatetimeIndex的实例方法，可以把一个DatetimeIndex转化为特定时区
print ts.index.tz_localize('Asia/Shanghai')
>>>
2012-03-09 09:30:00    0.079530
2012-03-10 09:30:00   -0.434450
2012-03-11 09:30:00    0.360739
2012-03-12 09:30:00    0.678065
2012-03-13 09:30:00   -0.705374
2012-03-14 09:30:00    0.684572
Freq: D 
    None
    <class 'pandas.tseries.index.DatetimeIndex'>
    [2012-03-09 00:00:00, ..., 2012-03-18 00:00:00]
    Length: 10, Freq: D, Timezone: UTC
    2012-03-09 09:30:00-08:00    0.079530
    2012-03-10 09:30:00-08:00   -0.434450
    2012-03-11 09:30:00-07:00    0.360739
    2012-03-12 09:30:00-07:00    0.678065
    2012-03-13 09:30:00-07:00   -0.705374
    2012-03-14 09:30:00-07:00    0.684572
    Freq: D
    <class 'pandas.tseries.index.DatetimeIndex'>
    [2012-03-09 09:30:00, ..., 2012-03-14 09:30:00]
    Length: 6, Freq: D, Timezone: US/Pacific
    2012-03-09 12:30:00-05:00    0.079530
    2012-03-10 12:30:00-05:00   -0.434450
    2012-03-11 12:30:00-04:00    0.360739
    2012-03-12 12:30:00-04:00    0.678065
    2012-03-13 12:30:00-04:00   -0.705374
    2012-03-14 12:30:00-04:00    0.684572
    Freq: D
    <class 'pandas.tseries.index.DatetimeIndex'>
    [2012-03-09 09:30:00, ..., 2012-03-14 09:30:00]
    Length: 6, Freq: D, Timezone: Asia/Shanghai
    [Finished in 0.6s]

#+END_SRC

操作时区意识型（time zone-aware）Timestamp对象
跟时间序列和日期序列差不多，Timestamp对象也能被从单纯型（navie）本地化为time zone-aware，并从一个时区转换为另一个时区。


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz

stamp = pd.Timestamp('2011-03-12 04:00')
print type(stamp),'\n'
stamp_utc = stamp.tz_localize('UTC')
print stamp_utc,'\n'
print stamp_utc.tz_convert('US/Eastern'),'\n'
stamp_moscow = pd.Timestamp('2011-03-12 04:00',tz = 'Europe/Moscow')
print stamp_moscow
#时区意识型Timestamp对象在内部保存了一个UTC时间戳值（自1970年1月1日起的纳秒数），这个UTC值在时区转换过程中是不会变化的
print stamp_utc.value
print stamp_utc.tz_convert('US/Eastern').value,'\n'
#当使用pandas的DataOffset对象执行运算时，会自动关注“夏时令”…………
>>>
    <class 'pandas.lib.Timestamp'>
    2011-03-12 04:00:00+00:00
    2011-03-11 23:00:00-05:00
    2011-03-12 04:00:00+03:00
    1299902400000000000
    1299902400000000000
    [Finished in 0.7s]

#+END_SRC

不同时区之间的运算
如果时间时间时区不同，那么结果就会是UTC时间，由于时间戳其实是以UTC储存的，索引计算很方便。


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz

rng = pd.date_range('3/7/2012',periods = 10,freq = 'B')
ts = Series(np.random.randn(len(rng)),index = rng)
print ts
ts1 = ts[:7].tz_localize('Europe/London')
#注意naive是不能直接转换为时区的，必须先转换为localize再进行转换
ts2 = ts1[2:].tz_convert('Europe/Moscow')
result = ts1 + ts2
#转换为UTC
print result.index
>>>
2012-03-07   -0.591417
2012-03-08    1.009651
2012-03-09   -1.922004
2012-03-12    0.246206
2012-03-13    0.033430
2012-03-14    0.614911
2012-03-15    1.944014
2012-03-16   -2.349846
2012-03-19    0.425925
2012-03-20    1.941166
Freq: B
<class 'pandas.tseries.index.DatetimeIndex'>
[2012-03-07 00:00:00, ..., 2012-03-15 00:00:00]
Length: 7, Freq: B, Timezone: UTC
[Finished in 0.7s]
#+END_SRC

*  第十章 时间序列（二）

5、时期及其算数运算
时期（period）表示的是时间区间，比如数日、数月、数季、数年等。Period类所表示的就是这种数据类型，其构造函数需要用到一个字符串或整数，以及频率。


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz

#下面的'A-DEC'是年第12月底最后一个日历日
p = pd.Period('2016',freq = 'A-DEC')
#Period可以直接加减
print p + 5
#相同频率的Period可以进行加减,不同频率是不能加减的
rng = pd.Period('2015',freq = 'A-DEC') - p
print rng
rng = pd.period_range('1/1/2000','6/30/2000',freq = 'M')
#类型是<class 'pandas.tseries.period.PeriodIndex'>，形式上是一个array数组
#注意下面的形式已经不是书上的形式，而是float类型，但是做索引时，还是日期形式
print rng
print type(rng)
print Series(np.random.randn(6),index = rng),'\n'
#PeriodIndex类的构造函数还允许直接使用一组字符串
values = ['2001Q3','2002Q2','2003Q1']
index = pd.PeriodIndex(values,freq = 'Q-DEC')
#下面index的
print index
>>>
2021
-1
array([360, 361, 362, 363, 364, 365], dtype=int64)
<class 'pandas.tseries.period.PeriodIndex'>
2000-01   -0.504031
2000-02    1.345024
2000-03    0.074367
2000-04   -1.152187
2000-05   -0.460272
2000-06    0.486135
Freq: M 
    array([126, 129, 132], dtype=int64)
    [Finished in 1.4s]

#+END_SRC

时期的频率转换
Period和PeriodIndex对象都可以通过其asfreq方法转换为别的频率。


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz

#下面这条语句实际上是一个被划分为多个月度时期的时间段中的游标
p = pd.Period('2007',freq = 'A-DEC')
print p
print p.asfreq('M',how = 'start')
print p.asfreq('M',how = 'end')
#高频率转换为低频率时，超时期是由子时期所属位置决定的,例如在A-JUN频率中，月份“2007年8月”实际上属于“2008年”
p = pd.Period('2007-08','M')
print p.asfreq('A-JUN'),'\n'
#PeriodIndex或TimeSeries的频率转换方式也是如此：
rng = pd.period_range('2006','2009',freq = 'A-DEC')
ts = Series(np.random.randn(len(rng)),index = rng)
print ts
print ts.asfreq('M',how = 'start')
print ts.asfreq('B',how = 'end'),'\n'
>>>
    2007
    2007-01
    2007-12
    2008
    2006    0.001601
    2007    0.285760
    2008   -0.458762
    2009    0.076204
    Freq: A-DEC
    2006-01    0.001601
    2007-01    0.285760
    2008-01   -0.458762
    2009-01    0.076204
    Freq: M
    2006-12-29    0.001601
    2007-12-31    0.285760
    2008-12-31   -0.458762
    2009-12-31    0.076204
    Freq: B
    [Finished in 1.4s]

#+END_SRC

Period频率转换示意图：

按季度计算的时期频率
季度型数据在会计、金融等领域中很常见。许多季度型数据都会涉及“财年末”的概念，通常是一年12个月中某月的最后一个日历日或工作日。就这一点来说，“2012Q4”根据财年末的会有不同含义。pandas支持12种可能的季度频率，即Q-JAN、Q-DEC。


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz

p = pd.Period('2012Q4',freq = 'Q-JAN')
print p
#在以1月结束的财年中，2012Q4是从11月到1月
print p.asfreq('D','start')
print p.asfreq('D','end'),'\n'
#因此，Period之间的运算会非常简单，例如，要获取该季度倒数第二个工作日下午4点的时间戳
p4pm = (p.asfreq('B','e') - 1).asfreq('T','s') + 16 * 60
print p4pm
print p4pm.to_timestamp(),'\n'
#period_range还可以用于生产季度型范围，季度型范围的算数运算也跟上面是一样的：
#要非常小心的是Q-JAN是什么意思
rng = pd.period_range('2011Q3','2012Q4',freq = 'Q-JAN')
print rng.to_timestamp()
ts = Series(np.arange(len(rng)),index = rng)
print ts,'\n'
new_rng = (rng.asfreq('B','e') - 1).asfreq('T','s') + 16 * 60
ts.index = new_rng.to_timestamp()
print ts,'\n'
>>>
2012Q4
2011-11-01
2012-01-31 

2012-01-30 16:00
2012-01-30 16:00:00 

<class 'pandas.tseries.index.DatetimeIndex'>
[2010-10-31 00:00:00, ..., 2012-01-31 00:00:00]
Length: 6, Freq: Q-OCT, Timezone: None
2011Q3    0
2011Q4    1
2012Q1    2
2012Q2    3
2012Q3    4
2012Q4    5
Freq: Q-JAN
2010-10-28 16:00:00    0
2011-01-28 16:00:00    1
2011-04-28 16:00:00    2
2011-07-28 16:00:00    3
2011-10-28 16:00:00    4
2012-01-30 16:00:00    5
[Finished in 3.3s]

#+END_SRC

下面是一个示意图，很直观：

将Timestamp转换为Period
通过to_period方法，可以将由时间戳索引的Series和DataFrame对象转换为以时期为索引的对象。


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz

rng = pd.date_range('1/1/2015',periods = 3,freq = 'M')
ts = Series(np.random.randn(3),index = rng)
print ts
pts = ts.to_period()
print pts,'\n'
#由于时期指的是非重叠时间区间，因此对于给定的频率，一个时间戳只能属于一个时期。
#新PeriodIndex的频率默认是从时间戳推断而来的，当然可以自己指定频率，当然会有重复时期存在
rng = pd.date_range('1/29/2000',periods = 6,freq = 'D')
ts2 = Series(np.random.randn(6),index = rng)
print ts2
print ts2.to_period('M')
#要想转换为时间戳，使用to_timestamp即可
print pts.to_timestamp(how = 'end')
>>>
2015-01-31   -1.085886
2015-02-28   -0.919741
2015-03-31    0.656477
Freq: M
2015-01   -1.085886
2015-02   -0.919741
2015-03    0.656477
Freq: M 

2000-01-29   -0.394812
2000-01-30    0.669354
2000-01-31    0.197537
2000-02-01   -1.374942
2000-02-02    0.451683
2000-02-03    1.542144
Freq: D
2000-01   -0.394812
2000-01    0.669354
2000-01    0.197537
2000-02   -1.374942
2000-02    0.451683
2000-02    1.542144
Freq: M
2015-01-31   -1.085886
2015-02-28   -0.919741
2015-03-31    0.656477
Freq: M
[Finished in 1.8s]

#+END_SRC

通过数组创建PeriodIndex
固定频率的数据集通常会将时间信息分开存放在多个列中。例如下面的这个宏观经济数据集中，年度和季度就分别存放在不同的列中。


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz

data = pd.read_csv('E:\\macrodata.csv')
print data.year
print data.quarter,'\n'
index = pd.PeriodIndex(year = data.year,quarter = data.quarter,freq = 'Q-DEC')
#index是以整数数组的形式存储的，当显示某一个是才会有年份-季度的展示
print index
print index[0],'\n'
data.index = index
#下面的结果证明，infl的index已经变为了年份-季度形式
print data.infl
>>>
0     1959
1     1959
2     1959
3     1959
4     1960
5     1960
6     1960
7     1960
8     1961
9     1961
10    1961
11    1961
12    1962
13    1962
14    1962
...
188    2006
189    2006
190    2006
191    2006
192    2007
193    2007
194    2007
195    2007
196    2008
197    2008
198    2008
199    2008
200    2009
201    2009
202    2009
Name: year, Length: 203
0     1
1     2
2     3
3     4
4     1
5     2
6     3
7     4
8     1
9     2
10    3
11    4
12    1
13    2
14    3
...
188    1
189    2
190    3
191    4
192    1
193    2
194    3
195    4
196    1
197    2
198    3
199    4
200    1
201    2
202    3
Name: quarter, Length: 203 

array([-44, -43, -42, -41, -40, -39, -38, -37, -36, -35, -34, -33, -32,
       -31, -30, -29, -28, -27, -26, -25, -24, -23, -22, -21, -20, -19,
       -18, -17, -16, -15, -14, -13, -12, -11, -10,  -9,  -8,  -7,  -6,
        -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7,
         8,   9,  10,  11,  12,  13,  14,  15,  16,  17,  18,  19,  20,
        21,  22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  32,  33,
        34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  45,  46,
        47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,
        60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,
        73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  85,
        86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,
        99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124,
       125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137,
       138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150,
       151, 152, 153, 154, 155, 156, 157, 158], dtype=int64)
1959Q1 

1959Q1    0.00
1959Q2    2.34
1959Q3    2.74
1959Q4    0.27
1960Q1    2.31
1960Q2    0.14
1960Q3    2.70
1960Q4    1.21
1961Q1   -0.40
1961Q2    1.47
1961Q3    0.80
1961Q4    0.80
1962Q1    2.26
1962Q2    0.13
1962Q3    2.11
...
2006Q1    2.60
2006Q2    3.97
2006Q3   -1.58
2006Q4    3.30
2007Q1    4.58
2007Q2    2.75
2007Q3    3.45
2007Q4    6.38
2008Q1    2.82
2008Q2    8.53
2008Q3   -3.16
2008Q4   -8.79
2009Q1    0.94
2009Q2    3.37
2009Q3    3.56
Freq: Q-DEC, Name: infl, Length: 203
[Finished in 1.8s]

#+END_SRC

6、重采样及频率转换
重采样（resampling）指的是将时间序列从一个频率转换到另一个频率的过程。将高频率数据聚合到低频率成为降采样（downsampling），而将低频率数据转换到高频率成为升采样（uosampling）。并不是所有的重采样都能被划分到这两类中，比如将W-WED转换为W-FRI既不是降采样也不是升采样。
pandas中的resample方法，它是各种频率转换工作的主力函数。


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz

rng = pd.date_range('1/1/2000',periods = 100,freq = 'D')
ts = Series(np.random.randn(100),index = rng)
#print ts
#注意下面的结果中有4个月的值，因为ts已经到了四月份
print ts.resample('M',how = 'mean')
print ts.resample('M',how = 'mean',kind = 'period')
>>>
2000-01-31    0.015620
2000-02-29    0.002502
2000-03-31   -0.029775
2000-04-30   -0.618537
Freq: M
2000-01    0.015620
2000-02    0.002502
2000-03   -0.029775
2000-04   -0.618537
Freq: M
[Finished in 0.7s]

#+END_SRC

    下面是resample的参数：


降采样
将数据的频率降低称为降采样，也就是将数据进行聚合。一个数据点只能属于一个聚合时间段，所有时间段的并集组成整个时间帧。在进行降采样时，应该考虑如下：
各区间那便是闭合的
如何标记各个聚合面元，用区间的开头还是结尾


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz

#下面生成1分钟线
rng = pd.date_range('1/1/2000',periods = 12,freq = 'T')
ts = Series(range(0,12),index = rng)
print ts,'\n'
#下面聚合到5min线
print ts.resample('5min',how = 'sum')
#传入的频率将会以“5min”的增量定义面元。默认情况下，面元的有边界是包含右边届的，即00:00到00:05是包含00:05的
#传入closed = 'left'会让左边的区间闭合
print ts.resample('5min',how = 'sum',closed = 'left')
#最终的时间序列默认是用右侧的边界标记，但是传入label = 'left'可以转换为左边标记
print ts.resample('5min',how = 'sum',closed = 'left',label = 'left'),'\n'
#最后，你可能需要对结果索引做一些位移，比如将右边界减去一秒更容易明白到底是属于哪一个区间
#通过loffset设置一个字符串或者日期偏移量即可实现此目的,书上作者没有加left是矛盾的，当然也可以调用shift来进行时间偏移
print ts.resample('5min',how = 'sum',closed = 'left',loffset = '-1s')
>>>
2000-01-01 00:00:00     0
2000-01-01 00:01:00     1
2000-01-01 00:02:00     2
2000-01-01 00:03:00     3
2000-01-01 00:04:00     4
2000-01-01 00:05:00     5
2000-01-01 00:06:00     6
2000-01-01 00:07:00     7
2000-01-01 00:08:00     8
2000-01-01 00:09:00     9
2000-01-01 00:10:00    10
2000-01-01 00:11:00    11
Freq: T 

2000-01-01 00:00:00     0
2000-01-01 00:05:00    15
2000-01-01 00:10:00    40
2000-01-01 00:15:00    11
Freq: 5T
2000-01-01 00:05:00    10
2000-01-01 00:10:00    35
2000-01-01 00:15:00    21
Freq: 5T
2000-01-01 00:00:00    10
2000-01-01 00:05:00    35
2000-01-01 00:10:00    21
Freq: 5T 

2000-01-01 00:04:59    10
2000-01-01 00:09:59    35
2000-01-01 00:14:59    21
Freq: 5T
[Finished in 0.6s]

#+END_SRC

下面是个下采样的一个直观展示：

a、OHLC重采样
金融领域中有一种无所不在的时间序列聚合方式，及计算四个面元值：open、close、hign、close。传入how = ‘ohlc’即可得到一个含有这四种聚合值的DataFrame。这个过程很高效！（顺便：真的很实用啊！）只需一次扫描即可计算出结果：


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz

rng = pd.date_range('1/1/2000',periods = 12,freq = 'T')
ts = Series(np.random.randn(12),index = rng)
print ts,'\n'
print ts.resample('5min',how = 'ohlc')
>>>
                         open      high       low     close
2000-01-01 00:00:00  1.239881  1.239881  1.239881  1.239881
2000-01-01 00:05:00  0.035189  0.371294 -1.764463 -1.764463
2000-01-01 00:10:00 -0.959353  1.441732 -0.959353  0.019104
2000-01-01 00:15:00  1.169352  1.169352  1.169352  1.169352
[Finished in 0.7s]

#+END_SRC

b、通过groupby进行重采样
另一种方法是使用pandas的groupby功能。例如，你打算根据月份或者周几进行分组，只需传入一个能够访问时间序列的索引上的这些字段的函数即可： 


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz

rng = pd.date_range('1/1/2000',periods = 100,freq = 'D')
ts = Series(np.arange(100),index = rng)
print ts.groupby(lambda x:x.month).mean()  #作真是越写越省事了……
print ts.groupby(lambda x:x.weekday).mean()
>>>
1    15
2    45
3    75
4    95
0    47.5
1    48.5
2    49.5
3    50.5
4    51.5
5    49.0
6    50.0
[Finished in 0.6s]

#+END_SRC

升采样和差值
    将数据从低频率转换到高频率是，就不需要聚合了。看一下下面的例子：


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz

frame = DataFrame(np.random.randn(2,4),index = pd.date_range('1/1/2000',periods = 2,freq = 'W-WED'),
    columns = ['Colorado','Texas','New York','Ohio'])
print frame,'\n'
#将其重采样到日频率，默认会引入缺省值
df_daily = frame.resample('D')
print df_daily,'\n'
#可以跟fillna和reindex一样，将上面的数值用resampling进行填充
print frame.resample('D',fill_method = 'ffill'),'\n'
#同样，这里可以只填充指定的时期数（目的是限制前面的观测值的持续使用距离）
print frame.resample('D',fill_method = 'ffill',limit = 2)
#注意，新的日期索引完全没必要跟旧的相交,注意这个例子展现了数据日期可以延长
print frame.resample('W-THU',fill_method = 'ffill')
>>>
            Colorado     Texas  New York      Ohio
2000-01-05  0.093695  1.382325 -0.146193  1.206698
2000-01-12 -1.873184  0.603526 -1.407574  1.452790 

            Colorado     Texas  New York      Ohio
2000-01-05  0.093695  1.382325 -0.146193  1.206698
2000-01-06       NaN       NaN       NaN       NaN
2000-01-07       NaN       NaN       NaN       NaN
2000-01-08       NaN       NaN       NaN       NaN
2000-01-09       NaN       NaN       NaN       NaN
2000-01-10       NaN       NaN       NaN       NaN
2000-01-11       NaN       NaN       NaN       NaN
2000-01-12 -1.873184  0.603526 -1.407574  1.452790 

            Colorado     Texas  New York      Ohio
2000-01-05  0.093695  1.382325 -0.146193  1.206698
2000-01-06  0.093695  1.382325 -0.146193  1.206698
2000-01-07  0.093695  1.382325 -0.146193  1.206698
2000-01-08  0.093695  1.382325 -0.146193  1.206698
2000-01-09  0.093695  1.382325 -0.146193  1.206698
2000-01-10  0.093695  1.382325 -0.146193  1.206698
2000-01-11  0.093695  1.382325 -0.146193  1.206698
2000-01-12 -1.873184  0.603526 -1.407574  1.452790 

            Colorado     Texas  New York      Ohio
2000-01-05  0.093695  1.382325 -0.146193  1.206698
2000-01-06  0.093695  1.382325 -0.146193  1.206698
2000-01-07  0.093695  1.382325 -0.146193  1.206698
2000-01-08       NaN       NaN       NaN       NaN
2000-01-09       NaN       NaN       NaN       NaN
2000-01-10       NaN       NaN       NaN       NaN
2000-01-11       NaN       NaN       NaN       NaN
2000-01-12 -1.873184  0.603526 -1.407574  1.452790
            Colorado     Texas  New York      Ohio
2000-01-06  0.093695  1.382325 -0.146193  1.206698
2000-01-13 -1.873184  0.603526 -1.407574  1.452790
[Finished in 0.7s]

#+END_SRC

通过日期进行重采样
    对那些使用时期索引的数据进行重采样是一件非常简单的事情。


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz

frame = DataFrame(np.random.randn(24,4),index = pd.period_range('1-2000','12-2001',freq = 'M'),
    columns = ['Colorado','Texas','New York','Ohio'])
print frame,'\n'
annual_frame = frame.resample('A-DEC',how = 'mean')
print annual_frame,'\n'
#升采样要稍微麻烦些，因为你必须决定在新的频率中各区间的哪端用于放置原来的值，就像asfreq方法一样，convention默认为'end',可设置为'start'
#Q-DEC：季度型（每年以12月结束）
print annual_frame.resample('Q-DEC',fill_method = 'ffill')
print annual_frame.resample('Q-DEC',fill_method = 'ffill',convention = 'start'),'\n'
#由于时期指的是时间区间，所以升采样和降采样的规则就比较严格
#在降采样中，目标频率必须是原频率的子时期
#在升采样中，目标频率必须是原频率的超时期
#如果不满足这些条件，就会引发异常，主要影响的是按季、年、周计算的频率。
#例如，由Q-MAR定义的时间区间只能升采样为A-MAR、A-JUN等
print annual_frame.resample('Q-MAR',fill_method = 'ffill')
#实话说，上面的几个例子需要在实战中去理解


>>>
         Colorado     Texas  New York      Ohio
2000-01  0.531119  0.514660 -1.051243  1.900872
2000-02  0.937613 -0.301391  1.034113 -0.015524
2000-03  0.368118 -1.236412  0.455100  1.648863
2000-04 -0.728873  0.250044  1.523354  0.230613
2000-05 -0.188811  1.418581 -1.285510  1.051915
2000-06  2.059990 -0.703682  1.293203 -0.792534
2000-07  0.911168 -0.362981 -1.873637  1.033383
2000-08  0.817223  1.512153 -0.365323 -1.325069
2000-09 -0.087511  0.238656 -2.078260  1.415511
2000-10  0.185765  0.223584  1.242821 -0.654831
2000-11 -0.725814  0.723152 -0.250924 -2.110532
2000-12 -0.153382  1.535816  1.455040  0.700309
2001-01 -0.146100 -1.036274 -0.954112 -0.212434
2001-02  0.283262  1.868316  2.128798 -0.857980
2001-03 -0.793054 -1.858595 -1.243900  0.952001
2001-04  0.878166 -0.846098  1.161008  1.060023
2001-05  0.071310 -0.705115  0.489365  0.187680
2001-06 -0.622563 -1.070024 -1.044217  0.119744
2001-07  1.086923 -1.142216  1.015157  0.804685
2001-08 -2.642336 -0.758853 -0.248052 -0.024919
2001-09 -0.335489 -1.354160  0.171963 -0.993819
2001-10 -0.715587 -0.833531  0.797166  0.127754
2001-11 -0.265285 -2.005336  1.271591  0.016298
2001-12  0.971353 -0.150070 -1.170043  1.067736 

      Colorado     Texas  New York      Ohio
2000  0.327217  0.317682  0.008228  0.256915
2001 -0.185783 -0.824330  0.197894  0.187231 

        Colorado     Texas  New York      Ohio
2000Q4  0.327217  0.317682  0.008228  0.256915
2001Q1  0.327217  0.317682  0.008228  0.256915
2001Q2  0.327217  0.317682  0.008228  0.256915
2001Q3  0.327217  0.317682  0.008228  0.256915
2001Q4 -0.185783 -0.824330  0.197894  0.187231
        Colorado     Texas  New York      Ohio
2000Q1  0.327217  0.317682  0.008228  0.256915
2000Q2  0.327217  0.317682  0.008228  0.256915
2000Q3  0.327217  0.317682  0.008228  0.256915
2000Q4  0.327217  0.317682  0.008228  0.256915
2001Q1 -0.185783 -0.824330  0.197894  0.187231 

        Colorado     Texas  New York      Ohio
2001Q3  0.327217  0.317682  0.008228  0.256915
2001Q4  0.327217  0.317682  0.008228  0.256915
2002Q1  0.327217  0.317682  0.008228  0.256915
2002Q2  0.327217  0.317682  0.008228  0.256915
2002Q3 -0.185783 -0.824330  0.197894  0.187231
[Finished in 0.8s]

#+END_SRC
*   第十章 时间序列（三）

7、时间序列绘图
    pandas时间序列的绘图功能在日期格式化方面比matplotlib原生的要好。


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz
#下面两个参数，一个是解析日期形式，一个是将第一列作为行名
close_px_all = pd.read_csv('E:\\stock_px.csv',parse_dates = True,index_col = 0)
print close_px_all.head(),'\n'
close_px = close_px_all[['AAPL','MSFT','XOM']]
close_px = close_px.resample('B',fill_method = 'ffill')
print close_px.head()
#注意下面的索引方式即可
close_px['AAPL'].plot()
close_px.ix['2009'].plot()
close_px['AAPL'].ix['01-2011':'03-2011'].plot()
#季度型频率的数据会用季度标记进行格式化，这种事情手工的话会很费力……（真是有道理……）
appl_q = close_px['AAPL'].resample('Q-DEC',fill_method = 'ffill')
appl_q.ix['2009':].plot()
#作者说交互方式右键按住日期会动态展开或收缩，实际自己做，没效果……
plt.show()
>>>
              AA  AAPL    GE    IBM   JNJ  MSFT   PEP     SPX   XOM
1990-02-01  4.98  7.86  2.87  16.79  4.27  0.51  6.04  328.79  6.12
1990-02-02  5.04  8.00  2.87  16.89  4.37  0.51  6.09  330.92  6.24
1990-02-05  5.07  8.18  2.87  17.32  4.34  0.51  6.05  331.85  6.25
1990-02-06  5.01  8.12  2.88  17.56  4.32  0.51  6.15  329.66  6.23
1990-02-07  5.04  7.77  2.91  17.93  4.38  0.51  6.17  333.75  6.33 

            AAPL  MSFT   XOM
1990-02-01  7.86  0.51  6.12
1990-02-02  8.00  0.51  6.24
1990-02-05  8.18  0.51  6.25
1990-02-06  8.12  0.51  6.23
1990-02-07  7.77  0.51  6.33
[Finished in 37.5s]

#+END_SRC

    下面是作出的几张图：




    8、移动窗口函数
    在移动窗口（可以带有指数衰减权数）上计算的各种统计函数也是一类常见于时间序列的数组变换。作者将其称为移动窗口函数（moving window function），其中还包括那些窗口不定长的函数（如指数加权移动平均）。跟其他统计函数一样，移动窗口函数也会自动排除缺失值。这样的函数通常需要指定一些数量的非NA观测值。


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz

#rolling_mean是其中最简单的一个。它接受一个TimeSeries或DataFrame以及一个window（表示期数）
close_px_all = pd.read_csv('E:\\stock_px.csv',parse_dates = True,index_col = 0)
print close_px_all.head(),'\n'
close_px = close_px_all[['AAPL','MSFT','XOM']]
close_px = close_px.resample('B',fill_method = 'ffill')
close_px.AAPL.plot()
pd.rolling_mean(close_px.AAPL,250).plot()
plt.show()
#默认情况下，诸如rolling_mean这样的涵涵素需要指定数量的非NA观测值。可以修改该行为以解决缺失数据的问题，其实，
#在时间序列开始处尚不足窗口期的那些数据就是个特例（也就是前250期均线值是没有的）
#看一下下面的图
#有个参数是min_periods，文档中说的是窗口中应该有值的最小的序列标号，可是如果是250期的标准差值，250之前怎么会有数呢？。。。难道是自动转换了周期？
#YES!确实是这样，min_periods是指自这个标号开始，计算前面所有数的std，比如min_periods = 10时，计算前10个数的，min_periods = 20时，计算前20个数的，知道min_periods = 250为止，这就是所谓的“指定的非NA观测值”
close_px.AAPL.plot()
appl_std250 = pd.rolling_std(close_px.AAPL,250,min_periods = 10)
print appl_std250[:15]
appl_std250.plot()
plt.show()
>>>
              AA  AAPL    GE    IBM   JNJ  MSFT   PEP     SPX   XOM
1990-02-01  4.98  7.86  2.87  16.79  4.27  0.51  6.04  328.79  6.12
1990-02-02  5.04  8.00  2.87  16.89  4.37  0.51  6.09  330.92  6.24
1990-02-05  5.07  8.18  2.87  17.32  4.34  0.51  6.05  331.85  6.25
1990-02-06  5.01  8.12  2.88  17.56  4.32  0.51  6.15  329.66  6.23
1990-02-07  5.04  7.77  2.91  17.93  4.38  0.51  6.17  333.75  6.33 

1990-02-01         NaN
1990-02-02         NaN
1990-02-05         NaN
1990-02-06         NaN
1990-02-07         NaN
1990-02-08         NaN
1990-02-09         NaN
1990-02-12         NaN
1990-02-13         NaN
1990-02-14    0.148189
1990-02-15    0.141003
1990-02-16    0.135454
1990-02-19    0.130502
1990-02-20    0.128690
1990-02-21    0.124108
Freq: B
[Finished in 4.6s]



#+END_SRC

要计算扩展窗口平均（expanding window mean），可以将扩展窗口看作一个特殊的窗口，其长度与时间序列一样，但只需一期或多期即可计算一个值。


#+BEGIN_SRC python
#通过rolling_mean定义扩展平均
expanding_mean = lambda x:rolling_mean(x,len(x),min_periods = 1)
#对DataFrame调用rolling_mean（以及其他类似函数）会将转换应用到所有列上
#下面的logy是将纵坐标显示为科学计数法，暂时搞不懂怎么变换的
mean_60 = pd.rolling_mean(close_px,60).plot()
mean_60 = pd.rolling_mean(close_px,60).plot(logy = True)


#print mean_60[(len(mean_60) - 20):len(mean_60)]
plt.show()
ts = pd.Series(range(10), index=pd.date_range('1/1/2000', periods=10))
#ts = np.exp(ts.cumsum())
print ts
print np.log(ts)
ts.plot(logy=True)
plt.show()



#+END_SRC

指数加权函数
另一种使用固定大小窗口及相等权数观测值的方法是，定义一个衰减因子（decay factor）常量，以便使近期的观测值拥有更大的权数。衰减因子的定义方式有很多，比较流行的是使用时间间隔（span），它可以使结果兼容于窗口大小等于时间间隔的简单移动窗口函数。


#+BEGIN_SRC python
fig,axes = plt.subplots(nrows = 2,ncols = 1,sharex = True,sharey = True,figsize = (12,7))
aapl_px = close_px.AAPL['2005':'2009']
ma60 = pd.rolling_mean(aapl_px,60,min_periods = 50)
ewma60 = pd.ewma(aapl_px,span = 60)

aapl_px.plot(style = 'k-',ax = axes[0])
ma60.plot(style = 'k--',ax = axes[0])
aapl_px.plot(style = 'k-',ax = axes[1])
ewma60.plot(style = 'k--',ax = axes[1])
axes[0].set_title('Simple MA')
axes[1].set_title('Exponentially-weighted MA')
plt.show()


#+END_SRC

二元移动窗口函数
有些统计运算（如相关系数和协方差）需要在两个时间序列上执行。比如，金融分析师常常对某只股票对某个参数（如标普500指数）的相关系数感兴趣。我们可以通过计算百分比变化并使用rolling_corr的方式得到该结果。 


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz


#rolling_mean是其中最简单的一个。它接受一个TimeSeries或DataFrame以及一个window（表示期数） 
close_px_all = pd.read_csv('E:\\stock_px.csv',parse_dates = True,index_col = 0)
print close_px_all.head(),'\n'
close_px = close_px_all[['AAPL','MSFT','XOM']]

spx_px = close_px_all['SPX']
print spx_px
#下面是将spx_px数据后移一位，减1是将数据减1，当然后面的是先除，再减1
#print spx_px.shift(1) - 1
spx_rets = spx_px / spx_px.shift(1) - 1
#看一下，下面的函数是跟上面的一样，作者是为了展示函数才这么写的
#spx_rets_pct_change = spx_px.pct_change()
#print spx_rets_pct_change[:10]
print spx_rets[:10],'\n'
returns = close_px.pct_change()
print returns[:10]
corr = pd.rolling_corr(returns.AAPL,spx_rets,125,min_periods = 100)
corr.plot()
plt.show()
>>>
              AA  AAPL    GE    IBM   JNJ  MSFT   PEP     SPX   XOM
1990-02-01  4.98  7.86  2.87  16.79  4.27  0.51  6.04  328.79  6.12
1990-02-02  5.04  8.00  2.87  16.89  4.37  0.51  6.09  330.92  6.24
1990-02-05  5.07  8.18  2.87  17.32  4.34  0.51  6.05  331.85  6.25
1990-02-06  5.01  8.12  2.88  17.56  4.32  0.51  6.15  329.66  6.23
1990-02-07  5.04  7.77  2.91  17.93  4.38  0.51  6.17  333.75  6.33 

1990-02-01    328.79
1990-02-02    330.92
1990-02-05    331.85
1990-02-06    329.66
1990-02-07    333.75
1990-02-08    332.96
1990-02-09    333.62
1990-02-12    330.08
1990-02-13    331.02
1990-02-14    332.01
1990-02-15    334.89
1990-02-16    332.72
1990-02-20    327.99
1990-02-21    327.67
1990-02-22    325.70
...
2011-09-26    1162.95
2011-09-27    1175.38
2011-09-28    1151.06
2011-09-29    1160.40
2011-09-30    1131.42
2011-10-03    1099.23
2011-10-04    1123.95
2011-10-05    1144.03
2011-10-06    1164.97
2011-10-07    1155.46
2011-10-10    1194.89
2011-10-11    1195.54
2011-10-12    1207.25
2011-10-13    1203.66
2011-10-14    1224.58
Name: SPX, Length: 5472
1990-02-01         NaN
1990-02-02    0.006478
1990-02-05    0.002810
1990-02-06   -0.006599
1990-02-07    0.012407
1990-02-08   -0.002367
1990-02-09    0.001982
1990-02-12   -0.010611
1990-02-13    0.002848
1990-02-14    0.002991
Name: SPX 

                AAPL      MSFT       XOM
1990-02-01       NaN       NaN       NaN
1990-02-02  0.017812  0.000000  0.019608
1990-02-05  0.022500  0.000000  0.001603
1990-02-06 -0.007335  0.000000 -0.003200
1990-02-07 -0.043103  0.000000  0.016051
1990-02-08 -0.007722  0.000000  0.003160
1990-02-09  0.037613  0.019608  0.003150
1990-02-12 -0.007500  0.000000 -0.023548
1990-02-13  0.015113  0.000000  0.001608
1990-02-14 -0.007444  0.000000 -0.004815
[Finished in 50.8s]

 
#+END_SRC


假如现在想同时计算多只股票与标普的相关系数。只需传入一个TimeSeries和一个DataFrame，rolling_corr就会自动计算TimeSeries与DataFrame各列的相关系数。


#+BEGIN_SRC python
corr = pd.rolling_corr(returns,spx_rets,125,min_periods = 100)
corr.plot()
plt.show()

 
#+END_SRC

用户自定义的移动窗口函数
rolling_apply函数使你能够在移动窗口上应用自己设计的数组函数。唯一的要求就是：该函数要能从数组的各个片段中产生单个值。比如，当用rolling_quantile计算样本分位数时，可能对样本中特定值的百分等级感兴趣。


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz
from scipy.stats import percentileofscore


#rolling_mean是其中最简单的一个。它接受一个TimeSeries或DataFrame以及一个window（表示期数） 
close_px_all = pd.read_csv('E:\\stock_px.csv',parse_dates = True,index_col = 0)
close_px = close_px_all[['AAPL','MSFT','XOM']]
returns = close_px.pct_change()
#这里的percentileofscore是指，0.02在x中的位置是x中的百分比
#AAPL %2回报率的百分等级
score_at_2percent = lambda x:percentileofscore(x,0.02)
result = pd.rolling_apply(returns.AAPL,250,score_at_2percent)
result.plot()
plt.show()

#+END_SRC

 
9、性能和内存使用方面的注意事项
TimeSeries和Period都是以64位整数表示的（即NumPy的datetime64数据类型）。也就是说，对于每个数据点，其时间戳需要占用8字节内存。因此，含有一百万个float64数据点的时间序列需要占用大约16MB的内存空间。由于pandas会尽量在多个时间序列之间共享索引，所以创建现有时间序列的视图不会占用更多内存。此外，低频率索引（日以上）会被存放在一个中心缓存中，所以任何固定频率的索引都是该日期缓存的视图。所以。如果你有一个很大的低频率时间序列，索引所占用的内存空间将不会很大。
性能方面，pandas对数据对齐（两个不同索引的ts1 + ts2的幕后工作）和重采样运算进行了高度优化。下面这个例子将一亿个数据点聚合为OHLC：


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time
from pandas.tseries.offsets import Hour,Minute,Day,MonthEnd
import pytz


rng = pd.date_range('1/1/2000',periods = 10000000,freq = '10ms')
ts = Series(np.random.randn(len(rng)),index = rng)
print ts,'\n'
print ts.resample('15min',how = 'ohlc'),'\n'
#下面测试一下代码运行时间,下面运行不成功
#%timeit ts.resample('15min',how = 'ohlc')
#换句话说，聚合的频率越高，耗费时间越多，但是，但是仍然是非常高效的
>>>
2000-01-01 00:00:00          -0.681229
2000-01-01 00:00:00.010000   -1.231560
2000-01-01 00:00:00.020000    0.437656
2000-01-01 00:00:00.030000    2.134065
2000-01-01 00:00:00.040000    0.264029
2000-01-01 00:00:00.050000   -2.273143
2000-01-01 00:00:00.060000    1.519468
2000-01-01 00:00:00.070000   -0.052764
2000-01-01 00:00:00.080000    1.329301
2000-01-01 00:00:00.090000   -1.078996
2000-01-01 00:00:00.100000   -1.121855
2000-01-01 00:00:00.110000   -0.157845
2000-01-01 00:00:00.120000    0.453539
2000-01-01 00:00:00.130000    0.043068
2000-01-01 00:00:00.140000    0.378264
...
2000-01-02 03:46:39.850000   -0.444970
2000-01-02 03:46:39.860000    0.296446
2000-01-02 03:46:39.870000   -1.051884
2000-01-02 03:46:39.880000    0.612868
2000-01-02 03:46:39.890000    0.682818
2000-01-02 03:46:39.900000    0.375605
2000-01-02 03:46:39.910000   -0.843553
2000-01-02 03:46:39.920000   -0.861029
2000-01-02 03:46:39.930000    0.349835
2000-01-02 03:46:39.940000    0.231722
2000-01-02 03:46:39.950000   -0.268164
2000-01-02 03:46:39.960000   -1.537572
2000-01-02 03:46:39.970000   -0.634842
2000-01-02 03:46:39.980000   -1.110032
2000-01-02 03:46:39.990000    0.071214
Freq: 10L, Length: 10000000 

                         open      high       low     close
2000-01-01 00:00:00 -0.681229 -0.681229 -0.681229 -0.681229
2000-01-01 00:15:00 -1.231560  4.113992 -4.589095 -0.241367
2000-01-01 00:30:00  1.171302  4.593611 -4.329438 -0.099641
2000-01-01 00:45:00 -0.720612  4.432697 -4.658295 -2.278497
2000-01-01 01:00:00  0.119403  4.259349 -4.922511  1.899723
2000-01-01 01:15:00  1.168395  4.351551 -4.087221 -0.124419
2000-01-01 01:30:00  1.888486  4.288424 -4.540685  0.337621
2000-01-01 01:45:00  0.263643  4.412893 -4.362212 -1.125978
2000-01-01 02:00:00  1.398256  4.301166 -4.140143  0.693118
2000-01-01 02:15:00 -0.307263  4.353092 -4.417690 -1.647730
2000-01-01 02:30:00  1.028139  4.727692 -4.089063  0.242530
2000-01-01 02:45:00  0.857454  3.946653 -4.745711  0.270212
2000-01-01 03:00:00 -0.925215  4.544331 -4.261408 -0.616690
2000-01-01 03:15:00 -0.008779  3.958481 -4.016185 -1.055645
2000-01-01 03:30:00  0.649988  4.939031 -4.446418  0.118234
2000-01-01 03:45:00 -0.533717  4.685563 -4.205492  0.731999
2000-01-01 04:00:00  0.511450  4.483055 -3.945226 -0.814555
2000-01-01 04:15:00  0.372549  4.449327 -4.087508  0.786998
2000-01-01 04:30:00 -1.015505  4.750429 -4.111374  0.955857
2000-01-01 04:45:00 -0.450577  4.155395 -4.628542  0.621572
2000-01-01 05:00:00  0.629534  4.144105 -4.302083  1.567992
2000-01-01 05:15:00  0.843481  4.092661 -4.509020 -0.997818
2000-01-01 05:30:00  1.026566  4.004000 -4.330091 -0.745961
2000-01-01 05:45:00  0.523910  4.286510 -4.147153 -0.334644
2000-01-01 06:00:00  1.481702  4.437908 -4.198872  0.309824
2000-01-01 06:15:00 -0.530256  4.551381 -4.218254  0.112050
2000-01-01 06:30:00 -1.224188  4.245407 -4.198838  0.973066
2000-01-01 06:45:00  0.114000  4.286166 -4.070633 -1.024489
2000-01-01 07:00:00 -2.148906  4.198777 -4.213584  2.137635
2000-01-01 07:15:00  2.716069  4.308833 -4.432955  0.196065
2000-01-01 07:30:00 -0.902512  4.315467 -4.376366 -1.944492
2000-01-01 07:45:00  0.978385  4.482707 -4.343861 -0.161608
2000-01-01 08:00:00  0.028728  4.334193 -4.995541 -1.409060
2000-01-01 08:15:00  0.254613  3.944059 -4.263927  1.022247
2000-01-01 08:30:00 -2.153415  4.282622 -4.681402  0.133295
2000-01-01 08:45:00  0.361382  4.332683 -4.124674 -1.810247
2000-01-01 09:00:00  0.218621  4.087920 -4.878364 -0.247444
2000-01-01 09:15:00  1.541770  4.709500 -4.100887  0.263939
2000-01-01 09:30:00  0.302456  4.072987 -4.402301 -0.695389
2000-01-01 09:45:00  0.758779  4.854449 -4.292967 -0.098260
2000-01-01 10:00:00 -1.033195  4.412930 -4.319737 -1.078443
2000-01-01 10:15:00 -0.702287  4.687409 -4.242148  0.108918
2000-01-01 10:30:00  2.040476  4.167678 -4.069875 -0.271023
2000-01-01 10:45:00 -1.719918  4.414900 -4.003430  0.178522
2000-01-01 11:00:00 -2.003960  4.681189 -4.407995 -1.532938
2000-01-01 11:15:00  2.071234  4.691175 -4.203442 -0.000271
2000-01-01 11:30:00 -0.335169  4.577745 -4.383428 -0.356682
2000-01-01 11:45:00  0.837294  4.158462 -4.667864 -1.214194
2000-01-01 12:00:00 -0.593185  4.491041 -4.229999 -0.906558
2000-01-01 12:15:00 -0.757815  4.283729 -4.824929  0.461968
2000-01-01 12:30:00 -0.627753  4.465840 -4.382329  1.758057
2000-01-01 12:45:00 -0.582081  4.248387 -5.043421 -1.665271
2000-01-01 13:00:00 -0.232743  4.151332 -4.197010 -1.040030
2000-01-01 13:15:00 -0.099233  4.065889 -4.025087  0.400879
2000-01-01 13:30:00  0.560333  4.441687 -4.372460 -1.212408
2000-01-01 13:45:00  0.442710  4.105972 -4.284578 -0.756200
2000-01-01 14:00:00  1.280060  4.613177 -4.435858  0.793312
2000-01-01 14:15:00  0.849877  4.445931 -4.143685 -1.522613
2000-01-01 14:30:00  1.084148  4.750917 -4.196053  0.154898
2000-01-01 14:45:00  1.055437  4.320318 -4.673456  1.022639
2000-01-01 15:00:00  0.708564  4.573142 -4.251478 -0.420195
2000-01-01 15:15:00 -2.163962  4.332879 -4.207693  0.909637
2000-01-01 15:30:00  0.316790  4.269409 -4.110165  0.698051
2000-01-01 15:45:00 -0.811775  4.356382 -4.576847  1.465054
2000-01-01 16:00:00 -0.000181  4.101318 -4.549553 -0.161170
2000-01-01 16:15:00  0.293171  4.565994 -4.279151  0.574916
2000-01-01 16:30:00  1.284430  4.438795 -4.384199 -0.357597
2000-01-01 16:45:00  0.922512  4.270791 -4.365019 -0.089139
2000-01-01 17:00:00 -1.434599  4.216443 -4.599743 -0.993626
2000-01-01 17:15:00 -2.289424  4.447081 -4.129147 -0.770931
2000-01-01 17:30:00  0.235515  4.122913 -3.901979  1.107505
2000-01-01 17:45:00  0.121232  4.316179 -4.294560 -0.325761
2000-01-01 18:00:00  1.406108  4.909856 -4.380683 -1.371316
2000-01-01 18:15:00 -0.330192  4.092084 -4.433832  0.451967
2000-01-01 18:30:00  0.069717  4.602332 -4.814984  1.041939
2000-01-01 18:45:00 -2.441102  4.077937 -4.477974 -0.284751
2000-01-01 19:00:00  1.117306  4.669111 -4.433551  1.887700
2000-01-01 19:15:00  0.482482  4.545320 -4.231923  2.098973
2000-01-01 19:30:00  0.146878  4.230201 -4.738262  0.260756
2000-01-01 19:45:00  0.491376  5.230373 -5.069700 -0.936606
2000-01-01 20:00:00 -1.075473  4.701905 -4.245575  2.898905
2000-01-01 20:15:00  1.728790  4.291821 -4.145234 -0.735600
2000-01-01 20:30:00  0.680025  4.509368 -4.176570  0.346777
2000-01-01 20:45:00 -0.603546  4.479395 -4.033444  1.901963
2000-01-01 21:00:00 -0.893833  4.472098 -4.658866  0.026791
2000-01-01 21:15:00 -0.571074  4.066533 -4.773198  0.719510
2000-01-01 21:30:00 -1.109575  4.377526 -4.154108 -0.419939
2000-01-01 21:45:00 -1.109197  4.244968 -4.476610  0.625287
2000-01-01 22:00:00 -0.500703  4.204465 -4.695903 -0.205293
2000-01-01 22:15:00 -0.474312  4.278451 -4.261542 -0.605803
2000-01-01 22:30:00 -0.929173  4.679216 -4.243371 -0.389516
2000-01-01 22:45:00  0.625107  4.588921 -3.944369  0.051261
2000-01-01 23:00:00  0.223470  4.300131 -4.556017  0.411957
2000-01-01 23:15:00  2.834194  4.669853 -4.894633 -0.172413
2000-01-01 23:30:00  0.271214  4.468473 -4.059279 -0.144921
2000-01-01 23:45:00  1.005364  4.311476 -4.373045 -0.532617
2000-01-02 00:00:00 -0.177777  4.288976 -4.784412  1.279124
2000-01-02 00:15:00  1.767240  4.268321 -4.964638  0.978593
2000-01-02 00:30:00  0.874845  4.114844 -4.735220  0.755658
2000-01-02 00:45:00  0.139810  4.480646 -4.530709  1.861165
2000-01-02 01:00:00 -1.633137  4.237701 -4.465151  1.502397
2000-01-02 01:15:00  0.497876  4.056503 -4.348021 -0.019043
2000-01-02 01:30:00  0.183521  4.369899 -4.264499  0.725734
2000-01-02 01:45:00 -0.365043  4.257799 -4.003001 -0.197835
2000-01-02 02:00:00  1.389697  4.463931 -4.166211  1.310472
2000-01-02 02:15:00 -0.829049  4.360859 -5.347301 -0.719968
2000-01-02 02:30:00 -0.257339  4.156498 -4.481656  0.804225
2000-01-02 02:45:00 -0.112207  4.238031 -4.277917 -1.851001
2000-01-02 03:00:00  1.024404  4.315122 -4.296867  1.567366
2000-01-02 03:15:00  1.506557  4.440672 -4.429984 -1.569164
2000-01-02 03:30:00  0.292707  4.088439 -3.877321 -0.169247
2000-01-02 03:45:00 -1.838429  4.056206 -4.687052  0.679375
2000-01-02 04:00:00  0.469589  3.651325 -3.386148  0.071214 

[Finished in 2.6s]

#+END_SRC
*  第十一章 金融和经济数据应用（一）

自2005年开始，python在金融行业中的应用越来越多，这主要得益于越来越成熟的函数库
（NumPy和pandas）以及大量经验丰富的程序员。许多机构发现python不仅非常适合成为交互式的
分析环境，也非常适合开发文件的系统，所需的时间也比Java或C++少得多。Python还是一种非常
好的粘合层，可以非常轻松为C或C++编写的库构建Python接口。

金融分析领域的内容博大精深。在数据规整化方面所花费的精力常常会比解决核心建模和研究问题
所花费的时间多得多。
在本章中，术语截面（cross-section）来表示某个时间点的数据。例如标普500指数中所有成份
股在特定日期的收盘价就形成了一个截面。多个数据在多个时间点的截面数据就构成了一个面板（
panel）。面板数据既可以表示为层次化索引的DataFrame，也可以表示为三维的Panel pandas对象。
1、数据规整化方面的话题 
时间序列以及截面对齐
处理金融数据时，最费神的一个问题就是所谓的数据对齐（data alignment）。两个时间序列的索引可能没有很好的对齐，或者两个DataFrame对象可能含有不匹
配的行或者列。MATLAB、R用户通常会耗费大量的时间来进行数据对对齐工作（确实如此）。
pandas可以在运算中自动对齐数据。这是极好的，会提高效率。
时间序列以及截面对齐


#+BEGIN_SRC python

  #-*- coding:utf-8 -*-
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  import datetime as dt
  from pandas import Series,DataFrame
  from datetime import datetime
  from dateutil.parser import parse
  import time


  prices = pd.read_csv('E:\\stock_px.csv',parse_dates = True,index_col = 0)
  volume = pd.read_csv('E:\\volume.csv',parse_dates = True,index_col = 0)
  prices = prices.ix['2011-09-06':'2011-09-14',['AAPL','JNJ','SPX','XOM']]
  volume = volume.ix['2011-09-06':'2011-09-12',['AAPL','JNJ','XOM']]
  print prices
  print volume,'\n'
  #如果想计算一个基于成交量的加权平均价，只需要做下面的事即可
  vwap = (prices * volume).sum() / volume.sum()  #sum函数自动忽略NaN值
  print vwap,'\n'
  print vwap.dropna(),'\n'
  #可以使用DataFrame的align方法将DataFrame显示地对齐
  print prices.align(volume,join = 'inner')
  #另一个不可或缺的功能是，通过一组索引可能不同的Series构建DataFrame
  s1 = Series(range(3),index = ['a','b','c'])
  s2 = Series(range(4),index = ['d','b','c','e'])
  s3 = Series(range(3),index = ['f','a','c'])
  data = DataFrame({'one':s1,'two':s2,'three':s3})
  print data
  >>>
                AAPL    JNJ      SPX    XOM
  2011-09-06  379.74  64.64  1165.24  71.15
  2011-09-07  383.93  65.43  1198.62  73.65
  2011-09-08  384.14  64.95  1185.90  72.82
  2011-09-06  18173500  15848300  25416300
  2011-09-07  12492000  10759700  23108400
  2011-09-08  14839800  15551500  22434800
  2011-09-09  20171900  17008200  27969100
  2011-09-12  16697300  13448200  26205800 

  AAPL    380.655181
  JNJ      64.394769
  SPX            NaN
  XOM      72.024288 

  AAPL    380.655181
  JNJ      64.394769
  XOM      72.024288 

  (              AAPL    JNJ    XOM
  2011-09-06  379.74  64.64  71.15
  2011-09-07  383.93  65.43  73.65
  2011-09-08  384.14  64.95  72.82
  2011-09-09  377.48  63.64  71.01
  2011-09-12  379.94  63.59  71.84,                 AAPL       JNJ       XOM
  2011-09-06  18173500  15848300  25416300
  2011-09-07  12492000  10759700  23108400
  2011-09-08  14839800  15551500  22434800
  2011-09-09  20171900  17008200  27969100
  2011-09-12  16697300  13448200  26205800)
     one  three  two
  a    0      1  NaN
  b    1    NaN    1
  c    2      2    2
  d  NaN    NaN    0
  e  NaN    NaN    3
  f  NaN      0  NaN
  [Finished in 2.8s]

#+END_SRC
频率不同的时间按序列的运算
经济学时间序列常常按年月日等频率进行数据统计。


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time


ts1 = Series(np.random.randn(3),index = pd.date_range('2012-6-13',periods = 3,freq = 'W-WED'))
print ts1
#如果重采样到工作日，就会有缺省值出现
print ts1.resample('B')
print ts1.resample('B',fill_method = 'ffill'),'\n'
#下面看一种不规则时间的序列
dates = pd.DatetimeIndex(['2012-6-12','2012-6-17','2012-6-18','2012-6-21','2012-6-22','2012-6-29'])
ts2 = Series(np.random.randn(6),index = dates)
print ts2,'\n'
#如果想将处理过后的ts1加到ts2上，可以先将两个频率弄相同再相加，但是要想维持ts2的reindex，则用reindex就好
print ts1.reindex(ts2.index,method = 'ffill'),'\n'
print ts2 + ts1.reindex(ts2.index,method = 'ffill'),'\n'
>>>
2012-06-13   -0.855102
2012-06-20   -1.242206
2012-06-27    0.380710
Freq: W-WED
2012-06-13   -0.855102
2012-06-14         NaN
2012-06-15         NaN
2012-06-18         NaN
2012-06-19         NaN
2012-06-20   -1.242206
2012-06-21         NaN
2012-06-22         NaN
2012-06-25         NaN
2012-06-26         NaN
2012-06-27    0.380710
Freq: B
2012-06-13   -0.855102
2012-06-14   -0.855102
2012-06-15   -0.855102
2012-06-18   -0.855102
2012-06-19   -0.855102
2012-06-20   -1.242206
2012-06-21   -1.242206
2012-06-22   -1.242206
2012-06-25   -1.242206
2012-06-26   -1.242206
2012-06-27    0.380710
Freq: B 

2012-06-12   -1.248346
2012-06-17    0.833907
2012-06-18    0.235492
2012-06-21   -1.172378
2012-06-22   -0.111804
2012-06-29   -0.458527 

2012-06-12         NaN
2012-06-17   -0.855102
2012-06-18   -0.855102
2012-06-21   -1.242206
2012-06-22   -1.242206
2012-06-29    0.380710
2012-06-12         NaN
2012-06-17   -0.021195
2012-06-18   -0.619610
2012-06-21   -2.414584
2012-06-22   -1.354010
2012-06-29   -0.077817 

[Finished in 1.7s]

#+END_SRC

使用Period
    Period是一种好工具，尤其适合于处理特殊规范的以年或者季度为频率的金融或经济序列。比如，一个公司可能会发布其以6月结尾的财年的每季度盈利报告，即频率为Q-JUN。来看两个例子：


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
import time


gdp = Series([1.78,1.94,2.08,2.01,2.15,2.31,2.46],index = pd.period_range('1984Q2',periods = 7,freq = 'Q-SEP'))
print gdp,'\n'
infl = Series([0.025,0.045,0.037,0.04],index = pd.period_range('1982',periods = 4,freq = 'A-DEC'))
print infl,'\n'
#跟Timestamp时间序列不同的是，由period索引的不同时间序列之间的转换必须经过显示转换
#转换为以九月份为一年结束，以季度为频率的序列，end就是说：这一年里面最后一个季度的名字
infl_q = infl.asfreq('Q-SEP',how = 'end')
print infl.asfreq('Q-SEP',how = 'start'),'\n' #看一下以start开头
print infl_q,'\n'
#显示转换为以后就可以被重新索引了
print infl_q.reindex(gdp.index,method = 'ffill')
>>>
1984Q2    1.78
1984Q3    1.94
1984Q4    2.08
1985Q1    2.01
1985Q2    2.15
1985Q3    2.31
1985Q4    2.46
Freq: Q-SEP 

1982    0.025
1983    0.045
1984    0.037
1985    0.040
Freq: A-DEC 

1982Q2    0.025
1983Q2    0.045
1984Q2    0.037
1985Q2    0.040
Freq: Q-SEP 

1983Q1    0.025
1984Q1    0.045
1985Q1    0.037
1986Q1    0.040
Freq: Q-SEP 

1984Q2    0.045
1984Q3    0.045
1984Q4    0.045
1985Q1    0.037
1985Q2    0.037
1985Q3    0.037
1985Q4    0.037
Freq: Q-SEP
[Finished in 1.4s]

#+END_SRC
时间和“最当前”数据选取


#+BEGIN_SRC python
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from pandas import Series,DataFrame
from datetime import datetime
from dateutil.parser import parse
from datetime import time


#假设有一个很长的盘中数据，现在希望抽取其中的一些，如果数据不规整该怎么办？
rng = pd.date_range('2012-06-01 09:30','2012-06-01 15:59',freq = 'T')
print type(rng)  #这种类型可以append
#注意下面的组做法，通过时间的偏移得到更多数据
rng = rng.append([rng + pd.offsets.BDay(i) for i in range(1,4)])
print rng,'\n'
ts = Series(np.arange(len(rng),dtype = float),index = rng)
print ts,'\n'  
print time(10,0)  #这就是10点
print ts[time(10,0)],'\n'   #只取10点钟的数据
#该操作实际上用了实例方法at_time（各时间序列以及类似的DataFrame对象都有）
print ts.at_time(time(10,0)),'\n'
#当然还会有between_time来选取两个Time对象之间的值
print ts.between_time(time(10,0),time(10,1)),'\n'
#可是可能刚好就没有任何数据落在某个具体的时间上（比如上午10点）。这时，可能会希望得到上午10点之前最后出现的值
#下面将该时间序列的大部分内容随机设置为NA
indexer = np.sort(np.random.permutation(len(ts))[700:])
irr_ts = ts.copy()
irr_ts[indexer] = np.nan
print irr_ts['2012-06-01 09:50':'2012-06-01 10:00'],'\n'
#如果将一组Timestamp传入asof方法，就能得到这些时间点处（或其之前最近）的有效值（非NA）。例如，构造一个日期范围（每天上午10点），然后将其传入asof：
selection = pd.date_range('2012-06-01 10:00',periods = 4,freq = 'B')
print irr_ts.asof(selection)
>>>
<class 'pandas.tseries.index.DatetimeIndex'>
<class 'pandas.tseries.index.DatetimeIndex'>
[2012-06-01 09:30:00, ..., 2012-06-06 15:59:00]
Length: 1560, Freq: None, Timezone: None 

2012-06-01 09:30:00     0
2012-06-01 09:31:00     1
2012-06-01 09:32:00     2
2012-06-01 09:33:00     3
2012-06-01 09:34:00     4
2012-06-01 09:35:00     5
2012-06-01 09:36:00     6
2012-06-01 09:37:00     7
2012-06-01 09:38:00     8
2012-06-01 09:39:00     9
2012-06-01 09:40:00    10
2012-06-01 09:41:00    11
2012-06-01 09:42:00    12
2012-06-01 09:43:00    13
2012-06-01 09:44:00    14
...
2012-06-06 15:45:00    1545
2012-06-06 15:46:00    1546
2012-06-06 15:47:00    1547
2012-06-06 15:48:00    1548
2012-06-06 15:49:00    1549
2012-06-06 15:50:00    1550
2012-06-06 15:51:00    1551
2012-06-06 15:52:00    1552
2012-06-06 15:53:00    1553
2012-06-06 15:54:00    1554
2012-06-06 15:55:00    1555
2012-06-06 15:56:00    1556
2012-06-06 15:57:00    1557
2012-06-06 15:58:00    1558
2012-06-06 15:59:00    1559
Length: 1560 

10:00:00
2012-06-01 10:00:00      30
2012-06-04 10:00:00     420
2012-06-05 10:00:00     810
2012-06-06 10:00:00    1200 

2012-06-01 10:00:00      30
2012-06-04 10:00:00     420
2012-06-05 10:00:00     810
2012-06-06 10:00:00    1200 

2012-06-01 10:00:00      30
2012-06-01 10:01:00      31
2012-06-04 10:00:00     420
2012-06-04 10:01:00     421
2012-06-05 10:00:00     810
2012-06-05 10:01:00     811
2012-06-06 10:00:00    1200
2012-06-06 10:01:00    1201 

2012-06-01 09:50:00    20
2012-06-01 09:51:00    21
2012-06-01 09:52:00    22
2012-06-01 09:53:00   NaN
2012-06-01 09:54:00    24
2012-06-01 09:55:00    25
2012-06-01 09:56:00    26
2012-06-01 09:57:00    27
2012-06-01 09:58:00   NaN
2012-06-01 09:59:00    29
2012-06-01 10:00:00    30 

2012-06-01 10:00:00      30
2012-06-04 10:00:00     419
2012-06-05 10:00:00     810
2012-06-06 10:00:00    1199
Freq: B
[Finished in 1.2s]

#+END_SRC
** 复利研究
[[info:org:Tables]]

  |  Task 1 |   Task 2 |    Total |           |
  |---------+----------+----------+-----------|
  |    2:12 |     1:47 | 03:59:00 | 6479.4398 |
  |    2:12 |     1:47 |    03:59 | 1159.2741 |
  | 3:02:20 | -2:07:00 |     0.92 | 1159.2741 |
  |         |          |          | 5468.4099 |
  #+TBLFM: $4=fvl(3%,5,1000)::@2$3=$1+$2;T::@2$4=pv(9%,4,2000)::@3$3=$1+$2;U::@4$3=$1+$2;t::@5$4=fvb(3%,5,1000)
  
|        |   |      | 利润(fvl) |   利润fvb |    利润fv |           |
|--------+---+------+-----------+-----------+-----------+-----------|
| 第一年 | 5 | 1000 | 1276.2816 |           |           |  276.2816 |
| 第二年 | 4 | 1000 | 1215.5063 |           |           |           |
| 第三年 | 3 | 1000 |  1157.625 |           |           |           |
| 第四年 | 2 | 1000 |    1102.5 |           |           |           |
| 第五年 | 1 | 1000 |     1050. |           |           |           |
|--------+---+------+-----------+-----------+-----------+-----------|
| Total  |   |      | 5801.9129 | 5801.9128 | 5525.6313 | -276.2815 |
#+TBLFM: @>$4=vsum(@2..@-1)::@>$5=fvb(5%,5,1000)::@>$6=fv(5%,5,1000)::@>$7=$6-$5::@2$4=fvl(5%,$2,$3)::@2$7=$4-$3::@3$4=fvl(5%,$2,$3)::@4$4=fvl(5%,$2,$3)::@5$4=fvl(5%,$2,$3)::@6$4=fvl(5%,$2,$3)



|        |   |      | 投资(pvl) |   投资pvb |    投资pv |            |
|--------+---+------+-----------+-----------+-----------+------------|
| 第一年 | 5 | 1000 | 783.52617 |           |           | -216.47383 |
| 第二年 | 4 | 1000 | 822.70247 |           |           |            |
| 第三年 | 3 | 1000 | 863.83760 |           |           |            |
| 第四年 | 2 | 1000 | 907.02948 |           |           |  5801.9128 |
| 第五年 | 1 | 1000 | 952.38095 |           |           |  5525.6313 |
|--------+---+------+-----------+-----------+-----------+------------|
| Total  |   |      | 4329.4767 | 4545.9505 | 4329.4767 |  -216.4738 |
#+TBLFM: @>$7=$6-$5::@>$6=pv(5%,5,1000)::@>$5=pvb(5%,5,1000)::@>$4=vsum(@2..@-1)::@2$4=pvl(5%,$2,$3)::@2$7=$4-$3::@3$4=pvl(5%,$2,$3)::@4$4=pvl(5%,$2,$3)::@5$4=pvl(5%,$2,$3)::@5$7=fvl(5%,5,@>$5)::@6$4=pvl(5%,$2,$3)::@6$7=fvl(5%,5,@>$4)


| money | rate |      npvb |       npv |
|-------+------+-----------+-----------|
|  1000 |   5% |           |           |
|  2000 |      |           |           |
|  3000 |      |           |           |
|  1000 |      |           |           |
|-------+------+-----------+-----------|
|  7000 |      | 6489.6879 | 6180.6552 |
#+TBLFM: @>$3=npvb(@2$2,[@2$1..@-1$1])::@>$4=npv(@2$2,[@2$1..@-1$1])::@>$1=vsum([@2$1..@-1$1])

第一年投资1000，后来2000，后来3000,最后1000


fvl的逐年累加利润等于fvb的计算方式

*  第十一章 金融和经济数据应用（二）


3、更多示例应用
本节介绍一些其他的例子。 
信号前沿分析
本小节将介绍一种简化的截面动量投资组合，并得出如何得到模型参数化网格。

* 第一步 命令流版本


#+BEGIN_SRC python
  import pandas as pd
  analysisFiles='J:\\BaiduYunDownload\\FAST816\\CertTest\\Output5MWYaw\\analysis15.csv'


  df_yaw=pd.read_csv(analysisFiles)
  the_end=df_yaw['RotTorq'].__len__()
  the_start=(the_end-the_end*1/4).__int__();
  print(sum(df_yaw['RotTorq'][the_start:the_end]))
  print(sum(df_yaw['RotTorq'][the_start:the_end])/the_end)

#+END_SRC


然后是把过程进行精炼
(你想着你愿意的代码是什么样子的，比如一个文件夹，然后遍历所有csv文件结尾


#+BEGIN_SRC python
    def analysisFolder(file):
        pass


    analysisFolder(file)

#+END_SRC

这样你就简化了你的双眼。



#+BEGIN_SRC python
  import pandas as pd
  analysisFiles='J:\\BaiduYunDownload\\FAST816\\CertTest\\Output5MWYaw\\analysis15.csv'

  def analysisFolder(file):
      df_yaw=pd.read_csv(analysisFiles)
      the_end=df_yaw['RotTorq'].__len__()
      the_start=(the_end-the_end*1/4).__int__();
      print(sum(df_yaw['RotTorq'][the_start:the_end]))
      print(sum(df_yaw['RotTorq'][the_start:the_end])/the_end)
      pass

  analysisFolder(analysisFiles)


#+END_SRC



第三个版本循环版本


#+BEGIN_SRC python
  import pandas as pd
  import os

  analysisFiles='J:\\BaiduYunDownload\\FAST816\\CertTest\\Output5MWYaw\\analysis15.csv'
  analysisFolder='J:\\BaiduYunDownload\\FAST816\\CertTest\\Output5MWYaw\\'


  def listdir(path, list_name):  # 传入存储的list
      #file2=os.listdir('L:\\newMovieGif\\NacelleContour\\png\\')
      files=os.listdir(path)
      files.sort()
      for file in files:
          file_path = os.path.join(path, file)
          if os.path.isdir(file_path):
              # listdir(file_path, list_name)
              continue
          else:
              list_name.append(file_path)
      return
  def analysisFile(file):
      df_yaw=pd.read_csv(analysisFiles)
      splitArrays=analysisFiles.split('\\')
      the_end=df_yaw['RotTorq'].__len__()
      the_start=(the_end-the_end*1/4).__int__();
      print(sum(df_yaw['RotTorq'][the_start:the_end]))
      print(splitArrays[splitArrays.__len__()-1],"\t",the_end,"\t", sum(df_yaw['RotTorq'][the_start:the_end])/the_end)
      pass

  #analysisFolder(analysisFiles)


  def summaryFolder(folder):
      files=[]
      listdir(analysisFolder,files)
      for file in files:
          analysisFile(file)
      pass


  summaryFolder(analysisFolder)

#+END_SRC


改进文件过滤


#+BEGIN_SRC python

  import pandas as pd
  import os

  analysisFiles='J:\\BaiduYunDownload\\FAST816\\CertTest\\Output5MWYaw\\analysis15.csv'
  analysisFolder='J:\\BaiduYunDownload\\FAST816\\CertTest\\Output5MWYaw\\'

  def file_list_start_WithStr_EndWithOut(dirname, sta='ana',ext='.csv'):
      """获取目录下所有特定后缀的文件
      @param dirname: str 目录的完整路径
      @param ext: str 后缀名, 以点号开头
      @return: list(str) 所有子文件名(不包含路径)组成的列表
      """
      return list(filter(
          lambda filename: os.path.splitext(filename)[1] == ext and \
          os.path.splitext(filename)[0].startswith(sta),
          os.listdir(dirname)))

  def analysisFile(file):
      df_yaw=pd.read_csv(file)
      splitArrays=file.split('\\')
      the_end=df_yaw['RotTorq'].__len__()
      the_start=(the_end-the_end*1/4).__int__();
      print(sum(df_yaw['RotTorq'][the_start:the_end]))
      print(splitArrays[splitArrays.__len__()-1],"\t",the_end,"\t", sum(df_yaw['RotTorq'][the_start:the_end])/the_end)
      pass


  def summaryFolder(folder):
      files=[]
      #listdir(analysisFolder,files)
      files=file_list_start_WithStr_EndWithOut(analysisFolder,sta='ana',ext='.csv')
      for file in files:
          analysisFile(analysisFolder+file)
      pass


  summaryFolder(analysisFolder)
#+END_SRC



然后你就接着可以改进字段等操作了


** 总结

你想要让函数怎么用起来，先写出来，把他的名字和所需参数放上去。
然后再逐步建立起来即可。

这种编程方式叫做`wishful thinking`
